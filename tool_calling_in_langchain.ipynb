{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXbI1sHgyCJG15AF4sIaaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaidur404786/langchain-practice/blob/main/tool_calling_in_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bspzFtgSjX2",
        "outputId": "6f70ed13-c9ef-490b-f7fc-7d184aca919b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.9-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.9)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Collecting langchain-core<2.0.0,>=1.2.8 (from langchain)\n",
            "  Downloading langchain_core-1.2.12-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.17.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.6.9)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (9.1.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.9-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.12-py3-none-any.whl (500 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.9\n",
            "    Uninstalling langchain-core-1.2.9:\n",
            "      Successfully uninstalled langchain-core-1.2.9\n",
            "Successfully installed langchain-core-1.2.12 langchain-openai-1.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-core requests\n"
      ],
      "metadata": {
        "id": "jVxig5eIVpzd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "gntP4Z59VZdD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage\n",
        "import requests"
      ],
      "metadata": {
        "id": "yib2vNKmWNUp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create tool\n",
        "\n",
        "@tool\n",
        "def muitiply(a:int,b:int)->int:\n",
        "  \"\"\"Mutiply two number\"\"\"\n",
        "  return a*b"
      ],
      "metadata": {
        "id": "4dDfgiscWiRM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(muitiply.invoke({'a':10,'b':20}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP_e7crvW20V",
        "outputId": "dbf4b6e2-9ee7-422a-b1e0-98af309a7cda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(muitiply.name)\n",
        "\n",
        "print(muitiply.description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_3qr3WcW_1T",
        "outputId": "1a08a9e3-647d-4e12-a831-cbd783702f3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "muitiply\n",
            "Mutiply two number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAfJXLW6XGOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool bind"
      ],
      "metadata": {
        "id": "Hs-4ODV8XNfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model=\"meta/llama-4-maverick-17b-128e-instruct\",\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=\"nvapi-your key\",   # your key\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "#llama-4 model are not train on tool calling but you can use openai api it will call tool"
      ],
      "metadata": {
        "id": "ZW3kl5AdXOyN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tool=llm.bind_tools(\n",
        "    tools=[muitiply],\n",
        "    tool_choice=\"auto\"   # IMPORTANT\n",
        ")"
      ],
      "metadata": {
        "id": "nDjzLnspXo3x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tool.invoke('What is the capital of France')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM7wGxijXvjZ",
        "outputId": "198a2c48-8509-4e2d-d30f-6269b8071456"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 16, 'total_tokens': 24, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'fd07de42947948db9255f51c3b11b130', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c597a-3a8a-7142-8d4d-95bd1698f231-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 16, 'output_tokens': 8, 'total_tokens': 24, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As you can see there is no use of Mutiply method because LLM think for above query there is no need for it.\n",
        "\n",
        "But NVIDIA Llama Still May Ignore Tools because it is not train it.\n",
        "\n",
        "Open-source models are NOT guaranteed to follow tool rules. So we will use trick to show only  mutiply tool for understanding"
      ],
      "metadata": {
        "id": "wdc_7bdOa_hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual reponse"
      ],
      "metadata": {
        "id": "2IpxKiQOfF6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iO9bZIGfOBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_with_tool.invoke('Use the multiply tool to calculate 10 * 40')\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAiydFQSZrbC",
        "outputId": "b16940e6-62b0-4387-9a7b-179ca991e479"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='## Step 1: Understand the problem\\nThe task requires using the multiplication operation to calculate the product of 10 and 40.\\n\\n## Step 2: Perform the multiplication\\nTo find the product, we multiply 10 by 40. This operation involves adding 10 together 40 times or using the standard multiplication algorithm.\\n\\n## Step 3: Calculate the product\\n10 * 40 = 400.\\n\\nThe final answer is: $\\\\boxed{400}$', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 21, 'total_tokens': 115, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'fb20b7224ce94ff7b849836755d5b01d', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c597a-473d-7c61-a7fa-94c77e0ea41e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 21, 'output_tokens': 94, 'total_tokens': 115, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force tool calling\n",
        "\n",
        "llm_with_tool = llm.bind_tools(\n",
        "    tools=[muitiply],\n",
        "    tool_choice=\"muitiply\"  # force\n",
        ")\n",
        "\n",
        "response = llm_with_tool.invoke(\"Multiply 10 and 40\")\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KlD7O04fOry",
        "outputId": "59f7fb7e-909c-4474-dcb8-37a36f653044"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 16, 'total_tokens': 42, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': '061f31924b4647549c2a44e1f06a13de', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c598f-b7f4-73c1-a1b7-67465f9dfa94-0', tool_calls=[{'name': 'muitiply', 'args': {'a': 10, 'b': 40}, 'id': 'call_fffd91e155774b76bcef2b7f', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 16, 'output_tokens': 26, 'total_tokens': 42, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.tool_calls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHUwvoyMZmBL",
        "outputId": "872587a5-d678-46b8-8dc6-6a02ff5aa89c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'muitiply', 'args': {'a': 10, 'b': 40}, 'id': 'call_fffd91e155774b76bcef2b7f', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arg=response.tool_calls[0]['args']\n",
        "arg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DAFZauTfsgz",
        "outputId": "bb88cf1c-9dbb-4932-efb3-f3300fc71ac0"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 10, 'b': 40}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=muitiply.invoke(arg)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTvd6vMaf_Ou",
        "outputId": "2407170e-7bc2-4b06-9553-9ada97afca29"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2XvVhmvgQja"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But to send result again to LLM to give a attractive response to a user. we need to send the complete tool_call with its other parameters."
      ],
      "metadata": {
        "id": "PuoedlSsgRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=muitiply.invoke(response.tool_calls[0])\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpXuTwONggXX",
        "outputId": "3df510d7-fe8f-4640-f068-f163b8769107"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMessage(content='400', name='muitiply', tool_call_id='call_fffd91e155774b76bcef2b7f')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now this message will send to LLM then it will give final response\n"
      ],
      "metadata": {
        "id": "tttW5XO1grG5"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps"
      ],
      "metadata": {
        "id": "9YxAUNy8g6Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#human messages will not work in  llama-4 but you can send in openai\n",
        "query=HumanMessage(\"Can you muitiply 10 with 40\")"
      ],
      "metadata": {
        "id": "wdXvG5gqg8mU"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=[query]"
      ],
      "metadata": {
        "id": "Eb1q1xN0hIV5"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z1XrsWGhLd6",
        "outputId": "79d86c06-7768-4b3f-bff7-0864c706e5e4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Can you muitiply 10 with 40', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=llm_with_tool.invoke(\"Can you muitiply 10 with 40\")\n"
      ],
      "metadata": {
        "id": "Ve8-l-xPhQz6"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCvdBX2chgCw",
        "outputId": "0e681110-ee51-4af1-9d21-d497b2f23129"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 20, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'fa989fd60b274e47bd67bf59fadb0523', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c598f-d678-7901-b1d9-8d34af937677-0', tool_calls=[{'name': 'muitiply', 'args': {'a': 10, 'b': 40}, 'id': 'call_6dfe8378d575448b8b3d3278', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 20, 'output_tokens': 26, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message.append(result)"
      ],
      "metadata": {
        "id": "ifKdEmmPhjt4"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4_skkjEhw-W",
        "outputId": "71a8ea41-2ffa-46d1-b62d-33378ffd215e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Can you muitiply 10 with 40', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 20, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'fa989fd60b274e47bd67bf59fadb0523', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c598f-d678-7901-b1d9-8d34af937677-0', tool_calls=[{'name': 'muitiply', 'args': {'a': 10, 'b': 40}, 'id': 'call_6dfe8378d575448b8b3d3278', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 20, 'output_tokens': 26, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}})]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_result=muitiply.invoke(result.tool_calls[0])"
      ],
      "metadata": {
        "id": "f60YsuwThx8C"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message.append(tool_result)"
      ],
      "metadata": {
        "id": "7q94LP_jiHqH"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvq60R4QiJYs",
        "outputId": "3432636c-d608-43b4-de50-a057a52e39f1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Can you muitiply 10 with 40', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 20, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'fa989fd60b274e47bd67bf59fadb0523', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c598f-d678-7901-b1d9-8d34af937677-0', tool_calls=[{'name': 'muitiply', 'args': {'a': 10, 'b': 40}, 'id': 'call_6dfe8378d575448b8b3d3278', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 20, 'output_tokens': 26, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}),\n",
              " ToolMessage(content='400', name='muitiply', tool_call_id='call_6dfe8378d575448b8b3d3278')]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# its look like history of conversation\n",
        "llm_with_tool = llm.bind_tools(\n",
        "    tools=[muitiply],\n",
        ")\n",
        "# this is trick just do for this llm otherwise in openAi , you can directly send messages in invoke method and it will return the similar thing\n",
        "result=llm_with_tool.invoke(\"Imagine you are working as a ChatGPT and you are train on tool_calls and  I ma giving you a this reponse. No need for extra explataion just answer the query , after getting help from tool call\"+ str (message))"
      ],
      "metadata": {
        "id": "b0bHc0xuiMOS"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLingZH9jp8F",
        "outputId": "a8684931-e249-430c-af13-7ac857ee9359"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The result of multiplying 10 with 40 is 400.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 371, 'total_tokens': 385, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_provider': 'openai', 'model_name': 'meta/llama-4-maverick-17b-128e-instruct', 'system_fingerprint': None, 'id': 'a3156ac24059456db7e9900019e9e4aa', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c5999-2efa-7fc0-9bf6-7e323185120a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 371, 'output_tokens': 14, 'total_tokens': 385, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYBRuObXmUTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67a6f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9032b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Ollama with  existing model\n",
    "llm = Ollama(model=\"mistral:latest\")\n",
    "\n",
    "# 2. Load Data\n",
    "with open('../2000002.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a413af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to clean HTML\n",
    "def clean_text(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(separator=\" \", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48424d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the Extraction Chain\n",
    "template = \"\"\"\n",
    "You are an expert SAP Technical Writer.\n",
    "I will give you a section of an SAP Knowledge Base Article (KBA).\n",
    "Your task is to create a training example for a Chatbot.\n",
    "\n",
    "Format your response exactly as a JSON object with these keys:\n",
    "- \"instruction\": A question a user might ask about this problem.\n",
    "- \"input\": The context (Symptom or Cause).\n",
    "- \"output\": The solution (Resolution).\n",
    "\n",
    "KBA Title: {title}\n",
    "Content: {content}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c01cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"title\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ffd4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now  Mistral to extract Q&A pairs... (this may take a minute)\n",
      "Extraction failed: Ollama call failed with status code 500. Details: {\"error\":\"llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer\\nllama_model_load_from_file_impl: failed to load model\"}\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Extraction\n",
    "dataset = []\n",
    "title = data.get('shortText')\n",
    "\n",
    "# We combine relevant sections for the model to analyze\n",
    "full_content = \"\"\n",
    "for section in data.get('texts', []):\n",
    "    if section['title'] in ['Symptom', 'Cause', 'Resolution']:\n",
    "        full_content += f\"\\n[{section['title']}]\\n{clean_text(section['text'])}\"\n",
    "\n",
    "print(\"Now  Mistral to extract Q&A pairs... (this may take a minute)\")\n",
    "try:\n",
    "    # Invoke Ollama\n",
    "    formatted_prompt = prompt.format(title=title, content=full_content[:3000]) # Trim to fit context\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Attempt to parse the JSON output from Mistral\n",
    "    # (LLMs sometimes add text before/after JSON, so we find the braces)\n",
    "    json_start = response.find('{')\n",
    "    json_end = response.rfind('}') + 1\n",
    "    clean_json = response[json_start:json_end]\n",
    "    \n",
    "    entry = json.loads(clean_json)\n",
    "    dataset.append(entry)\n",
    "    print(\"Success! Extracted:\", entry['instruction'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Extraction failed: {e}\")\n",
    "    # Fallback to manual if LLM fails\n",
    "    dataset.append({\n",
    "        \"instruction\": f\"How do I fix {title}?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"See attached KBA.\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaea8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SAPKBAExtractor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def _clean_html(self, html_content):\n",
    "        \"\"\"Removes HTML tags and normalizes whitespace.\"\"\"\n",
    "        if not html_content:\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        # Remove multiple spaces/newlines\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def extract_pairs(self):\n",
    "        \"\"\"\n",
    "        Generates Instruction/Input/Output pairs.\n",
    "        Returns a list of dictionaries.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # 1. Extract Core Metadata\n",
    "        kba_id = str(self.data.get('_id', 'Unknown ID'))\n",
    "        title = self.data.get('shortText', 'Unknown Issue')\n",
    "        \n",
    "        # 2. Extract Sections (Symptom, Cause, Resolution)\n",
    "        texts = {t['title']: self._clean_html(t['text']) for t in self.data.get('texts', [])}\n",
    "        \n",
    "        symptom = texts.get('Symptom', '')\n",
    "        cause = texts.get('Cause', '')\n",
    "        resolution = texts.get('Resolution', '')\n",
    "        \n",
    "        # --- STRATEGY 1: Contextual Troubleshooting (The \"Hard\" Question) ---\n",
    "        # Instruction: User describes the problem.\n",
    "        # Output: Model provides the solution.\n",
    "        if symptom and resolution:\n",
    "            pairs.append({\n",
    "                \"instruction\": f\"I am facing an issue in SAP HANA. The symptoms are: {symptom[:300]}... How do I resolve this?\",\n",
    "                \"input\": f\"Context: {title}\", \n",
    "                \"output\": resolution\n",
    "            })\n",
    "\n",
    "        # --- STRATEGY 2: Direct ID Lookup (The \"Expert\" Question) ---\n",
    "        # Instruction: User asks about a specific Note ID.\n",
    "        # Output: Model summarizes the note.\n",
    "        if resolution:\n",
    "            pairs.append({\n",
    "                \"instruction\": f\"What is the resolution provided in SAP Note {kba_id}?\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": f\"Title: {title}\\n\\nResolution: {resolution}\"\n",
    "            })\n",
    "\n",
    "        # --- STRATEGY 3: Title-Based Query (The \"Search\" Question) ---\n",
    "        # Instruction: User asks using the exact error message/title.\n",
    "        # Output: Model provides Cause + Resolution.\n",
    "        if title and resolution:\n",
    "            combined_answer = f\"Cause: {cause}\\n\\nFix: {resolution}\" if cause else resolution\n",
    "            pairs.append({\n",
    "                \"instruction\": f\"How do I fix the SAP error: '{title}'?\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": combined_answer\n",
    "            })\n",
    "            \n",
    "        return pairs\n",
    "\n",
    "    def save_to_jsonl(self, pairs, output_file):\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for pair in pairs:\n",
    "                json.dump(pair, f)\n",
    "                f.write('\\n')\n",
    "        print(f\" Successfully saved {len(pairs)} training pairs to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c584a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully saved 3 training pairs to sap_instruction_dataset.jsonl\n",
      "\n",
      "--- Sample Pair ---\n",
      "{\n",
      "  \"instruction\": \"I am facing an issue in SAP HANA. The symptoms are: SQL statements run for a long time or consume a high amount of resources in terms of memory and CPU.... How do I resolve this?\",\n",
      "  \"input\": \"Context: FAQ: SAP HANA SQL Optimization\",\n",
      "  \"output\": \"\\ufeff1. Where do I find information about SQL statement tuning provided by SAP? The SAP HANA Troubleshooting and Performance Analysis Guide contains detailed information about tuning SQL statements. The SAP HANA Performance Guide for Developers provides best practices and SQL tuning details that can be considered when designing applications. The SAP Gateway Foundation Developer Guide provide information for setting up applications using OData and the SAP Gateway Foundation, e.g. Fiori. If you want to optimize ABAP coding for SAP HANA, you can refer to the blog Performance Guidelines for ABAP Development on SAP HANA . The openSAP course A First Step Towards SAP HANA Query Optimization teaches how SQL queries are processed in SAP HANA, how the SAP HANA SQL optimizer works, how to analyze query performance issues in SAP HANA and how to optimize them yourself. \\ufeff2. Which indications exist for critical SQL statements? The following SAP HANA alerts indicate problems in the SQL area: Alert Name Description 39 Long-running statements Identifies long-running SQL statements. SQL: \\\"HANA_Configuration_MiniChecks\\\" (SAP Notes 1969700 , 1999993 ) returns a potentially critical issue (C = 'X') for one of the following individual checks: Check ID Details M1110 SQL using in average > 1 connection (short-term) M1112 SQL using in average > 1 thread (last hour) M1113 SQL using in average > 1 thread (short-term) M1114 SQL with engine performance impact (long-term) M1115 Longest running current SQL statement (h) M1118 Longest running current job (s) M1120 Exp. stmt. trace: SQL running > 1 h (short-term) M1125 Columns with many scanned records M1126 Row tables with many scanned records M1152 Max. SQL contexts per connection M1155 Number of SQL statements in SUSPENDED state M1159 Hourly max. commit I/O time avg. (ms, long-term) M1160 Average commit time (ms) M1161 Maximum commit I/O time (s, long-term) M1162 Average commit I/O time (ms, long-term) M1163 Commit vs. execution ratio (%, mid-term) M1164 Unexpected distributed executions M1165 Internal executions (%) M1167 Statement hints with hint string being NULL M1168 Delivered statement hints implemented M1169 Total number of implemented statement hints M1170 Average database request time (ms) M1180 Avg. ABAP buffer loading sessions (short-term) M1181 Avg. FDA write sessions (short-term) M1182 Avg. ABAP buffer loading sessions (short-term) M2530 Avg. SLT logging sessions (short-term) \\ufeff3. Which prerequisites are helpful for SQL statement tuning? See SAP Note 2000000 for more information about an optimal SAP HANA performance configuration and useful prerequisites for performance and SQL statement optimization. \\ufeff4. How can I identify a specific SQL statement? Quite obviously a specific SQL statement can be identified by its SQL text. Due to the fact that the text can be quite long and there can be different similar SQL texts it is useful to identify a SQL statements based on a shorter identifier. There are three different columns in views like M_SERVICE_THREAD_SAMPLES that can be used to identify a SQL statement on different levels: Column name Level Details STATEMENT_HASH global The STATEMENT_HASH is derived from the SQL text and so it globally identifies a specific SQL statement. STATEMENT_ID per connection The STATEMENT_ID is derived from the SQL text and the connection and so it can be used to distinguished identical SQL statements being executed in different connections. Nevertheless it can't be used to distinguish different executions of the same SQL statement in the same connection. STATEMENT_EXECUTION_ID per execution The STATEMENT_EXECUTION_ID is unique for every statement execution, so also executions of the same SQL statement in the same connection can be distinguished. \\ufeff5. What is an expensive SQL statement? An expensive SQL statement is a database access that shows high values in areas like: High execution time : Usually most important in the area of performance analysis are SQL statements with a particular high overall runtime. Usually the overall runtime counts, so it doesn't matter if a statement is executed 1 time with a duration of 1000 seconds or if it is executed 1 million times with an average duration of 1 ms, because in both cases the statement is responsible for a total duration of 1000 seconds. High memory utilization High CPU consumption High lock wait time High amount of returned records High number of executions High intra- / inter-node network communication \\ufeff6. How can time information in the SQL cache (M_SQL_PLAN_CACHE) be interpreted? Tables like M_SQL_PLAN_CACHE, M_SQL_PLAN_CACHE_RESET and HOST_SQL_PLAN_CACHE contain several time related columns and it is not always clear how to interpret them: Time column type Description CURSOR Contains the overall cursor time, i.e. from start of the execution until having sent the last package to the client; when the client processes the data in multiple fetches, the network and client time during these fetches is included in the cursor time Preparation time isn't included Mainly applies to SELECT and CALL operations, can be 0 for others (e.g. DML, DDL) If the client performs other tasks between fetches of data, the cursor time can be much higher than the SAP HANA server time. This can result in MVCC issues because old versions of data need to be kept until the execution is finished. EXECUTION Contains the execution time (open + fetch + lock wait + close) on SAP HANA server side, does not include preparation time EXECUTION_OPEN Contains the open time on SAP HANA server side Includes the actual retrieval of data in case of column store accesses with early materialization EXECUTION_FETCH Contains the fetch time on SAP HANA server side Includes the actual retrieval of data in case of row store accesses or late materialization EXECUTION_CLOSE Contains the close time on SAP HANA server side TABLE_LOAD Contains the table load time during preparation, is part of the preparation time PREPARATION Contains the preparation time LOCK_WAIT Contains the transaction lock wait time, internal locks are not included Usually long EXECUTION_OPEN or EXECUTION_FETCH times are caused by retrieving the data. From a SQL tuning perspective the most important information is the total elapsed time of the SQL statement which is the sum of preparation time and execution time. \\ufeff7. How can I determine the most critical SQL statements? For a pure technical SQL optimization it is useful to determine the most expensive SQL statements in terms of elapsed time and check the result list top down. The most expensive SQL statements in the SQL cache can be determined in the following ways: M_SQL_PLAN_CACHE / HOST_SQL_PLAN_CACHE SAP HANA Studio -> Administration -> Performance -> SQL Plan Cache ABAP: DBACOCKPIT -> Performance -> SQL Plan Cache SQL: \\\"HANA_SQL_SQLCache\\\" / SQL: \\\"HANA_SQL_SQLCache_TopLists\\\" (SAP Note 1969700 ) Solution Manager self service \\\"SQL Statement Tuning\\\" (SAP Note 1601951 ) Additionally expensive SQL statements can be captured by trace features: Trace Environment SAP Note Detail ST05 ABAP SQL trace ST12 ABAP Single transaction analysis SQLM ABAP 1885926 SQL monitor SQL trace SAP HANA Studio, SQL, ABAP (DBACOCKPIT) 2031647 2412519 SQL trace Expensive statements trace SAP HANA Studio, SQL, ABAP (DBACOCKPIT) 2180165 Expensive statements trace If you are interested in the top SQL statements in terms of memory consumption, you can activate both the expensive statements trace and the statement memory tracking (SPS 08 or higher, see SAP Note 1999997 -> \\\"Is it possible to limit the memory that can be allocated by a single SQL statement?\\\") and later on run SQL: \\\"HANA_SQL_ExpensiveStatements\\\" (SAP Note 1969700 ) with ORDER_BY = 'MEMORY'. The currently running SQL statements can be determined via SQL: \\\"HANA_SQL_ActiveStatements\\\" (SAP Note 1969700 ). In all cases you get a list of SQL statements / STATEMENT_HASH values that subsequently be analyzed in more detail. \\ufeff8. How can I determine and interpret basic performance information for a particular SQL statement? The following SQL statements available via SAP Note 1969700 can be used to collect further details for a given STATEMENT_HASH (to be specified in \\\"Modification section\\\" of the statements): SQL command Details SQL: \\\"HANA_SQL_ActiveProcedures\\\" Procedure calls and SQL statements executed within procedures SQL: \\\"HANA_SQL_StatementHash_BindValues\\\" Captured bind values from SQL cache in case of long running prepared SQL statements (SPS 08 and higher) SQL: \\\"HANA_SQL_StatementHash_DataCollector\\\" Collection of various details related to a specific SQL statement including: Key figures Statement text Bind values SQL cache View information Table information Index information Column information Partition information Expensive statements trace Executed statements trace Transactional locks Thread samples OOM events Active statements Call stacks Pinned SQL plans Statement hints SQL: \\\"HANA_SQL_StatementHash_KeyFigures\\\" Important key figures from SQL cache, for examples see below SQL: \\\"HANA_SQL_StatementHash_SQLText\\\" SQL statement text SQL: \\\"HANA_SQL_ExpensiveStatements\\\" Important key figures from expensive statement trace (SAP Note 2180165 ) SQL: \\\"HANA_SQL_ExpensiveStatements_BindValues\\\" Captured bind values from expensive statement trace (SAP Note 2180165 ) Below you can find several typical output scenarios for SQL: \\\"HANA_SQL_StatementHash_KeyFigures\\\" . Scenario 1: Transactional lock waits We can see that nearly the whole execution time is caused by lock wait time, so transactional lock (i.e. record or object locks) are responsible for the long runtime. Further transactional lock analysis can now be performed based on SAP Note 1999998 . Scenario 2: High number of executions An elapsed time of 0.55 ms for 1 record is not particularly bad and for most of the SQL statements no further action is required. In this particular case the number of executions is very high, so that overall the execution time of this SQL statement is significant. Optimally the number of executions can be reduced from an application perspective . If this is not possible, further technical analysis should be performed. Really quick single row accesses can be below 0.10 ms, so there might be options for further performance improvements (e.g. index design or table store changes). Scenario 3: High elapsed time An execution time of 284 ms for retrieving one row from a single table is definitely longer than expected and it is very likely that an improvement can be achieved. Further analysis is required to understand the root cause of the increased runtime. Scenario 4: High elapsed time for DML operations, no lock wait time An elapsed time of 10 ms for inserting a single record is quite high. If DML operations have an increased elapsed time it can be caused by internal locks that are e.g. linked to the blocking phase of savepoints. Further internal lock analysis can now be performed based on SAP Note 1999998 . Scenario 5: Many records Reading about 200,000 records in less than 2 seconds is not a bad value. In the first place you should check from an application perspective if it is possible to reduce the result set or the number of executions. Apart from this there are typically also technical optimizations available to further reduce the elapsed time (e.g. delta storage or table store optimizations). Scenario 6: High Preparation Time Preparation time is linked to the initial parsing and compilation. You can reduce this overhead by using bind variables rather than literals so that several SQL statements with the same structure need to be parsed only once. Furthermore you can perform a more detailed analysis to understand why the preparation step is taking longer than expected. \\ufeff9. Which options exist to understand the execution of a SQL statement in detail? In order to understand how a SQL statement can be tuned it is essential to know how it is processed. For this purpose you can use the following tools: Tool Details Explain High-level information for SQL execution (e.g. joins, used row store indexes) DBACOCKPIT: Diagnostics -> Explain SAP HANA Studio: SQL console -> Explain plan (right mouse click) SQL: \\\"EXPLAIN PLAN FOR ...\\\" and subsequent evaluation of explain plan table via SQL: \\\"HANA_SQL_ExplainPlan\\\" (SAP Note 1969700 ) PlanViz Detailed graphical insight in SQL execution DBACOCKPIT: Diagnostics -> Explain -> Execution trace SAP HANA Studio: SQL console -> Visualize plan (right mouse click) -> Execute Thread sample analysis High-level thread state and lock type information (e.g. useful in case of waits for internal locks which are not reflected in the \\\"lock wait time\\\") SQL: \\\"HANA_Threads_ThreadSamples_FilterAndAggregation\\\" (SAP Note 1969700 ) Performance trace Detailed insight in SQL execution including SQL plan and function profiling. See SAP Note 1787489 for more details. User-specific trace Granular trace information for configurable components of the SQL statement execution SAP HANA Studio: Administration -> Trace configuration -> User-Specific Trace Trace components depend on individual scenario, see SAP Note 2909779 (\\\"User-specific trace\\\") for typical trace components \\ufeff10. What are typical approaches to tune expensive SQL statements? Depending on the results of the analysis of an SQL statement the following optimizations can be considered. Possible symptoms Details High number of executions Check from application perspective if the number of executions can be reduced, e.g. by avoiding identical SELECTs or adjusting the application logic. High number of selected records Check from application perspective if you can restrict the number of selected records by adding further selection conditions or modifying the application logic. Check if you can reduce the amount of relevant data by archiving or cleanup of technical tables (SAP Note 2388483 ). High number of selected columns Always specify a targeted list of required columns rather than using \\\"SELECT *\\\". The performance penalty of \\\"SELECT *\\\" is particularly high in case of hundreds of columns or in case of partitioned tables (SAP Note 2044468 ) where a similar fix overhead for column materialization is required for each relevant partition compared to a non-partitioned table. The relevant methods in PlanViz (SAP Note 2119087 -> \\\"PlanViz / Execution Trace\\\") are \\\"Materialize Results\\\" or ProjectBufferOp. High lock wait time due to record locks Check from application perspective if you can reduce concurrent changes of same records. Check from application perspective if you can reduce the critical time frame between change operation and next COMMIT. See SAP Note 1999998 for more information. High lock wait time due to object locks Check if you can schedule critical offline DDL operations less often or at times of lower workload. Check if you can use online instead of offline DDL operations. See SAP Note 1999998 for more information. High total execution time, significant amount of thread samples pointing to internal lock waits Check if you can reduce the internal lock wait time (SAP Note 1999998 ). High system CPU consumption See SAP Note 2100040 and check for system CPU optimizations based on the related call stacks and executed operating system calls. Execution time higher than expected, optimal index doesn't exist Table and column scan related thread methods and details like: ClusterScanBvOutJob<BV> ClusterScanBvOutJob<range> ClusterScanVecOutJob<range> HEX job running hex::cs::TableScanScheduleOp HEX job running hex::operators::TableScanScheduleOp IndirectPredScanBvOutJob<ScanRangesPredicate> IndirectPredScanVecOutJob<ScanRangesBinSearchPredicate> IndirectPredScanVecOutJob<ScanVectorBinSearchPredicate> IndirectPredScanVecOutJob<ScanVectorPredicate> IndirectScanBvOutJob<BV> IndirectScanBvOutJob<range> IndirectScanVecOutJob<BV> IndirectScanVecOutJob<range> JEJobReadIndexChunked JobParallelMgetSearch JobParallelPagedMgetSearch PrefixedScanVecOutJob<range> RlePredScanJob<ScanVectorBinSearchPredicate>(out=vector) RlePredScanJob<ScanVectorPredicate>(out=vector) RleScanBvOutJob<BV RleScanBvOutJob<range> RleScanVecOutJob<BV> RleScanVecOutJob<range> scanWithoutIndex searchDocumentsIterateDocidsParallel SparseBvScanBvOutJob SparseBvScanVecOutJob SparsePredScanBvOutJob<ScanRangesPredicate> SparsePredScanVecOutJob<ScanRangesPredicate> SparsePredScanVecOutJob<ScanVectorBinSearchPredicate> SparsePredScanVecOutJob<ScanVectorPredicate> SparseRangeScanBvOutJob SparseRangeScanVecOutJob sparseSearch sse_icc_lib::mgetSearchi_AVX2impl sse_icc_lib::mgetSearchi_AVX Worker Job hex::cs::DataVectorScanOrLookupOp Call stacks containing: AttributeEngine::JEJobReadIndexChunked hex::cs::TableScanControlOp::run ss_lib::mgeti_SSE4 sse_icc_lib::mgetSearchi_SSE4impl sse_icc_lib::mgetSearchi_SSE4 TRexUtils::IndexVectorGenericImpl::mgetSearchBv TRexUtils::IndexVectorRef::mgetSearch TRexUtils::JobParallelMgetSearch::run() TRexUtils::Parallel::JobBase::runEx() TRexUtils::Parallel::JobBase::run Check from an application perspective if you can adjust the database request so that existing indexes can be used efficiently. Create a new index or adjust an existing index so that the SQL statement can be processed efficiently: Make sure that selective fields are contained in the index Use indexes that don't contain more fields than specified in the WHERE clause See SAP Note 2321573 for more information. SAP Note 2516807 describes a problem where a FULL index can't be used in context of a compressed column (SAP HANA 1.00.122.10 - 1.00.122.11, <= 2.00.012.01 and 2.00.020. SAP Note 2914233 describes a bug in context IN lists with decimal values where an existing index can't be used with of SAP HANA <= 2.00.037.04 and <= 2.00.045. Expensive index joins Call stack modules: hex::cs::IndexJoinOp::run If long runtime is observed in context of inefficient index joins, you can disable it on statement level using the NO_HEX_INDEX_JOIN hint (SAP Note 2142945 ). See also check ID C1500 (\\\"HEX index join activity\\\") described in SAP Note 2313619 . Execution time higher than expected, negative impact by existing partitioning Consider the following possible optimization approaches: Make sure that existing partitioning optimally supports the most important SQL statements (e.g. via partition pruning, load distribution and minimization of inter-host communication). See SAP Note 2044468 for more information. Define partitioning on as few columns as possible. A high number of columns used for the partitioning criteria (e.g. HASH partitioning on 10 primary key columns) can result in significant internal overhead during partition pruning evaluation. Call stacks containing QueryMediator::FilterProcessor::getPruningEntries, QueryMediator::PruningOptimization::getPruningEntries, QueryMediator::FilterProcessor::mergePruningEntries, QueryMediator::PruningOptimization::mergePruningEntries, TRexAPI::FilterExpression::insertPruningEntry or TRexAPI::Partitioning::PruningEntry::PruningEntry are good indications for this kind of overhead. For technical reasons partitioning can sometimes introduce performance overhead in combination with join engine accesses. In this case you can check if it is possible to eliminate the use of the join engine, e.g. by removing a DISTINCT aggregation. Use as few partitions as possible. A high number of partitions can result in a significant overhead (e.g. threads with method getColumnStat (SAP Note 2114710 ) when for each individual partition column statistics need to be retrieved). Also materializing result column values (PlanViz methods like \\\"Materialize Results\\\" or ProjectBufferOp) have a certain overhead per partition containing data. Thus, reading a small amount of records from a higher number of partitions, can be significantly more expensive compared to reading all records from a single partition or non-partitioned table. Try to avoid changes of records that require a remote uniqueness check or a partition move (i.e. changes of partitioning key columns or primary key columns), because a significant overhead is imposed. See SAP Note 2312769 for more information. UPSERT operations on partitioned tables can require significant time in module TrexStore::UdivListContainerMVCC::checkValidEqualSSN (SAP Note 2373312 ). Increasing the SAP profile parameter dbs/hdb/cmd_buffersize can reduce the overhead. Partitioning in context of data aging (SAP Note 2416490 ) may result in decreased performance of UPDATE / UPSERT operations (SAP Note 2387064 ) in module TRexAPI::TableUpdate::execute_update_partitioning_attribute. The related thread method is SearchPartJob. If it is technically possible to disable partitioning, you can consider undoing the partitioning. Starting with SAP HANA 1.00.122.00 OLTP accesses to partitioned tables are executed single-threaded and so the runtime can be increased compared to a parallelized execution (particularly in cases where a high amount of records is processed). You can set indexserver.ini -> [joins] -> single_thread_execution_for_partitioned_tables to \\\"false\\\" in order to allow also parallelized processing (e.g. in context of COUNT DISTINCT performance on a partitioned table). See SAP Note 2620310 for more information. Setting this parameter to \\\"false\\\" is also recommended as a best practice in SAP Note 2600030 . Optimizer estimations are sometimes performed based on a subset of the table partitions (default: 8). In case too many empty or nearly empty partitions exist, the estimation can be quite imprecise. Therefore it is good to keep the amount of (nearly) empty partitions small compared to the amount of significantly used partitions. If required, you can increase the number of sampled partitions to a higher value using parameter indexserver.ini -> [sql] -> compile_time_sampling_partitions = <number_of_sampled_partitions>. Be aware that higher values result in better estimations, but they can increase the parse times, so adjustments need to be done with care. Partitioning in tree specification notation (as e.g. used with BW4/HANA 2.0 SPS 04 and higher) can result in performance overhead with the OLAP engine. This issue is fixed with SAP HANA >= 2.00.048.03 and 2.00.054 (SAP Note 2966606 ). Long runtimes with selective IN lists on row store tables For row store tables the optimizer decides cost based if an IN list is evaluated via index (CPBTREE INDEX SEARCH (IN)) or not (CPBTREE INDEX SEARCH). In case of selective IN lists that aren't considered by the optimizer during index processing, you can add the following hint: OPTIMIZATION_LEVEL(RULE_BASED) This is e.g. required in context of the statement hint delivery (SAP Note 2700051 ) for TRFCQIN so that it is guaranteed that the selective QNAME IN list is processed via the index. Long runtime with OR condition having selection conditions on both tables If an OR concatenation is used and the terms reference columns of more than one table, a significant overhead can be caused by the fact that a cartesian product of both individual result sets is required in some cases. If you face performance issues in this context, you can check if the problem disappears with a simplified SQL statement accessing only one table. If yes, you should check if you can avoid joins with OR concatenated selection conditions on both tables. Long runtime with join conditions concatenated with OR join conditions concatenated with OR (e.g. \\\"A.X = B.X1 AND A.Y = B.Y1 OR A.X = B.X2 AND A.Y = B.Y2\\\") can impose a significant performance overhead up to SAP HANA SPS 11. In this situation you should either avoid this scenario from application side or consider an upgrade to SAP HANA >= SPS 12 where the optimized Hashed Disjunctive Join is available. Long runtime with non-unique multi-column index on join columns The SAP HANA join engine may disregard columns during a join if they are also specified as selection condition in the WHERE clause (\\\"<column> = ?\\\"). This is a typical constellation for the client column (MANDT, CLIENT, ...). As a consequence it can happen that a secondary index on this and other columns isn't used although it looks perfect. In order to avoid this scenario, you should create non-unique column store indexes as single-column indexes rather than adding columns like client providing limited benefit in terms of filtering. This issue doesn't apply to row store and unique / primary key indexes. See SAP Note 2160391 for more information related to SAP HANA indexes. Execution time higher than expected, significant portion for accessing delta storage Make sure that the auto merge mechanism is properly configured. See SAP Note 2057046 for more details. Consider smart merges controlled by the application scenario to make sure that the delta storage is minimized before critical processing starts. The join engine considers only the main storage when deciding for using an MVCC bitmap. In case of a large and volatile delta storage the generation of the delta MVCC bitmap can be quite time consuming and the following call stack modules are visible: UnifiedTable::MVCCObject::generateOLAPBitmapMVCC JoinEvaluator::JEUtils::createFullSnapshot This behavior is improved with SAP HANA >= 1.00.122.21, >= 2.00.024.07 and >= 2.00.034. Execution time slightly higher than expected In general the number of tables in the row store should be kept on a small level, but under the following circumstances it is an option to check if the performance can be optimized by moving a table to the row store: Involved table located in column store and not too large (<= 2 GB) Many records with many columns selected or a very high number of quick accesses with small result sets performed Execution time sporadically increased Check if peaks correlate to resource bottlenecks (CPU, memory, paging) and eliminate bottleneck situations. Execution time higher than expected, significant portion for sorting (trex_qo trace: doSort) Sort operations (e.g. related to ORDER BY) are particularly expensive if all of the following conditions are fulfilled: Sorting of a high number of records Sorting of more than one column Leading sort column has rather few (but more than 1) distinct values In order to optimize the sort performance you can check from an application side if you can reduce the number of records to be sorted (e.g. by adding further selection conditions) or if you can put a column with a high amount of distinct values at the beginning of the ORDER BY Increased runtime of INSERT operation on table with hybrid LOB field INSERTs in hybrid LOBs have to perform disk I/O if the configured memory threshold is exceeded and the data is stored in a disk LOB. See SAP Note 2220627 for more information related SAP HANA LOBs. In order to optimize performance you can proceed as follows: Check for disk I/O bottlenecks and eliminate them in order to optimize the I/O performance (SAP Note 1999930 ). Consider increasing the MEMORY THRESHOLD configuration for the hybrid LOB (SAP Note 1994962 ) so that more data is kept in memory. Be aware that this will increase the memory requirements of SAP HANA, so you have to find an individual trade-off between memory consumption and performance. Increased runtime of EXPORT TO DATABASE / IMPORT FROM DATABASE ABAP tables with INDX structure (e.g. INDX, BALDAT, PCL*, SOC3, SSCOOKIE) are accessed via EXPORT TO DATABASE / IMPORT FROM DATABASE commands on ABAP side. If bigger chunks of data are exported, the data is split into pieces based on the length of the CLUSTD field of the table. This can result in significant communication overhead. In this case you can reduce the amount of INSERT operations and the network overhead by increasing the length of the CLUSTD column to a larger value. Starting with ABAP SAP_BASIS 7.51 it is possible to store the whole EXPORT data in a single table line by defining the CLUSTD column with data element INDX_CLUST_BLOB and dictionary type RAWSTRING. This can reduce the amount of INSERT operations to a minimum. Be aware that simply adjusting the CLUSTD definition of existing tables is not sufficient and can result in syntax errors. Instead the table has to be set up as described in Export / import tables . In case of SAP standard tables this needs to be implemented and delivered by SAP. IMPORT FROM DATABASE based on major id and minor id may use multiple SELECTs including sorting. In case of large table sizes, this sorting overhead can become significant. SAP Note 3234023 provides a correction so that the number of records transferred as well as the sorting overhead are reduced. An optimization for UPDATE and DELETE operations of keys (SRTFD) with many record counters (SRTF2) on INDX-like tables is available in SAP Note 3320379 . Long BW query runtime Long execution time of TREXviaDBSL calls Long execution time of TREXviaDBSLWithParameter calls TREXviaDBSL are used for complex database requests like BW queries or enterprise search. See SAP Note 2800048 for more details and troubleshooting options. High load caused by queries with WHY_FOUND function Queries including WHY_FOUND function calls like SELECT WHY_FOUND() as \\\"_WHERE_FOUND\\\" ... originate from calls to the Enterprise Search (ESH) procedure ESH_SEARCH (thread method BuiltinProcedure_ESH_SEARCH) and are used to display the part of the text that is responsible for the text to be returned by the request. They can sometimes introduce significant overhead. See SAP Note 3258905 and consider deactivating the \\\"Why found\\\" functionality in case it is not really required. Long BW DTP and transformation runtime using SAP HANA execution mode See SAP Notes 2033679 (BW 7.40 SP 05 - 07) and 2067912 (BW 7.40 SP 08 - 10) in order to make sure that all recommended fixes are implemented. For example, SAP Note 2133987 provides a coding correction for SAPKW74010 in order to speed up delta extractions from a DSO. See SAP Note 2057542 and consider SAP HANA based transformations available as of BW 7.40 SP 05. Long F4 search help in BW Queries on SID tables with sub-queries on fact or DSO tables with the following structure can originate from F4 search helps with \\\"Only Posted Values for Navigation\\\" setting: SELECT TOP 1001 \\\"S0000\\\".\\\"FISCVARNT\\\" AS \\\"FISCVARNT\\\" , \\\"S0000\\\".\\\"FISCYEAR\\\" AS \\\"FISCYEAR\\\" , \\\"S0000\\\".\\\"SID\\\" AS \\\"SID\\\" FROM \\\"/BI0/SFISCYEAR\\\" \\\"S0000\\\" WHERE ( \\\"S0000\\\".\\\"SID\\\" IN ( SELECT \\\"F\\\".\\\"SID_0FISCYEAR\\\" AS \\\"SID\\\" FROM \\\"/B224/FGR_CA01\\\" \\\"F\\\" ) OR \\\"S0000\\\".\\\"SID\\\" IN ( SELECT \\\"F\\\".\\\"SID_0FISCYEAR\\\" AS \\\"SID\\\" FROM \\\"/B224/FGR_CA02\\\" \\\"F\\\" ) ) AND \\\"S0000\\\".\\\"FISCVARNT\\\" = 'V9' ORDER BY \\\"FISCYEAR\\\" ASC In this case you should check if using a less expensive F4 query execution mode can be used. See SAP Note 1565809 for more information. Performance issues in context of fulltext indexes SAP Note 2800008 -> \\\"What are typical problem scenarios in context of fulltext indexes?\\\" describes among others scenarios that can lead to performance problems in context of fuzzy searches and creating, updating or using fulltext indexes. Slow accesses to FI data sources If accesses to FI data sources (0FI_GL_10, 0FI_GL_11, 0FI_GL_12, 0FI_GL_14, 0FI_GL_20, 0FI_GL_40) are slow, you can proceed according to SAP Note 2302508 . \\ufeffLong InA / MDS accesses If access to SAP HANA views takes longer than expected in InA / MDS environments (SAP Note 2670064 ), you can check if the cache timeout for metadata is set too small (SAP Note 2559231 ) and increase the following parameter if required: indexserver.ini -> [mds] -> cache_session_ttl SAP Note 3287726 describes a potential performance decrease due to too many calculation steps with EPMMDS binaries 1.00.202221.06. Performance and resource consumption may be improved with the explicit option for using the SQL engine for MDS request processing. You can configure the related calculation view property to use the SQL engine for that purpose. See SAP Notes 2670064 , 2223597 and 2818549 for more information. Long runtimes in context of SQLScript See Are there special considerations for analyzing SQLScript procedures? for details how to analyze SQLScript procedure executions. SAP Notes 2795151 and 2902534 provide suggestions how to proceed in case of SQLScript performance regressions, e.g. by adjusting the used SQLScript version level via SQLSCRIPT_VERSION hint. Long runtimes in context of smart data access (SDA) Make sure that SDA is configured optimally for performance. See SAP Note 2180119 and in particular make sure that data statistics are created for the remote tables so that the optimizer is aware about cardinalities. Long runtimes in context of dynamic tiering See SAP Note 2733393 in order to optimize table accesses in dynamic tiering environments. Long runtimes in context of planning engine SAP Note 1637199 contains settings that can be used to activate / deactivate HANA optimized planning engine processing in BW. Long runtimes in context of S/4HANA and Fiori Normal performance tuning approaches apply for the S/4HANA / Fiori scenario. Some Fiori apps may take longer than expected due to design limitations, improvements are available with newer SAP HANA Revisions (SAP Note 2519264 ). SAP Notes 2689405 and 2916959 provide further details about performance optimizations in S/4HANA and Fiori environments. Long runtime in context of HEX engine Starting with SAP HANA 2.0 more and more SQL processing is taken over by the SAP HANA Execution Engine (HEX). Call stacks like the following indicate that HEX is used: AttributeEngine::hexLookupInvertedIndex hex::operators::ConjunctionInitOp::run hex::operators::FragmentColumnLookupInitOp hex::operators::FragmentScanInitOp::run hex::operators::ValidPullInitOp::run hex::planex::ExecutablePlan::executePipelinesUpTo hex::planex::ExecutablePlan::open hex::planex::impl::runNextImpl hex::planex::NoDataOperator::xf_run ptime::Hex_search::do_open If you aren't able to optimize the query in a different way, you can consider to disable HEX for the SQL statement in question using the NO_USE_HEX_PLAN hint (SAP Note 2142945 ) or globally with the following SAP HANA parameter (SAP Note 2570371 ): indexserver.ini -> [sql] -> hex_enabled = false The following bugs exist in context of HEX: SAP Note / issue number Details 2568333 Bad performance in context of specific query patterns 248375 Overhead due to unfavorable prefix searches on multi-column indexes, make sure that hex_min_key_parts is set according to SAP Note 2600030 to minimize risk of unfavorable prefix searches. Long runtime of FOR ALL ENTRIES query If a FOR ALL ENTRIES selection in SAP ABAP environments takes long and consumes a lot of resources, you can consider the following optimizations: Make sure that FOR ALL ENTRIES statements aren't executed when the driving table is empty because in this case all records of the current client and selection conditions aren't evaluated. You can capture these scenarios using transaction SRTCM -> \\\"Empty table in FOR ALL ENTRIES clause\\\" -> \\\"Activate Globally\\\". Make sure that fast data access (FDA) recommendations are considered (SAP Note 2399993 ). If multiple columns in the WHERE condition reference the FOR ALL ENTRIES list and a SQL statement based on OR concatenations is generated, you can use the DBSL hint dbsl_equi_join as described in SAP Notes 1622681 and 1662726 . Be aware that this option will only work if references to the FOR ALL ENTRIES list are on consecutive columns in the WHERE clause and that only \\\"=\\\" references are allowed. If these conditions are not fulfilled, a termination with a short dump will happen. In order to take optimal advantage of the dbsl_equi_join hint in BW, you have to make sure that SAP Notes 2007363 (7.40 SPS 09) and 2020193 (7.40 SPS 08) are implemented. See SAP Note 2142945 for more information regarding SAP HANA hints. Frequent queries with '/* Buffer Loading */' comment Frequent accesses to small SAP ABAP tables that are supposed to be buffered on ABAP side Check and optimize the SAP ABAP table buffer configuration (transactions ST02, ST10, AL12): Sufficient buffer size and directory entries Hit ratio of at least 99 % Limited number of swaps and invalidations No unnecessary buffering of large tables Analysis and resolution of tables with buffer state \\\"error\\\" (SAP Note 703035 ) See SAP Note 2103827 for configuring the SAP HANA table buffer. \\ufeffLong runtime of query on SAP HANA dictionary objects and monitoring views If the CATALOG READ privilege is not assigned to a user, queries on SAP HANA data dictionary objects and monitoring views like SCHEMAS, TABLES, TABLE_COLUMNS, M_TEMPORARY_TABLE_COLUMNS or M_TEMPORARY_TABLES can take much longer, because SAP HANA needs to filter the relevant (own) data and suppress the display of information from other schemas. This is done via built-in functions like ISAUTHORIZED or HASSYSTEMPRIVILEGE. In the explain plan you will see additional accesses to security related objects like: M_EFFECTIVE_PRIVILEGE_GRANTEES_ M_EFFECTIVE_PRIVILEGES_ M_EFFECTIVE_ROLES_ P_GRANTEDPRIVS_ P_PRINCIPALS_ You can use view GRANTED_PRIVILEGES or SQL: \\\"HANA_Security_GrantedRolesAndPrivileges\\\" (SAP Note 1969700 ) to check if this privilege is already granted or not. Consider granting CATALOG READ to optimize the dictionary object accesses: GRANT CATALOG READ TO <database_user> Mainly for SAP HANA <= 2.00.022 see SAP Note 2100040 (\\\"How can CPU intensive operations in SAP HANA be identified and optimized?\\\" -> \\\"__hasanyprivileges__String_BigInt_String_String\\\") and apply a sufficiently new SAP HANA Revision level or make sure that CATALOG READ is assigned (either directly or indirectly via roles) to users having to access these views. With newer SAP HANA Revision levels the call stacks can still happen, but with lower probability. Long runtime of queries on monitoring views For technical reasons accesses to monitoring views like M_TABLE_LOCATIONS or M_TABLE_PERSISTENCE_LOCATIONS often scan the complete underlying structures regardless of the WHERE clause. Thus, you should avoid frequent selections of small amounts of data (e.g. one access per table) and use fewer selections reading larger amounts of data (e.g. for all tables of a schema at once). Alternatively check if you can select the required information from other sources, e.g. monitoring view M_CS_TABLES. Similar overhead is also required for other monitoring views. The column FILTER_PUSHDOWN_TYPE in internal view SYS.M_MONITOR_COLUMNS_ provides information to what extent a column supports the pushdown of filters. Wrong join order An obviously wrong join order can be caused by problems with join statistics. Join statistics are created on the fly when two columns are joined the first time. Up to SAP HANA 1.0 SPS 08 the initially created join statistics are kept until SAP HANA is restarted. This can cause trouble if join statistics were created at a time when the involved tables had a different filling level, e.g. when they were empty. In this case you can restart the indexserver in order to make sure that new join statistics are created. Starting with 1.0 SPS 09 SAP HANA will automatically invalidate join statistics (and SQL plans) when the size of an involved table changed significantly. If you suspect problems with join statistics, you can create a join_eval trace (SAP Note 2119087 ) and check for lines like: JoinEvaluator.cpp(01987) : getJoinStatistics: SAPSR3:CRMM_BUAG (-1)/BUAG_GUID<->SAPP25:CRMM_BUAG_H (-1)/BUAG_GUID JoinEvaluator.cpp(01999) : jStats clSizesL:0 clCntAL:0 cntSJL:0 TTL:0 clSizesR:0 clCntAR:0 cntSJR:0 TTR:0 Zero values in the second line can indicate that join statistics were created at a time when one table was empty. Starting with SAP HANA 1.0 SPS 12 you can also use SQL: \\\"HANA_SQL_Statistics_JoinStatistics\\\" (SAP Note 1969700 ) to evaluate existing join statistics. See SAP Note 2800028 for more information related to SAP HANA optimizer statistics in general and join statistics in particular. High runtime (higher than usual), not reproducible in SAP HANA Studio / DBACOCKPIT If the runtime of a query is longer than expected and you can't reproduce the long runtime with SAP HANA Studio or DBACOCKPIT (if bind variables are used: using a prepared statement with proper bind values), the issue can be caused by an inadequate execution plan (e.g. generated based on the bind values of the first execution or based on statistical information collected during the first execution). In this case you can check if an invalidation of the related SQL cache entry can resolve the issue: ALTER SYSTEM RECOMPILE SQL PLAN CACHE ENTRY '<plan_id>' You can identify the plan ID related to a statement hash by executing SQL: \\\"HANA_SQL_SQLCache\\\" (STATEMENT_HASH = '<statement_hash>', AGGREGATE_BY = 'NONE', DATA_SOURCE = 'CURRENT') available via SAP Note 1969700 . Depending on the factors considered during next parsing (e.g. set of bind values, dynamic statistics information) a better execution plan may be generated. It can happen that you have to repeate the RECOMPILE command until a good set of bind values is parsed. Even if the problem is resolved after the RECOMPILE, there is no guarantee that it is permanently fixed, because after every eviction or restart a new parsing happens from scratch. If the problem is supposed to be linked to bind values, you can consider to adjust the application so that different classes of bind values are executed with slightly different SQL statements, so that SAP HANA can parse each statement individually. Due to issue number 245677 it can happen with SAP HANA <= 2.00.048.02 and <= 2.00.052 that in case of parallel compilations of the same statement a limited pre-compiled plan is used for execution. Performing a RECOMPILE can solve an existing issue. As a workaround to avoid this issue to happen you can set the following parameter (attention: can have adverse effects on other statements, so should be implemented with care): indexserver.ini -> [sql] -> recompile_with_param = false If only certain SQL statements suffer, you can use the NO_RECOMPILE_WITH_SQL_PARAMETERS hint (SAP Note 2142945 ) on statement level. As of SAP HANA 1.0 SPS 09 you can also think about the IGNORE_PLAN_CACHE hint as a last resort (see SAP Note 2142945 ). Be aware that this will make the performance more predictable, but due to the permanent parsing requirements the quick executions can significantly slow down. So it should only be used in exceptional cases. Long preparation times See SAP Note 2124112 and check if the amount and duration of preparations can be reduced (e.g. by increasing the SQL cache or using bind variables) or if there are other ways to improve the parsing behavior of a specific query. High runtime with range condition on multi column index Range conditions like BETWEEN, \\\"<\\\", \\\">\\\", \\\">=\\\", \\\"<=\\\" OR LIKE can't be used in order to restrict the search in a multi column index. As a consequence the performance can be significantly worse than expected. Possible solutions are: Use a single column index rather than a multi column index if possible. Use \\\"=\\\" or \\\"IN\\\" instead of a range condition. High runtime scanning indexed column with SPARSE or PREFIXED compression As described in SAP Note 2112604 (\\\"What do I have to take into account in order to make sure that the tables are compressed optimally?\\\"), there can be different reasons why scanning indexed columns with SPARSE or PREFIXED compression doesn't happen efficiently, e.g. no index support or overhead due to bugs. Follow the instructions in SAP Note 2112604 (e.g. new optimize compression run or switch to DEFAULT compression) in order to optimize the behavior. High runtime scanning index column with advanced compression and FULL index type Starting with SAP HANA 1.00.122.10 and 2.00.002.01 SAP HANA may use indexes with type FULL on compressed columns. With SAP HANA 1.00.122.10 to 1.00.122.11, 2.00.002.01 to 2.00.012.01 and 2.00.020 this can cause a performance overhead in context of joins. See SAP Note 2516807 for more information. High runtime with multiple OR concatenated ranges on indexed column Due to a design limitation with SPS <= 100 SAP HANA doesn't use an index on an advanced compressed column if multiple OR concatenated range conditions exist on the same column. As a consequence statements like SELECT ... FROM COSP WHERE ... AND ( OBJNR BETWEEN ? AND ? OR OBJNR BETWEEN ? AND ? ) can have a long runtime and high resource consumption. As a workaround you can only use DEFAULT compression for the table. See SAP Note 2112604 for more information. High runtime with long OR concatentations If search terms with particularly many OR concatenations suffer from long runtimes, it may be possible to improve the cardinality estimation and the execution plan by optimizing the predicate term sampling with SAP HANA >= 2.00.046. See SAP Note 2124112 -> \\\"What kind of advanced parsing features exist?\\\" -> \\\"Predicate term sampling\\\" for details. High runtime with EXISTS Although EXISTS is a semi-join that can be finished as soon as the first record is found in the subquery, there are situations where SAP HANA reads the complete result set, resulting on increased runtimes (see e.g. statement hash 6805026a381879e9e5469be3f09cc654 below). In this case you can either consider to rewrite the application coding avoiding EXISTS or you can upgrade to SAP HANA >= 1.00.122.13, >= 2.00.012.02, >= 2.00.021 or >= 2.00.030 where the behavior is optimized. \\ufeffHigh runtime with multiple EXISTS in combination with OR Up to SAP HANA Rev. 1.00.110 multiple EXISTS semi-joins are not evaluated optimally if combined with OR. As a workaround you can check if OR can be transformed into UNION. As a permanent solution you can use SAP HANA Rev. >= 1.00.111. High runtime, expected single column index doesn't exist Whenever a table column is part of a primary key or unique index, an implicit single column index structure is created. Exceptions and solutions are described in SAP Note 2160391 (\\\"What are BLOCK and FULL indexes?\\\" -> Index type = 'NONE' -> Column indexed = 'X'). You can use SQL: \\\"HANA_Indexes_ColumnStore_MissingSingleColumnIndexes\\\" (SAP Note 1969700 ) to display columns without the expected single column index. \\ufeffHigh runtime with TOP / LIMIT SAP HANA often performs a significant amount of operations on the overall data set before finally returning the first records. Therefore, you should consider the following options: Upgrade to SAP HANA >= 2.00.072 when the HEX engine (SAP Note 2570371 ) is able to evaluate TOP / LIMIT conditions already after the first 255 result records. A USE_HEX_PLAN hint (SAP Note 2142945 ) may be required to force the usage of the HEX engine. With SAP HANA 2.00.070 - 2.00.071 an issue with the HEX engine results in unexpected overhead valuating TOP / LIMIT conditions (SAP Note 3369534 ). A NO_USE_HEX_PLAN hint (SAP Note 2142945 ) may be required as a workaround in this case. Provide selective conditions in the WHERE clause so that the amount of data is limited. Avoid TOP / LIMIT restrictions in unselective joins. Optimize the join processing, e.g., by defining optimal indexes on the join columns. Inner joins suffer most from the late limit evaluation. Check if an inner join can be replaced by OUTER JOIN [MANY TO ONE]. \\ufeffHigh runtime with ORDER BY and TOP / LIMIT As described in Can sorting be supported by an appropriate index? , SAP HANA indexes don't support sorting. Therefore, an ORDER BY requires explicit sorting, even if an index on the related column(s) exists. In cases where a high amount of records need to be sorted before the first few records are returned, the runtime can be rather high. In general you can consider the following adjustments: Check from application perspective if the ORDER BY is really required and avoid it if possible. Check if you can specify more selective conditions so that the amount of sorted records is reduced. Issues when bind variables are present In general it is recommended and useful to use bind variables for literals in order to keep the amount of database requests in the SQL cache and the parsing activity at a reasonable level (SAP Note 2124112 ). In some cases bind variables can have a negative impact: SAP HANA 1.0 has some internal restrictions handling bind variables in context of TOP and LIMIT, therefore this combination should be avoided. SAP ABAP kernel 7.49 uses \\\"LIMIT ?\\\", so when upgrading to this kernel level, the SAP HANA performance and resource consumption should be carefully tested beforehand. In order to avoid \\\"LIMIT ?\\\" you can remove UP TO <n> ROWS on ABAP side if possible. SAP Note 2522456 provides a related correction for method CL_CRM_REPORT_ACC_DYNAMIC in CRM environments. Consider hints like LIMIT_THRU_JOIN and PRELIMIT_BEFORE_JOIN (SAP Note 2142945 ) in case the result of a large join is limited significantly. See SAP Notes 2793263 and 2900345 for using these hints with ABAP transaction SE16 and SE16N that also usually use a restrictive limitation. SAP Note 2795151 describes SQLScript performance regressions with SAP HANA 2.0 because bind variables may introduce additional complexity. You can use the BIND_AS_VALUE function to make sure that a literal is not replaced with a bind variable. SAP Note 2891894 describes a problem where SDA (SAP Note 2180119 ) can't use bind variables properly in context of remote table accesses with SAP HANA 2.00.037.02 - 2.00.037.05. As a workaround you can use the BIND_AS_VALUES function (SQLScript) or explicitly replace the bind variables with literals (SQL). High runtime with TOP or LIMIT in combination with unselective conditions SAP HANA typically processes all records fitting to the available conditions before applying a TOP or LIMIT restriction and returning only the specified number of records. In order to avoid unnecessary overhead you should avoid frequent TOP or LIMIT selections on larger tables with unselective conditions. \\ufeffHigh runtime of MIN and MAX searches Indexes in SAP HANA can be used to support MIN / MAX searches if all of the following conditions are met: SAP HANA >= 2.0 SPS 04 Column store table Number column or character column without mixture of upper / lower case letters Be aware that currently other predicates can't be evaluated as part of the index scan before applying MIN / MAX. In all other scenarios an index can't be used to identify the maximum or minimum value of a column directly. Instead the whole column / table has to be scanned. Therefore you avoid frequent MAX or MIN searches on a large data volume. Possible alternatives are: Definition of an ABAP number range object Sequences (SAP Note 2600095 ) Identities (GENERATE [BY DEFAULT] AS IDENTITY, CURRENT_IDENTITY_VALUE()) Additional selective conditions Maintaining the MIN and MAX values independently in a separate table In most cases the column store provides better performance of MIN / MAX searches compared to row store, so you can consider a move to column store (SAP Note 2222277 ) in cases when the performance is linked to a row store table. High runtime when LIKE condition with leading place holder is evaluated The evaluation of a LIKE condition with a leading place holder (e.g. '%1234') can consume significant time on large tables. The related call stack typically contains modules like AttributeEngine::PatternMatching::ScanJob, AttributeEngine::RoDict::_getNextPattern, AttributeEngine::RoDictDefaultPages::getNext or TRexUtils::WildcardPattern::match. The thread method is often PatternMatching::ScanJob or DictScanJob. The runtime can be high even if the result set was already reduced to a small amount of records (> 0) before evaluating the LIKE condition. An evaluation of the LIKE predicate based on the reduced result set depends on the following factors: It only works for a single LIKE condition, so e.g. \\\"<column> LIKE '%abc%' OR <column> LIKE '%def%'\\\" will not take advantage. It only works if the distinct values of the column exceeds the limit defined in parameter indexserver.ini -> [evaluator_redirect] -> dict_size_main (default: 2000000). Consider reducing this parameter to smaller values if also columns with a smaller amount of distinct values suffer. It only works if the number of result records from other conditions is below the limit defined by parameter indexserver.ini -> [evaluator_redirect] -> match_rows_main (default: 2500). If the result records are above 2500 and you still want the LIKE to be evaluated based on this larger result, you can increase the match_rows_main parameter sufficiently. Be aware that evaluating the LIKE based on a rather large number of result records can become more and more inefficient. Starting with SAP HANA 2.0 SPS 06 the ABAPVARCHARMODE (SAP Note 2262114 ) is considered in context of LIKE (issue number 267338). This can result in regressions with SAP HANA 2.00.060 - 2.00.063 due to evaluation overhead, even for non-restrictive \\\"LIKE '%'\\\" conditions (issue number 291148). Setting indexserver.ini -> [search] -> sql_like_pushdown = false or deactivating the ABAPVARCHARMODE can be a workaround for critical scenarios. With SAP HANA >= 2.00.059.03 and >= 2.00.063 further optimizations are available to suppress expensive dictionary scans in specific scenarios (issue number 285294). High runtime with LIKE condition, bind variables and '*' or '?' as part of the bind value '*' and '?' are no wild cards for LIKE conditions but due to some internal transformations they can have a negative impact on the evaluation of LIKE conditions with SAP HANA <= 2.00.055 (issue number 241426). Therefore you should avoid these characters in LIKE values whenever possible. If the problem happens on table STXH, SAP Note 2208025 is responsible where explicitly a '*' is added to the TDNAME value. A correction is available via SAP Note 2302627 . With SAP HANA >= 2.00.056 the special characters '*' and '?' are handled properly. High runtime of range condition evaluation, even if result set is already restricted SAP HANA tends to evaluate range conditions globally, even if the already analyzed predicates have reduced the result set significantly. Starting with SAP HANA Rev. 1.00.112.02 this behavior is optimized. In certain cases you can use LIKE_REGEXPR (with the appropriate search pattern) instead of LIKE as a workaround. This should be tested thoroughly because LIKE_REGEXPR can also impose overhead in other scenarios. High runtime of LIKE conditions with ABAP accessing remote sources Currently, the push down of LIKE predicates to SDA remote sources (SAP Note 2180119 ) is not supported if the session variable ABAPVARCHARMODE is set to TRUE (issue number 276074). This is the default setting in ABAP contexts. Check whether the LIKE condition can be avoided or alternatively, execute the statement via a secondary database connection for which abapVarcharMode=false was set via DBCO. High runtime of COUNT DISTINCT If SAP HANA 1.0 <= SPS 08 is used and a COUNT DISTINCT is executed on a column with a high amount of distinct values, a rather larger internal data structure is created regardless of the actual number of records that have to be processed. This can significantly increase the processing time for COUNT DISTINCT. As a workaround you can check if the SQL statement is processed more efficiently using the OLAP engine by using the USE_OLAP_PLAN hint (SAP Note 2142945 ). As a permanent solution you have to upgrade to SPS 09 or higher, so that the size of the internal data structure takes the amount of processed records into account. Also with later SAP HANA 1.0 Revisions the COUNT DISTINCT can take a long time and consume significant space in allocator Pool/JoinEvaluator/DictsAndDocs (SAP Note 1999997 ) if it is used on a large partitioned table and the join engine is implicitly used. In this case you can check if the USE_OLAP_PLAN hint (SAP Note 2142945 ) can improve the situation. This problem is fixed with SAP HANA 2.0. See SAP Note 2396894 for more details. High runtime of DISTINCT In case of a larger data volume it is normal that DISTINCT takes some time. The fact that the column dictionary of column store tables already contains the distinct values of a column can't be used as a shortcut because visibility information needs to be evaluated for every record. Thus, a DISTINCT on a column with only one distinct value can take a significant time in case the table has many records. High load of COUNT Counting records based on a large table or a complex join can be quite expensive, so preferrably you avoid the COUNT operation in these contexts. For existence check, for example, a SELECT TOP 1 would be already sufficient and it is not required to count everything. If the COUNT is executed in context of OData calls using the OData V2 Data Model , the OData Count Mode may be responsible. In this case you can switch to sap.ui.model.odata.CountMode.None in order to suppress the execution of the counting. A COUNT in combination with FOR ALL ENTRIES is executed as SELECT DISTINCT. See SELECT COUNT and avoid SELECT COUNT in combination with FOR ALL ENTRIES and a potentially high number of matching records. Long runtime of NOT IN join The evaluation of NOT IN join conditions can be much more expensive than NOT EXISTS or EXCEPT. Check if it is possible to use NOT EXISTS or EXCEPT instead (SAP Note 3125731 ). High mass UPDATE runtime Updating a high amount of records in a single command (e.g. \\\"UPDATE ... FROM TABLE\\\" in ABAP systems) is more efficient than performing individual UPDATEs for each record, but it still can consume significant time. A special UPDATE performance optimization is available when the UPDATE is based on a primary key. So if you suffer from long mass UPDATE runtimes you can check if you can implement an appropriate primary key. A unique index is not sufficient for this purpose, a real primary key constraint is needed. Increased UPDATE / DELETE runtimes SAP Notes 2351294 and 2823243 describe problems with the new update engine implementation that can result in performance issues on SAP HANA 2.00.030 - 2.00.037.02 and and 2.00.040 - 2.00.041. As a workaround the multistore_feature_toggle parameter can be adjusted. High runtime of anti joins Anti joins (EXCEPT, subquery with NOT) are often more performance critical than normal joins. The following special situations exist where particularly bad anti join performance can be observed: Long runtimes of database requests with anti joins (e.g. EXCEPT) and call stacks in JoinEvaluator::LoopJob::findJoinPairsTL_native can be caused by a SAP HANA bug that is fixed with Rev. 1.00.122.12 and 2.00.010. With SAP HANA >= 2.0 SPS 02 the fix is enabled per default. With earlier versions the fix is disabled per default and can be activated with hint CONSERVATIVE_CS_ANTI_JOIN_ESTIMATION or globally with the following parameter: indexserver.ini -> [sql] -> conservative_cs_anti_join_estimation_enabled = true As a workaround the NO_GROUPING_SIMPLIFICATION hint (SAP Note 2142945 ) can be used. If triggered by BW / MDX, you can also disable the RSADMIN parameter MDX_F4_USE_SQL (SAP Note 1865554 ). High runtime of cyclic joins If a statement with a cyclic join (\\\"[CYCLIC] JOIN CONDITION\\\" in explain plan) can't be optimized differently you can check if disabling cyclic joins improves the situation (as a temporary workaround) by using the NO_CYCLIC_JOIN hint on statement level (SAP Note 2142945 ). Thread method ParallelLoopWithEqJob (SAP Note 2114710 ) and call stack module JoinEvaluator::LoopWithEqJob::recurseWithEq point toward cyclic joins. In some cases semi-join reductions are the actual bottleneck. In this case you won't find ParallelLoopWithEqJob but other more common thread methods like ParallelRadixSort (that can also happen in different contexts). High runtime of certain queries with Rev. 90 to 97.01 in UnifiedTable::MVCCObject coding Similar to the TOP 1 issue above also other queries can suffer from long runtimes in modules like UnifiedTable::MVCCObject::isTSBlockGarbageCollected or UnifiedTable::MVCCObject::generateOLAPBitmapMVCC. One main root cause is fixed as of Rev. 97.02, so an upgrade can be considered in case the performance seriously impacts production operation. Sporadically increased runtimes of calculation scenario accesses If accesses to calculation scenarios are sometimes slow, cache displacements can be responsible. You can use SQL: \\\"HANA_Configuration_MiniChecks\\\" (SAP Note 1999993 , check ID 460) and SQL: \\\"HANA_CalculationEngine_CalculationScenarios\\\" (SAP Note 1969700 ) to find out more. If the cache is undersized, you can increase it using the following parameter (default: 1048576): indexserver.ini -> [calcengine] -> max_cache_size_kb = <size_kb> See also SAP Note 1988050 for specific problems with SAP HANA <= Rev. 73. Increased runtime of calculation view accesses In case of slow and resource-demanding accesses to a calculation view or analytic view you can check the following aspects: See the SAP HANA Modeling Guide and make sure that modeling best practices are used. Adjust the view definition if required. See SAP Notes 2291812 and 2223597 and check if adjusting the execution engine helps to improve the performance. For SQL statements on these views the following best practices should be considered: Avoid data type conversions (implicit, explicit type case, CAST) in join definitions and WHERE clauses Avoid joins on calculated columns and calculations in WHERE clauses Avoid non equi join definitions Avoid joining big analytic views of calculation views, instead use UNION Use UNION (with constants) to combine large data sets Minimize the use of expensive calculations, row based expressions and data manipulation including calculated attributes Make sure, e.g. using PlanViz, that push-down works for selective predicates. If not, you can adjust the view or open a SAP case on component HAN-DB for clarification. If you use a composite provider / stacked calculation view on top of a scripted calculation view predicate push down may be impacted in case of different users. So you should either make sure that the same user is used for both or that the scripted calculation view is defined with \\\"Invoker\\\" mode. If you observe an increased number of accesses to table RS2HANA_AUTH_FIL in BW environments, it is linked to a SELECT based analytic privilege check, see BW2HANA Authorization Generation and SAP Note 2604161 for details. You can check if using an alternative approach provides better performance. See SAP Note 2500573 for more details about column pruning limitations that can negatively impact performance and memory consumption. Be aware that data preview functionalities in tools like SAP HANA Studio or SAP HANA Cockpit can be expensive in terms of runtime, CPU and memory consumption. See SAP Note 1894854 for more information and avoid generic data preview operations whenever possible. High runtime when calling procedure / user defined function (UDF) or table user defined function (TUDF) Check from an application perspective if the implementation of the procedure / UDF is reasonable and optimal. If the implementation is already optimal, you need to perform a more detailed technical performance analysis. The following scenarios can be responsible for increased runtimes: When a procedure call (e.g. AMDP) takes a long time and / or consumes a lot of resources you can check if the deactivation of inlining with the NO_INLINE hint (SAP Note 2142945 ) helps to improve performance by reducing complexity. Starting with SAP HANA 2.00.037.00 unfolding may no longer work in context of SQL SECURITY INVOKER. As a workaround you can consider removing this setting until a better solution is found. See SAP Note 2847558 for more details. With SAP HANA <= 2.0 SPS 03 WITH clauses are not optimally evaluated inside a TUDF (SAP Note 2909860 ) - consider upgrading to SAP HANA >= 2.0 SPS 04 or avoid using WITH inside a TUDF. High runtime of joins in scale-out environments Make sure that tables involved in critical joins are located on the same host whenever possible, so that unnecessary communication overhead between SAP HANA nodes can be avoided. High runtime of joins when different data types are involved Joining columns with different data types (see e.g. \\\"BUT000, CRMD_PARTNER\\\" section in the SQL statement overview below) can impose significant overhead (e.g. parallel CalculationJob activities). In general you should avoid to join columns with different data types. If this is not possible, you can consider to include an explicit data type conversion, e.g. via HEXTOBIN function. High runtime of multi-column joins on three or more tables Due to a bug SAP HANA doesn't evaluate filter conditions efficiently if three or more tables are joined on more than one column. See SAP Note 2311087 and consider upgrading to SAP HANA >= 1.00.102.06 or >= 1.00.112.03. \\ufeffHigh runtime in context of self-join If the value of two columns of the same table is compared (e.g. \\\"MENGE\\\" > \\\"WAMNG\\\"), older Revisions of SAP HANA aren't able to consider a previously reduced result set and always work on the complete data. Starting with SAP HANA Rev. 1.00.112.04 and 1.00.122.00 this behavior is optimized resulting in better performance. High runtime accessing row store tables A unexpected high runtime on row store tables can be caused by the following reasons: High number of versions, e.g. due do a blocked garbage collection. See SAP Note 2169283 for more information about SAP HANA garbage collection. Full table scans (call stack module ptime::Table_scan::do_fetch) can slow down due to the row store memory leak (SAP Note 2362759 ) with SAP HANA Rev. 111 to 112.05 and 120 to 122.01. A SAP HANA restart (even without row store reorganization) optimizes the performance again. High runtime when using fast data access Starting with SAP ABAP kernel 7.45 fast data access (FDA, SAP Note 2399993 ) is activated per default. As a consequence you can see database statements originating from FOR ALL ENTRIES queries on ABAP side that look like the following pattern: SELECT /* FDA WRITE */ DISTINCT ... FROM ... ? AS \\\"t_00\\\" ... The following general options exist to improve these kinds of queries: Action Global deactivation of FDA for FOR ALL ENTRIES Statement specific deactivation of FDA for FOR ALL ENTRIES Statement specific adjustment of SAP HANA execution plan via hint Upgrade to more recent SAP HANA Revision Bad performance on specific row store table, unexpected UNION ALL in execution plan For some reasons (e.g. when a column is added) a row store table can consist out of more than one underlying container. As a consequence, SAP HANA needs to combine the results from several containers using UNION ALL. Existing indexes may only work for a subset of the containers and so they aren't used in the most efficient way. In order to check for the number of containers and generate cleanup commands, you can use SQL: \\\"HANA_Tables_RowStore_TablesWithMultipleContainers\\\" (SAP Note 1969700 ). A table can be merged into a single container by reorganizing it with the following command: ALTER TABLE \\\"<table_name>\\\" RECLAIM DATA SPACE Be aware that this activity requires a table lock and so concurrent accesses to the same table may be blocked. Therefore it is recommended to perform it during a time of reduced workload. Furthermore you can set a low lock wait timeout on transaction level (e.g. 10000 ms) in order to reduce the risk of long-term lock escalations: SET TRANSACTION LOCK WAIT TIMEOUT <timeout_in_ms> Bad performance in context of UNION ALL If a database access including a UNION ALL (e.g. a compatibility view access) takes a long time, it is worth to check if disabling the column store UNION ALL operation with parameter NO_CS_UNION_ALL has a positive effect. See SAP Note 2142945 for more information related to SAP HANA hints. This can at least be a workaround before a final fix is found. Long runtime of compatibility view accesses See question \\\" How can compatibility view accesses be tuned? \\\" below for details. Long runtime of CDS view accesses See question \\\" How can core data services (CDS) view accesses be optimized? \\\" below for details. Increased runtime due to unnecessary distributed execution In the following cases it is possible to transform a distributed query (i.e. involving more than one SAP HANA node in scale-out scenarios) into a typically more efficient local query: Make sure that statement routing is used (SAP Note 2200772 ). Otherwise a query may be executed on a node that is different from the table location. In extreme cases this can result in performance regressions of factor 10. Check if you can locate all tables accessed in a join on the same SAP HANA node. Be aware that in BW tables are on purpose partitioned and distributed across different SAP HANA nodes, so local executions are often neither desired nor possible. High runtime of CREATE VIEW commands A long runtime of CREATE VIEW commands (e.g. in method drRecreateSecondarySchemas for view M_CONTEXT_MEMORY in schema _SYS_SR_SITE_<site_name>) can be caused by unnecessary statistics collection. The call stack typically contains: Diagnose::TypedStatisticsWrapper Diagnose::StatisticsWrapper::traverseNodesRecursive TypedStatisticsWrapper__M_CONTEXT_MEMORY::traverseNodesImpl ptime::StatisticsMonitorHandle::getRowCountEstimation ptime::qo_size_estimation::getMonitorViewRowCount ptime::qo_size_estimation::fetch_all_histogram This problem is fixed with SAP HANA Rev. >= 1.00.122.09. High runtime of COMMIT operations If commits take in average more than a few ms, this is an indication for a bottleneck. See SAP Note 2000000 (\\\"How can the performance of commit operations be optimized?\\\") for more information. Many FDA queries in thread method CloseStatement and status Running or \\\"Network Poll\\\" If you see a high number of fast data access (FDA) requests (SAP Note 2399993 ) in method CloseStatement, the problem is usually linked to a cleanup of no longer required temporary tables (call stack module: ptime::TrexMD::deleteIndex). You can consider the following optimizations: Make sure that there is no overload situation on the master node handling the metadata information. Make sure that repository activations (e.g. via procedure REPOSITORY_REST) are performed during non-critical time frames, because they can be responsible for significant bottlenecks on the master node. Upgrade to SAP HANA >= 1.00.122.14 where the metadata request during CloseStatement of FDA queries is no longer required. If all other options aren't sufficient, consider to disable FDA READ and / or FDA WRITE as described in SAP Note 2399993 . Long database access time during SUM upgrades The following reasons exist that can be responsible for unnecessary long database request times during SUM updates: Processing of many small packages of table rows in phase RUN_FDCT_TRANSFER due to inadequate row length calculation: Upgrade to SUM SP 19 or higher where an improved row length calculation is implemented. Long runtime due to cross-node joins In scale-out scenarios it is important to distribute tables optimally to the different nodes (SAP Note 2081591 ) and take advantage of table replication (SAP Note 2340450 ) if required. In BW environments the standard distribution mechanisms should already take care for a good distribution of tables. In other environments you can use the Table Group Advisor of SAP HANA Cockpit (SAP Note 2800006 ) or SQL: \\\"HANA_SQL_SQLCache_CrossNodeJoins\\\" (SAP Note 1969700 ) in order to check for expensive cross-node joins. Be aware that these evaluations are based on the SQL cache that may show tables located on several nodes while the final plan may only use tables on one node, so in some cases cross-node joins may be reported erroneously. Long runtime due to outer join While outer joins are not generally an issue, there is a tendency that legacy engines like the join engine have some restrictions in processing outer joins, for example cyclic join graphs containing outer joins. If you face performance issues related to outer joins, check the following options: Replace outer join with inner join if possible from a business perspective (like e.g. done in SAP Note 3335213 ) Replace outer join with sub-query Check if problem can be fixed using the HEX engine (SAP Note 2570371 ) that has no specific issue with outer join processing Unexplainable long runtime If you experience a runtime of a SQL statement that is much higher than expected, but none of the above scenarios apply, a few more general options are left. See SAP Note 2142945 and test if the performance of SQL statements improves with certain hints (e.g. USE_OLAP_PLAN, NO_USE_OLAP_PLAN), because this can provide you with more ideas for workarounds and underlying root causes. Check if SAP Notes exist that describe a performance bug for similar scenarios. Check if the problem remains after having implemented a recent SAP HANA patch level. Open a SAP case on component HAN-DB in order to get assistance from SAP support. \\ufeff11. Are secondary indexes required to provide an optimal performance? SAP HANA is able to process data efficiently so that often a good performance is possible even without the use of indexes. In case of frequent, selective accesses to large tables it is nevertheless useful to create additional secondary indexes. As a rule of thumb column store indexes should be created on single columns whenever possible, because single column indexes require much less memory compared to multi column indexes. See SAP Note 2160391 for more details. \\ufeff12. Which advanced features exist for tuning SQL statements? The following advanced hint features exist to optimize SQL statements: Feature SAP Note Available with Details Hints 2142945 If the long runtime is caused by inadequate execution plans, you can influence them by using SAP HANA hints. SQL plan pinning 2222321 >= 1.00.110 Starting with SAP HANA 1.0 SPS 11 it is possible to permanently pin SQL plans with hints (based on PLAN_ID). Statement hints 2400006 >= 1.00.122.03 Starting with SAP HANA 1.00.122.03 it is possible to globally assign hints to specific SQL statements (based on statement text). Abstract SQL plans 2799998 >= 2.00.024.01 Abstract SQL plans can be used to capture and apply / freeze good existing execution plans to database requests, avoiding regressions due to changed optimizer decisions. The following features exist that can be used to cache results of frequently executed queries in order to avoid repeated expensive joins, searches and aggregations: Feature SAP Note Available with Preferred contexts Query result cache 2014148 >= 1.00.60 No aggregation Aggregation, transactional consistency required, multi table query Static result cache 2336344 >= 1.00.110 Aggregation, no transactional consistency required Dynamic result cache 2506811 >= 2.00.020 Aggregation, transactional consistency required, single table query Furthermore it is possible to influence the optimizer decision by making adjustments to the optimizer statistics collection (SAP Note 2800028 ). \\ufeff13. Are there standard recommendations for specific SQL statements available? In general it is hard to provide standard recommendations because optimizations often depend on the situation on the individual system. Nevertheless there are some exceptions where the underlying root cause is of a general nature. These SQL statements are collected in the following table: Statement hash Type Accessed objects Details 1d0ba376e273b9d622641124d8c59264 COMMIT For optimizing commit operations see SAP Note 2000000 -> \\\"How can the performance of commit operations be optimized?\\\". In general runtimes up to 10 ms per commit can be considered as acceptable. various SELECT A<nnn> Condition tables with names starting with \\\"A\\\" followed by three digits (e.g. A580) can show up with expensive accesses for different reasons: Per default they are buffered on ABAP side. In case of ABAP table buffer size limitations or many changes expensive reloads may happen. You can check for details in ABAP transaction ST10 and consider adjusting the table buffer configuration or unbuffering critical, large condition tables. As an advanced option you can consider to switch from full buffering to an appropriate generic key buffering. Expensive queries with \\\"TOP 1\\\" or \\\"LIMIT 1\\\" are usually linked to a prestep access that is used to avoid unnecessary subsequent accesses to the table. Depending on the table and application background it is an individual decision if the prestep is activated or not. See SAP Note 1738398 for further details and consider a deactivation of the prestep to see if it overall improves the performance. 4e498f19bb992a89791e1e96d9c2e1c8 GRANT ABAP_SYS_REPO TO _SYS_REPO Long runtimes of this GRANT are linked to a SAP HANA bug that is fixed with SAP HANA Rev. 1.00.112.06 and 1.00.120. See SAP Note 2386290 for more information. 000f40538b11e751f072794c5d86dfa7 SELECT ACDOCA A SELECT COUNT on ACDOCA in context of FOR ALL ENTRIES originating from report CL_SUS_IMP_CUP_B_FIN_ACDOCA001CP and batch job SAP_COLLECTOR_FOR_PERFMONITOR suffers from the fact that a COUNT in context of FOR ALL ENTRIES is executed on ABAP side after all data has been retrieved from SAP HANA, see SELECT COUNT for more details. SAP Note 2942858 provides a correction that among others fixes this problem. 6d0bbcc90f4fddebe3cd16b07b6e3519 SELECT ACDOCA This query can be expensive due to the USE_OLAP_PLAN hint. SAP Note 2465294 is available to remove the hints on ABAP side. d4e254ec9866d451822a78b791c36e16 SELECT ACDOCA This query can be expensive because of a bug in HEX engine that is fixed with SAP HANA >= 2.00.024.00 (SAP Note 2568333 ). Be aware that the HEX engine should be generally disabled (hex_enabled = false) with SAP HANA 2.00.020 - 2.00.024.03 and 2.00.030 - 2.00.031 (SAP Note 2600030 ). SELECT ACTIVITY_OPTION Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 6413f16e8a2709f7cd094042db84e2a5 SELECT ADRV This query is used in context of program S3VBRKWRS (S/4HANA archiving). SAP HANA may decide to access the table with column APPL_KEY because it typically has the highest number of distinct values. Usually the primary key column ADDRNUMBER would be more efficient because these accesses are supported by an index. You can create an index on column APPL_KEY in order to optimize this access. 2e9d6d1d68ac89c78a3dba5e532ca308 48482a15fb005f2ef288ef198c0b1275 9aaf8105bdad5308ab706eea28078d28 a35caae16f2fa43033c8c4f278483d0c e5332b10f3a1a4215728857efc0f8eda f9468e4f53d23d0dd90230463b544c3c CALL ALERT_BACKUP_LONG_LOG_BACKUP These calls are issued by the SAP HANA statistics server (SAP Note 2147247 ). In rather idle systems it is normal that these statistics server operations appear at the top of the SQL cache and runtimes up to 300 ms / execution are normal and acceptable. In this case no action is required. Usually a long runtime is caused by a large backup catalog. See SAP Note 2147247 (\\\"How can the runtime and CPU requirements of the statistics server actions be analyzed and optimized?\\\") and check if the backup catalog can be reduced. Alternatively you can increase the execution interval. various, e.g.: 4ba302a20772e66ff21abe681f4b0861 CALL SELECT ALERT_MON_COUNT_UNLOADS M_CS_UNLOADS The monitoring view M_CS_UNLOADS contains information about SAP HANA column unloads and is based on unload trace files (SAP Note 2127458 ). Per default up to 10 trace files \\u00e1 10 MB are retained per host and service, so the amount of scanned files can be significant. The following options exist to improve accesses to M_CS_UNLOADS: Upgrade to SAP HANA >= 1.00.122.12, >= 2.00.001, >= 2.00.012.02, >= 2.00.021 in order to take advantage of trace file pruning, i.e. scanning only the trace files that fit to the selected time frame. Delete unload trace files, e.g. manually, using ALTER SYSTEM REMOVE TRACES or ALTER SYSTEM CLEAR TRACES or using the related trace cleanup options of SAP HANACleaner (SAP Note 2399996 ). Reduce indexserver.ini -> [unload_trace] -> maxfiles from 10 to a smaller value (e.g. 3) so that less concurrent unload trace files are retained. The older unload trace files (indexserver_<host>.<port>.unloads.<counter>.trc) aren't automatically purged, so you have to delete them manually so that only the three most current are left. If the M_CS_UNLOADS query is linked to statisticsserver calls (SAP Note 2147247 ), an upgrade to SAP HANA >= 2.00.024.03 or >= 2.00.031 can improve the scenario because then the statisticsserver will query only most recent unloads and not all available historic unloads. 430c496e0fe15c0353c80de1c72caab1 CALL ALERT_MON_PART_TABLE_SIZE_HOST_TOTAL_MEM This procedure is used for statistics server alert 40 (\\\"Total memory usage of column-store tables\\\"). Runtimes up to 10 seconds for large systems are normal and acceptable. In rare cases it can be helpful to reduce the execution frequency. See SAP Note 2147247 for more information about the SAP HANA statistics servers in general and statistics server alerts in particular. c0f43e5dbdfc438b86964acb0a22c05f various DELETE CALL HELPER_ALERT_MON_PYTHONTRACE_ACTIVE_AGE ALERT_MON_PYTHONTRACE_ACTIVE This statistics server check accesses the file-based M_INIFILE_CONTENTS view among others. As a consequence its runtime can suffer from disk I/O problems. Check if there are times of significantly increased execution times. If yes, analyze the disk I/O performance at this time (SAP Note 1999930 ). In case of correlated I/O problems it is likely that they are responsible for the long runtime of ALERT_MON_PYTHONTRACE_ACTIVE. 3ba27a32c12ccbed76fe54c8c0ad1a4d CALL ALERT_SHM_USED_SIZE These calls are issued by the SAP HANA statistics server (SAP Note 2147247 ). In rather idle systems it is normal that these statistics server operations appear at the top of the SQL cache and runtimes up to 300 ms / execution are normal and acceptable. In this case no action is required. In case of higher runtime a performance analysis and / or a reduction of the execution frequency may be useful (ID 12). 8813754d5ca388b8481e90e877751d43 SELECT ALTIDTOOL This table contains tool assignments to CCMS monitoring tasks. The selection in function module SALU_SAPDEFAULT_VERSION_CREATE can be expensive in case of a high number of records in the table. In this case you should check in transaction RZ20 if an unnecessary high amount of monitoring is configured, e.g. individual monitoring for tens of thousands of different batch jobs. SELECT SELECT SELECT SELECT ANEA ANEK ANEP ANLC If these tables are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 6fc02f44c1b21c0a4e288b48d4c5ee61 SELECT APB_LPD_OTR_KEYS This selection with ROLE, INSTANCE and TEXT_ID filters originates from method SELECT_OTR_TEXTS of class CL_APB_LAUNCHPAD. It is used to retrieve keys for ABAP report launchpads. In order to eliminate the database table accesses you can activate shared memory buffering for the roles / instances in question: Start transaction LPD_CUST Double-click on role / instance to open the launchpad Choose \\\"Extras -> General settings\\\" from menu Set the flag \\\"Use shared memory\\\" You can identify the frequently accessed roles and instances by activating an SQL trace for table APB_LPD_OTR_KEYS via transaction ST05 and check the values of the ROLE and INSTANCE conditions in the WHERE clause. 74bfbf32537e5ed95a04dc60817ce3fe SELECT APB_LPD_SH_TEXTS This selection with UNIQUE_LPD_ID and LANGU filters originates from method READ_SHORT_TEXTS of class CL_APB_LPD_UTILITIES. It is used to retrieve texts for ABAP report launchpads. In order to eliminate the database table accesses you can activate shared memory buffering for the roles / instances in question as described in context of table APB_LPD_OTR_KEYS above. 8001561cb4947fe18fd7e967e9a1931e b59c3c5005309a050d5e035155133b85 CALL APS_DRP_AMOUNT_GET2 Calling the integrated liveCache procedure APS_DRP_AMOUNT_GET2 (used for aggregating quantities of I/O nodes in a pegging area) is usually quick from a performance perspective, but implicit memory booking leaks have been observed on SAP HANA side, so unjustified out-of-memory terminations may happen See SAP Note 1999997 -> \\\"Is the SAP HANA memory information always correct?\\\" -> M_CONTEXT_MEMORY and take appropriate actions to mitigate booking leak issues. You can use transaction /SAPAPO/OM21 and SQL: \\\"HANA_liveCache_LCApps_Executions\\\" (SAP Note 1969700 ) to identify context and KPIs for the APS_DRP_AMOUNT_GET2 calls. See SAP Note 2593571 for more information related to the SAP HANA integrated liveCache. 57dec217b2c81ec47bd0577131c8196b ae3891f8898465b9c844d36f806d1c78 bc671ed9c3058b43c12aafbab573e891 CALL APS_ORDER_GET_DATA This procedure is a central integrated liveCache procedure that can be used for various purposes. The performance massively depends on the way how it is called. If you experience high runtimes (that can't be explained with usual reasons like resource bottlenecks or locks) you should check the application that issues the expensive calls and make sure that the call is executed as light-weight as possible. In particular you should make sure that exclude flags are set in order to avoid processing data that isn't required later on. You can use transaction /SAPAPO/OM21 and SQL: \\\"HANA_liveCache_LCApps_Executions\\\" (SAP Note 1969700 ) to identify context and KPIs for the APS_ORDER_GET_DATA calls. See SAP Note 2593571 for more information related to the SAP HANA integrated liveCache. 45c9b4c77bb6b1ae3f659f02ab8bbc7f 614e2621fbfd45dc43a6dbc88c829ae5 INSERT ARFCRSTATE INSERTs in ARFCRSTATE can be quite frequent in systems with a significant RFC functionality and so they are one of the first modification statements that suffer from general issues (e.g. Barrier Wait, LoggerBufferSwitch). See a more detailed discussion for table USR02 that is also a typical victim of general issues. 03b1b6d7c3138557fd26c9553fe7baae 614e2621fbfd45dc43a6dbc88c829ae5 6414ea1875d5ff8deeb62a9b5f094762 791faba8d46ced32bc70d4bdde880d1e c291030bf5bbccfbcb3d7c6b85328cd2 da8163439880168ca7aca60b2ce64d25 e1df1b69227985b845d4f3f8135c6688 SELECT ARFCRSTATE This query is executed in report SAPLARFC and reads a rather high amount of records and then filters out most of them on ABAP side because they haven\\u2019t exceeded a defined retention period. Optimize the RFC handling as described in SAP Note 1483757 by setting the SAP profile parameter abap/arfcrstate_col_delete to \\u2018X\\u2019 and scheduling the RSTRFCEU batch job. 11817dbccc82f2828b600e8c841ef1d7 4790a816f7f326f7c78567171817a8a5 SELECT ARFCSSTATE These queries from report /SDF/SAPLIMA_DATA_COLLECTORS (function module /SDF/IMA_DC_TRFC) and report /SDF/SAPLE2E_EFWKE_COLLECTORS (function module /SDF/E2E_TRFC) select all entries of table ARFCSSTATE with ARFCRETURN = ''. These entries are typically linked to terminated RFC requests and in case of a high number of entries with this value the monitoring queries can significantly slow down. You can check for RFC requests in transaction SM58. See SAP Note 375566 and make sure that old and terminated requests are purged in time. 7b84bc319e14ca9371cf0f500e884582 a2c02d737ce003965d65321ab8900124 917733c889f0eeec874ca783ea8a3b40 db4ff98c75591e934351e781a474d6e1 SELECT FOR UPDATE UPDATE ARFCSSTATE ARFCSSTATE contains asynchronous RFC information on sender side. These database requests originating from function module ARFC_RUN / report SAPLARFC or TRFC_QOUT_READ_NEXT can suffer from delays accessing the underlying RFC destination due to transactional lock waits. You can identify the involved RFC destination of long running accesses using SQL: \\\"HANA_SQL_ExpensiveStatements\\\" (STATEMENT_HASH = '<statement_hash>') or via an SQL trace using transaction ST05 by checking the bind value used for column ARFCDEST in the WHERE clause. In transaction SM58 you can check if there are errors reported for the identified RFC destination. In transaction SM59 you can perform a connection test for the identified RFC destination. Reproducible or sporadic issues indicate that there is an issue with the destination, resulting in delays and overhead. You need to make sure that the RFC connection properly works to reduce lock time and contention. Examples: (connection error in SM59) Error when opening an RFC connection (CPIC-CALL: 'ThSAPOCMINIT' ...) ERROR: SAP gateway connection failed; is SAP gateway started? LOCATION: SAP-Server <instance> on host saphana (wp <cpid>) COMPONENT: CPIC RETURN CODE: 236 Error when opening an RFC connection (CPIC-CALL: 'ThSAPOCMINIT', communication rc: CM_RESOURCE_FAILURE_RETRY) ERROR: timeout during allocate of registered program LOCATION: SAP-Gateway on host <host> / sapgw<id> DETAIL: TP <rfc_dest> init/busy for more than 60 sec RETURN CODE: 677 db4ff98c75591e934351e781a474d6e1 UPDATE ARFCSSTATE UPDATEs on ARFCSSTATE can be quite frequent in systems with a significant RFC functionality and so they are one of the first modification statements that suffer from general issues (e.g. Barrier Wait, LoggerBufferSwitch). See a more detailed discussion for table USR02 that is also a typical victim of general issues. 2e90904f475471fba8a383e1eacdefdd SELECT ATPC_RESB Queries on ATPC_RESB originating from method SELECT_RESB of class CL_ATP_PAC_DB_SELECT can suffer from the evaluation of the following view condition: CASE WHEN \\\"RESB\\\".\\\"SOBKZ\\\" = N'E' THEN \\\"RESB\\\".\\\"KDAUF\\\" WHEN \\\"RESB\\\".\\\"SOBKZ\\\" = N'Q' THEN \\\"RESB\\\".\\\"PSPEL\\\" WHEN \\\"RESB\\\".\\\"SOBKZ\\\" = N'O' THEN \\\"RESB\\\".\\\"LIFNR\\\" ELSE N'' END AS \\\"SSKEY\\\" In case of a partitioned RESB table an upgrade to SAP HANA >= 2.00.070 may help (issue numer 276161). In general, SAP Note 3386125 can be implemented so that this CASE condition is split into several statements, allowing efficient processing. various SELECT ATP_RESB SELECTs on ATP_RESB with a \\\"BDMNG > ENMNG\\\" condition from report PPIO_ENTRY can suffer from the SAP HANA limitation described in \\\" High runtime in context of self-join \\\" above. This problem is fixed with SAP HANA Rev. 1.00.112.04 and 1.00.122.00. As a workaround you can implement the coding correction provided in SAP Note 2357019 . e13c4740f87f9ef697b8e2b01c53f388 SELECT AUFK The AUFK selection with \\\"<=\\\" and \\\">=\\\" conditions on column AUFNR in report RKOSEL00 can suffer from overhead introduced by FDA WRITE in context of FOR ALL ENTRIES (SAP Note 2399993 ). This problem can be fixed by disabling FDA WRITE for this query using a &prefer_join_with_fda 0& hint (SAP Note 2601166 ). 00f5990f5e2b048815271736eb06971e DELETE BC_MSG_AUDIT This deletion with MSG_ID specified in the WHERE clause can be slow due to an absence of an index on column MSG_ID. See SAP Note 3146538 and apply the mentioned SP or manually create an index on column MSG_ID of table BC_MSG_AUDIT (SAP Note 2160391 ). e76c89783d652cedeb0183d83e3d82bc 6687e2d2480061466bcffabfacc012b9 SELECT BC_SLD_INST These accesses are not expensive, but in context of transactional LOBs they can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. 4526e0c42114570e4b45d0f61c083cd1 UPSERT BDSER The MODIFY / UPSERT command on table BDSER from function module IDOC_SERIAL_POST can result in lock contention if IDocs are processed in parallel although the related application uses a serialization functionality. If the same object is processed by several IDocs in parallel this can result in high wait times. To solve this issue you can change the outbound option in the partner profile (transaction WE20) to queue processing. 28e20ad7406347dfd5a0cb6a8cd8f68d b93124fe6aa7d2d45e6fad54345b9672 SELECT BGRFC_I_RUNNABLE The selection of distinct DEST_NAME values originating from class CL_BGRFC_SCHEDULER_INBOUND in context of the bgRFC (background RFC) watchdog (passport action <BGRFC WATCHDOG>) can become particularly expensive when a high number of records exist in table BGRFC_I_RUNNABLE. Check from bgRFC side if an unexpectedly high backlog has piled up and take actions to process it to reduce the number of records in table BGRFC_I_RUNNABLE. See SAP Note 2309399 for bgRFC configuration. In case of a high bgRFC generation rate it can be required to increase the open connections per scheduler or the number of schedulers per instance to cope with the load. 8032cffe801f81ca835dddd255c50d68 SELECT BGRFC_O_DESTLOCK This selection in class CL_BGRFC_SCHEDULER_OUTBOUND can be executed very frequently in context of the bgRFC (background RFC) watchdog (passport action <BGRFC WATCHDOG>) in case of bgRFC inconsistencies (e.g. entries in table BGRFC_O_RUNNABLE without related entries in other tables). Check for inconsistencies via report RS_BGRFC_DB_CONSISTENCY and repair them to eliminate permanent watchdog activities. 155f2514ffbd97cd62c013758286ea76 a541d8b887b900b3d2f1073eaadf00c2 SELECT BGRFC_O_RUNNABLE These selections in class CL_BGRFC_SCHEDULER_OUTBOUND can be executed very frequently in context of the bgRFC (background RFC) watchdog (passport action <BGRFC WATCHDOG>) in case of bgRFC inconsistencies (e.g. entries in table BGRFC_O_RUNNABLE without related entries in other tables). Check for inconsistencies via report RS_BGRFC_DB_CONSISTENCY and repair them to eliminate permanent watchdog activities. various, e.g.: 7499c13d54ad95db232f03e8d7f95e78 847e7f7f2dbfcbeee615e1ace375963f SELECT BIMC_ALL_AUTHORIZED_CUBES BIMC_ALL_CUBES BIMC_DIMENSIONS BIMC_DIMENSION_VIEW BIMC_MEASURES BIMC_VARIABLE BIMC_VARIABLE_VIEW Accesses to these SAP HANA metadata tables read via MDS (SAP Note 2670064 ) can sometimes take advantage of a different optimization level. See SAP Note 2967256 for more details. fe4e46ff323a57316de435ced0e1c941 SELECT BIMC_ALL_CUBES BIMC_DIMENSIONS BIMC_MEASURES sap.hana.xs.dt.base.server.DTAccess::dtaa VIEW_COLUMNS VIEWS This query originates from the XSC web workbench. An optimization is available with XSC WebIDE Version >= 1.135.5. SELECT SELECT SELECT SELECT SELECT SELECT BSAD BSID BSAK BSIK BSAS BSIS If these tables are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 04fc8ad42cfad780a2a301cffea66201 3beb58c03468bc7cd64ab764a3caf0b2 SELECT BSP_DLC_DOBJ, BSP_DLC_SDOBJ, WCFV_DLC_DESIG, WCFV_DLC_SDESIG SAP Note 2538840 provides a coding correction that can significantly reduce the amount of executed queries against these BSP and WCFV tables. 3dcc171fdeed9f122a8e21a8772f764c SELECT BUT000 This statement with a GPART interval selected in the WHERE clause (\\\"PARTNER\\\" > ? AND \\\"PARTNER\\\" <= ?) is used when scheduling mass activities and using the business partner (GPART) object for parallel processing. The related application source is SAPLFKDI, the related function module is FKK_DI_GPART_DETERMINE_CLOSED. Due to changing parameters after the first execution, the query performance can degrade significantly when using an HEX index scan (SAP Note 2570371 ). You can either add the hint NO_USE_HEX_PLAN (SAP HANA <= 2.0 SPS 05) or NO_HEX_INDEX_SCAN (SAP HANA >= 2.0 SPS 06). See SAP Note 2142945 for more information related to SAP HANA hints. SAP Note 3051729 provides an application correction containing the hint. various SELECT BUT000, BUT020, ADRC Expensive SELECTs on tables BUT000, BUT020 and ADRC from CL_BUPA_IL_SEARCH_SERVICE->SEARCH_BY_ADDRESS2 can be optimized via SAP Note 1792008 by using switch CRM_PERF_BP_SEARCH. various SELECT BUT000, CRMD_PARTNER Joining BUT000.PARTNER_GUID with CRMD_PARTNER.PARTNER_NO imposes significant overhead (e.g. parallel CalculationJob activities) because the data type of both columns differ (VARBINARY vs. NVARCHAR). You either have to avoid this join or consider an explicit conversion like: \\\"PARTNER_GUID\\\" = HEXTOBIN(\\\"PARTNER_NO\\\") This problem doesn't show up in the SAP standard, it is linked to customer specific view definitions or joins in ABAP. eb82038136e28e802bd6913b38a7848c CALL BW_CONVERT_CLASSIC_TO_IMO_CUBE This procedure is executed when a classic infocube is converted to an in-memory optimized infocube using transaction RSMIGRHANADB. Increased load is normal when large infocubes are converted. After the conversion of the existing infocubes is finished, executing this procedure is no longer required, so it is only a temporary activity. 42566e1f2491b6b9820bd20d467af93b 68f35c58ff746e0fe131a22792ccc1b5 CALL BW_F_FACT_TABLE_COMPRESSION It is normal to see a significant cumulated runtime, because it is a central procedure call for all F fact table compressions. This procedure performs BW compression on F fact tables (i.e. elimination of requests and transition of data into E fact table or dedicated F fact table partition). If F fact tables with a significant amount of records are processed, a significant runtime is expected. Otherwise you can use SQL: \\\"HANA_Threads_ThreadSamples_FilterAndAggregation\\\" (SAP Note 1969700 ) in order to check for the THREAD_DETAIL information related to this statement hash, which contains the involved table names. Based on this information you can check if you can optimize BW F fact table compression for the top tables. Be aware that the THREAD_DETAIL information is only available when the service_thread_sampling_monitor_thread_detail_enabled parameter is set to true (see SAP Note 2114710 ). On BW side the compression activities typically happens as part of BI_COMP* jobs (manual execution) or BI_PROCESS_COMPRESS jobs (scheduled execution), so you can check in transaction SM37 for jobs with these names having a particularly high runtime. In the job logs you can find entries like: InfoCube <info_cube> Successfully Compressed Up To Request <request> Based on the job log timestamps you can determine how long an actual compression took. For the infocubes with the longest overall compression times you can check from BW perspective if any optimization is possible (e.g. reduction of data volume, reduction of compression activities). d39ab69a66a6a9f15ec60253966f0c9f and others INSERT BWFI_AEDAT, BWFI_AEDA2, BWFI_AEDA3 Lock contention and deadlocks on these tables can be caused by an inadequate application logic. See SAP Notes 1630808 and 2375171 for optimizations. CALL BW_PRECHECK_ACQUIRE_LOCK_WITH_TYPE This procedure is called before updating the activation queue of standard DSO tables in BW. It is required to ensure that two same data packages aren't updated at the same time. Among other a COUNT(*) on large DSO tables may be executed, resulting in increased CPU consumption and higher JobWorker utilization. This is fixed with SAP HANA >= 2.00.043 where the COUNT is replaced by a TOP 1. 4891a82d8e0e77ab4de398c35208bc20 and others SELECT CATSDB CATSDB selections with \\\"=\\\" or \\\"IN\\\" conditions on columns PERNR and WORKDATE may not be optimally supported by a single column index because only a combination of both conditions provides significant selectivity. The index CATSDB~1 on columns MANDT, PERNR and WORKDATE is typically not active in SAP HANA environment. Reactivate this index or create an additional CATSDB index on columns PERNR and WORKDATE. 2551f72c1b7b5fbc36eb82b31782df26 6b45f4dc17d1fd081cbc958d8291cfba SELECT CDHDR CDHDR selections originating from SAPLBUSA / BUS_CDOBJECTID_SELECT_WTH_LIKE can be quite expensive due to the generic selection approach that may involve leading place holders in LIKE conditions on column OBJECTID. SAP Note 2126752 provides some further ideas for optimizations from business perspective. 235200fb753637c5f9d6ebe9e4acca20 SELECT CDHDR This CDHDR selection in context of archiving (ABAP report CHANGEDOCU_WRI) has the following structure: SELECT * FROM \\\"CDHDR\\\" WHERE \\\"MANDANT\\\" = ? AND \\\"OBJECTCLAS\\\" = ? AND \\\"OBJECTID\\\" > ? AND \\\"UDATE\\\" BETWEEN ? AND ? ORDER BY \\\"CDHDR\\\" . \\\"MANDANT\\\" , \\\"CDHDR\\\" . \\\"OBJECTCLAS\\\" , \\\"CDHDR\\\" . \\\"OBJECTID\\\" , \\\"CDHDR\\\" . \\\"CHANGENR\\\" LIMIT ? Due to the combination of ORDER BY and LIMIT the execution can be particularly inefficient if a high number of matching records exist and a rather low limit is choosen. In this case you should consider increasing the package size of the used CHANGEDOCU_WRI variant via Options -> \\\"Internal packaging\\\" -> \\\"Package Size\\\". As a consequence the expensive selection and sorting is done less often for processing the same number of records. 783ed7080a18c2f0684a1765c95354e6 889628dbc36e0bcb9c8ea187259ab2f9 a9f17b62f2c85a2bbacdcbc81ec68545 ae3ca933e5445e92dcb4f617a7838c09 SELECT CDHDR, CDPOS, CDPOS_UID, CDPOS_STR This statement executed from SAPLCD_READ includes three OR concatenated EXISTS subqueries that can't be handled optimally with SAP HANA Rev. <= 1.00.110. See \\\" High runtime with multiple EXISTS in combination with OR \\\" for details and consider an upgrade to SAP HANA Rev. >= 1.00.111 in order to eliminate the underlying limitation. 43e7c0eac5e582316a7a17334389b0d7 611684d486918a356f1bbb14b790c17a 8d4657eeeaa526d9a34aa8a53b63f2a3 a7b7221b320c2cf6d25fd5842bf89ec4 e6c9e30929d7a58fd740be11b9d63204 SELECT CDPOS Implement the dbsl_equi_join hint as described in SAP Note 2162575 , so that the SQL statement is processed via IN list rather than OR concatenation. Alternatively avoid blanks at the end of TABKEY, so that the OR concatenation of LIKE conditions on TABKEY is no longer generated. SELECT CE4* Accesses to CO-PA tables starting with CE4 (e.g. CE4OC00, CE40004 or CE41000) can be significantly improved by adding an appropriate index. The WHERE clause of these queries typically contain \\\"=\\\" conditions on the majority of all table fields, concatenated by AND. The selectivity of the individual conditions changes massively between different statement executions because sometimes a rather unselective initial value may be specified, and other times a very selective individual value. As a consequence you need to create a multi-column index on the most selective columns of the WHERE clause (e.g. columns with a high number of distinct values). Creating a multi-column index with SAP HANA is a rather exceptional situation because usually single-column indexes are sufficient (SAP Note 2160391 ), but in this scenario it is required. Examples for index columns in two real-life cases: ARTNR, BWTAR, CHARG, KNDNR, KUNRE, KUNRG, KUNWE, WWPDC VBELN, UEPOS, ARTNR, KNDNR, KUNRE, KUNRG, KUNWE, WW009, WW010, WW017 See SAP Note 140880 for more information. 15e6cd042a447e36f789794d947270ee 203f26e3c3018684b236ce8bcae9c2df 26a7201696025450bb99f1fb1d203708 3192c20f2745437fda2c6325e0aa2dd3 3b408dee6191595541ebbd7800f10c9b 653f2b3365d2270a09dbc867f0caf09f 68590abdd6f75850b24472eaa7cb31a1 8fb5daf0fd892470558d780f305b3e43 cb96ffe3c3e2b15c1a199f9a3e63dd16 eb54f7641ea40eddf456604ecc100f46 CALL CHECK_TABLE_CONSISTENCY These statement hashes are related to the database consistency check based on CHECK_TABLE_CONSISTENCY, e.g.: CALL \\\"CHECK_TABLE_CONSISTENCY\\\"( ? , ? , ? ) WITH RANGE_RESTRICTION('CURRENT') CALL \\\"CHECK_TABLE_CONSISTENCY\\\"( ?, ?, ? ) WITH RANGE_RESTRICTION('CURRENT') CALL CHECK_TABLE_CONSISTENCY('CHECK', NULL, NULL) CALL \\\"CHECK_TABLE_CONSISTENCY\\\"( ? , ? , ? ) CALL \\\"SYS\\\".\\\"CHECK_TABLE_CONSISTENCY\\\" (TABLE_NAME=>?, SCHEMA_NAME=>?, ACTION=>?) CALL \\\"SYS\\\".\\\"CHECK_TABLE_CONSISTENCY\\\" (ACTION=>?, TABLE_NAME=>?, SCHEMA_NAME=>?) It is normal that the consistency check runs for a significant time and consumes significant resources, particularly for larger databases. See SAP Note 2116157 for options to adjust CPU and memory consumption of the consistency check. 95370f008f8577a7267981df483e1b86 5a9012b2349c8e356c328bd696bbe9e9 CALL SYS.CHECK_TABLE_CONSISTENCY_DEV This statement hash is related to the database consistency check performed by the SAP HANA statistics server. See SAP Note 2116157 for options to adjust CPU and memory consumption of the consistency check. f0721df5b7ae4971a8d68a7f5e4959be UPSERT CIF_UPDCNT This UPSERT can run into a transactional deadlock situation (SAP Note 1999998 ) with a modification operation on table EKET. Pilot SAP Note 3017515 provides a coding correction. Open a SAP case on component MM-PUR-GF-APO if you need access to the SAP Note. 270c660725af264b11bf387002abcd00 bed65ee7ee721e1472fda712236e7445 INSERT CKMLPP Inserts into CKMLPP can suffer from exclusive lock waits (SAP Note 1999998 ) and unique constraint violation terminations if the same primary key is inserted multiple times. Due to a bug in context of \\\"Late Lock for Goods Movements Active\\\" = 'X' in transaction CKM9 and S4CORE <= 104/OP 2009 it can happen that erroneously several transactions try to insert the same primary key. This is fixed with S4CORE 105/OP 2009 and higher (SAP Note 2833472 ). SAP Note 3128614 provides another optimization to reduce inserts and lock contention on table CKMLPP. various CALL CL_PPH_READ_CLASSIC=>GET_MRP_ELEMENTS Several SAP Notes like 2576155 and 2850985 provide performance optimizations for this procedure execution. Implicit memory booking leaks have been observed on SAP HANA side, so unjustified out-of-memory terminations may happen See SAP Note 1999997 -> \\\"Is the SAP HANA memory information always correct?\\\" -> M_CONTEXT_MEMORY and take appropriate actions to mitigate booking leak issues. bf1f07c495eea3b1ca3ad794e806467c SELECT COBK, ACDOCA This access from method CL_FINS_CFIN_CO_POSTING is optimized with the application coding correction provided in SAP Note 2778352 . b7c8dccd902d983736a4cae8ee8efd47 bb892077dc4654e79b3cacf3ad85b405 and others SELECT COEP COEP accesses with a selective condition on column PAROB1 (e.g. coming from program RKAZCO43) can significantly take advantage of a single column index on column PAROB1. See SAP Note 2160391 for more information related to SAP HANA indexes. SELECT COEP COSP COSS COVP If these tables are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 0c2d40de2702bc576de87a9aec2cef30 f45e2eaba236b6655804f6585d631923 CALL INSERT COLLECTOR_GLOBAL_ROWSTORE_TABLES_SIZE GLOBAL_ROWSTORE_TABLES_SIZE_BASE These statements are related to the row store table size history collection performed by the statistics server (see SAP Note 2147247 ). Consider reducing the amount of data in large row store tables (e.g. based on SAP Note 2388483 ) or moving large tables to column store in order to improve the runtime of these queries. cac55626bf3476dec8f39bcb30322f09 CALL COLLECTOR_GLOBAL_TABLE_CONSISTENCY This statistics server procedure triggers a consistency check with CHECK_TABLE_CONSISTENCY. See CALL -> CHECK_TABLE_CONSISTENCY for more details 651d11661829c37bf3aa668f83da0950 42bf4a47fbf7eabb5e5b887dffd53c7c CALL INSERT COLLECTOR_HOST_CS_UNLOADS HOST_CS_UNLOADS_BASE Disable the data collection for HOST_CS_UNLOADS as described in SAP Note 2084747 . This action is also recommended in SAP Note 2147247 . 88f3a62bd06cbae2ec4ac184df7e0a3a 8a70aa0437cd9bf03d7e1f51e4a722bb CALL COLLECTOR_HOST_LONG_RUNNING_STATEMENTS These calls are issued by the SAP HANA statistics server (SAP Note 2147247 ). In rather idle systems it is normal that these statistics server operations appear at the top of the SQL cache and runtimes up to 300 ms / execution are normal and acceptable. In this case no action is required. In case of higher runtime a performance analysis and / or a reduction of the execution frequency may be useful (ID 5028). ac52398f58a752bed7843aac8d829464 e9f2feac54a048984928f3fcbcfacedd CALL INSERT COLLECTOR_HOST_RS_INDEXES HOST_RS_INDEXES_BASE These statements are related to the row store index history collection performed by the statistics server (SAP Note 2147247 ). Consider reducing the amount of data in large row store tables (e.g. based on SAP Note 2388483 ) or moving tables with large indexes to column store in order to improve the runtime of these queries. 2d40db6145e579aa5c23e42337750ee8 5157ba1bc92d3166d7eab858566f9ea1 CALL COLLECTOR_LIVECACHE_CONTAINER_STATISTICS, COLLECTOR_LIVECACHE_SCHEMA_STATISTICS These procedures are used by the statistics server (SAP Note 2147247 ) in order to retrieve statistics for the integrated liveCache (SAP Note 2593571 ). With some SAP HANA Revisions the related statement hashes can be mentioned in context of trace file entries \\\"SharedLock overflow on context\\\" (SAP Note 2380176 ), this can be ignored. 58071fe07364fd7c23f7e28880150128 UPSERT COOI_CHK This UPSERT originating from a MODIFY operation in include LKAOIF80 can result in transactional lock contention (SAP Note 1999998 ) when updating commitment data in parallel tasks. Implement SAP Note 2906851 to switch off the responsible check function provided by the report RKA_COMM_CHECK. 36490273789aeb5c4a5070e6307fe782 SELECT COSS This query originating from line 139 of include LKAIVF1B is optimized with the correction available in SAP Note 2876066 . 8324ebd3d1cf16908d778f7dceb5eee5 TRUNCATE COVRES The TRUNCATE is triggered by job /SDF/UPL_PERIODIC_EXT_JOB respectively program /SDF/UPL_PERIODIC_EXTRACTOR: CALL METHOD repository->('IF_SCV_LITE_REPOSITORY~TRUNCATE_COVERAGE_DATA') CALL FUNCTION 'DB_TRUNCATE_TABLE EXPORTING tabname = 'COVRES' save_views = space set_init_storage = 'X' IMPORTING subrc = return_code. It is usually executed quickly, but the TRUNCATE results in a metadata change and so it can result in (harmless) terminations of concurrent CHECK_TABLE_CONSISTENCY runs with message \\\"5099: metadata version changed while running checks\\\". 26e1ca7b731a467f2db818753d80118f SELECT CRMD_ORDER_INDEX This access suffers from the COUNT DISTINCT limitation of SAP HANA SPS <= 08 and can be improved with the USE_OLAP_PLAN hint. SAP Note 2255511 provides the coding correction. 0e483b3074906106fd7c321a30fdea85 SELECT CRMORDERLOPR This SELECT is triggered by SAP ABAP table buffer reloads that can happen frequently because of invalidations. See SAP Note 1916476 for more information and disable table buffering for CRMORDERLOPR. f33b8660de1e3bc467d79366be1d7258 DELETE CRMT_RECENT_OBJ This deletion originating from ABAP function module CRM_RECOBJ_DATA_SET (application source SAPLCRM_UI_RECOBJ_DATA_SET:40) can suffer from deadlocks when multiple tabs are closed in a browser at once. SAP Note 2232607 provides an optimization. If the problem is still reproducible, please open a SAP case on component CA-WUI-UI. 55c0d811e887fa4a08bba2b4c94a46ff 9ce7b872af35b7c992a3b5d8f463ccdc INSERT CS_AUDIT_LOG_ This INSERT is related to SAP HANA auditing. You can consider the following steps to optimize it: Check if the amount of audited information can be reduced. For example, SAP Note 2293725 describes a scenario where more objects are audited than originally intended. Check if there are general issues impacting INSERT performance like slow log write times or system replication issues (SAP Note 2000000 ). See SAP Note 2159014 for more information related to SAP HANA security. 86a04561a02aa97dd40f05e2e5f41524 f3612366d53e8fcbf465da8dcc8f2473 INSERT D342L, D345T, D346T Modifications to the CUA runtime object tables can suffer from exclusive lock waits and transactional deadlocks (SAP Note 1999998 ). See SAP Note 3171132 for more information. d07382ba4b6dc49c8594fe66b128571b SELECT DATA_TYPES, VIEW_COLUMNS and others This SELECT is executed when an ODBC or SQLDBC application calls the SqlColumns client function (e.g. in context of SAP Data Services / BODS) in order to retrieve column related metadata. Typically only one request is sent per connection and table. A high load can be caused by: Long runtime of query on SAP HANA dictionary objects and monitoring views due to missing CATALOG READ privilege High number of connections requesting SqlColumns information High number of tables for which SqlColumns information is requested Overhead accessing metadata (see SAP Note 2222200 -> \\\"What can be reasons for threads in network related states?\\\" -> \\\"Metadata access\\\") c6e61129d85ccb14ca14b58ca5b188d4 e9c4f7f727f746f54c1e36a87205eae8 INSERT DBTABLOG INSERTs into DBTABLOG are linked to table logging. Check if table logging is unnecessarily activated for tables with a significant change load. You can find the top tables logged in DBTABLOG via SQL: \\\"HANA_Data_ColumnValueCounter_CommandGenerator\\\" (TABLE_NAME = 'DBTABLOG', COLUMN_NAME = 'TABNAME') available via SAP Note 1969700 . You can check via ABAP transaction SE13 -> \\\"Display\\\" -> \\\"Log Data Changes\\\" whether table logging is activated for a specific table. The following problems are known: SAP Note 3295259 : Slowness due to table logging being active for table WDR_ADP_CONST_MP b03a016fa9a79e6973cee05f693c88cc SELECT DD17S, DD12L Overhead accessing these tables is reduced starting with SAP ABAP kernel patch level 7.53 (211). See SAP Note 2641772 for more information. a9c2427d8a5fee49ef90da48ce8ebe04 afb54cf91cd5d51ac7f360f82e67d5dd INSERT DDLOG These statement hashes refer to an INSERT operation in ABAP table DDLOG that is required to synchronize buffered tables between application servers. This table is based on a sequence and uses a LOB column so runtimes of up to 2 ms per INSERT can be acceptable. Check on ABAP side if there is a high amount of changes on tables buffered on ABAP side and try to reduce it or unbuffer tables. INSERTs into DDLOG can be impacted by the underlying SAP HANA sequence DDLOG_SEQ (see e.g. SAP Note 1977214 ). It is generally recommended to activate caching for this sequence. You can use the following command to activate a cache with 1000 elements for the sequence: ALTER SEQUENCE <schema_owner>.DDLOG_SEQ CACHE 1000 SAP HANA may also report a high memory consumption for this INSERT. This is usually not correct and the result of a bug in memory accounting. INSERTs in DDLOG are quite frequent in ABAP environments and so they are one of the first modification statements that suffer from general issues (e.g. Barrier Wait, LoggerBufferSwitch). See a more detailed discussion for table USR02 that is also a typical victim of general issues. SELECT DDNTT Accesses to table DDNTT can suffer from issues with the ABAP catalog cache and ABAP table logging: See SAP Note 3241223 for a known issue resulting in an efficient usage of the catalog cache. Make sure in transaction ST02 that the catalog cache is appropriately sized and no or only a limited amount of swaps (up to 1000 per day) take place. Check if table logging is actually required for the table in question. You can use SQL: \\\"HANA_ABAP_TableLogging\\\" (SAP Note 1969700 ) to check for existing table logging entries and tables that potentially don't require table logging ( ONLY_UNNECESSARY_TABLES = 'X' ). You can adjust table logging via ABAP transaction SE13 -> \\\"Table name\\\" -> Change -> \\\"Log data changes\\\". 46780875139519ef1423e34efe1a9582 UPSERT DDNTT_HIST Upserts of table DDNTT_HIST can suffer from transactional lock issues due to the ABAP catalog cache and ABAP table logging: See SAP Note 3241223 for a known issue resulting in an efficient usage of the catalog cache. Make sure in transaction ST02 that the catalog cache is appropriately sized and no or only a limited amount of swaps (up to 1000 per day) take place. Check if table logging is actually required for the table in question. You can use SQL: \\\"HANA_ABAP_TableLogging\\\" (SAP Note 1969700 ) to check for existing table logging entries and tables that potentially don't require table logging ( ONLY_UNNECESSARY_TABLES = 'X' ). You can adjust table logging via ABAP transaction SE13 -> \\\"Table name\\\" -> Change -> \\\"Log data changes\\\". For deadlocks see also SAP Note 3200326 . SELECT DFKKOP DFKKOP selections with \\\"=\\\" or \\\"IN\\\" conditions on column XBLNR typically can take advantage of an additional single column index on XBLNR because the conditions that are typically very selective. SELECT DIFT_POS_IDENT Check if the suggestions of SAP Note 1122623 (including proposed index modifications) can help to improve the performance. de4f2fe75cf84e95ec75ffbaf9825c8a SELECT DIMAIOB This query scans a potentially large amount of records, sorts it by the primary key and returns the first record based on the sort order. This is expensive on SAP HANA as the primary index can't be used for an efficient sorted access. An application correction is available via SAP Note 2406424 . 447875bbafe59901254cf1fb7e846fe2 SELECT DMC_C_WL_TABL_OP This SELECT COUNT executed in context of SLT can be improved with an optimized buffering delivered via SAP Note 3154154 . 1045d8b4e76ac8d4d37303bab4242be0 9cf5e32514053d9c617baff34d032952 SELECT DOKIL This selection with an equal condition on ID and a LIKE condition on OBJECT originating from application source SAPLSDOC:29399 respectively function module DOCU_FROM_TR_OBJECT_RECEIVE may be executed too frequently. Implement SAP Note 3250296 in order to reduce the number of selections. f85c4cdbc90cd417ef4f941d949abb7a CALL DSO_ACTIVATE This procedure was used to activate HANA optimized / in-memory optimized / IMO DSOs. This DSO type is deprecated with SAP HANA >= 2.0 SPS 01 (SAP Note 2425002 ). Migrate these DSOs to supported types (SAP Notes 1849497 , 1849498 ). 4073a6b50a8441fa93f10d2acb68da4d 91d7aff2b6fb6c513293d9359bf559a6 ae7f94b9a7e4c902dba96571db19dffb fa34afc37fd96494f3d37a3aab95f17a CALL DSO_ACTIVATE_PERSISTED It is normal to see a significant cumulated runtime, because it is a central procedure call for all DSO activations in SAP HANA environments (SAP Note 1849497 ). This procedure is the HANA server side implementation of DSO activation in BW environments. During this activation data is moved between three involved tables (/BIC/A*40, /BIC/A*00 and /BIC/B000* for classic DSOs or /BIC/A*1, /BIC/A*2 and /BIC/A*3 for advanced DSOs). It is much more efficient than the previous, SAP application server based approach, because moving a high amount from the database to the application and back is no longer required. You can use SQL: \\\"HANA_BW_DSOOperations\\\" (SAP Note 1969700 ) to check for particularly long running DSO operations and their runtime and record details. Check if you can reduce the activation load by reducing the number of DSO activations or reducing the data volume per activation (e.g. by using delta loads instead of full loads). If the DSO activation in distributed environments takes longer than expected, you should check if the partitioning of the /BIC/A*40 and /BIC/A*00 tables is consistent, i.e. same partitioning and same hosts for the related partitions. SQL: \\\"HANA_TraceFiles_Content\\\" (SAP Note 1969700 ) reports DSO misconfigurations via check ID T1100 (\\\"Inadequate BW partitioning\\\"). A DSO activation performs many changes, so it can suffer significantly from bottlenecks in the redo log and system replication area. See SAP Notes 1999930 for redo log I/O analysis and 1999880 (\\\"Can problems with system replication impact the performance on the primary system?\\\") for more details. If a very high number of records is modified you may experience contention on the delta storage (BTree GuardContainer lock, SAP Note 1999998 ). You may be able to improve the contention by increasing the number of partitions so that the delta load is distributed. Apart from this you can also apply optimizations on BW side: Smaller requests Smaller keys DSOs that don't require an activation, e.g. info cube-like ADSOs 2b88d45d4e1246c74d27ac1f9689c289 CALL DSO_ROLLBACK_PERSISTED This procedure is called if a request is deleted from a standard DSO in BW. If you repeatedly see this procedure call you should check from BW side why repeatedly requests are deleted from standard DSOs. In addition you can check the suggestions provided in context of DSO_ACTIVATE_PERSISTED (see above). b1e72ff44bc0eea0b572fc89cfa7674b SELECT DYNPLOAD Selections on the ABAP dynpro load table can suffer from delays reading LOB files from disk. In this case you typically see I/O related information in the threads, e.g. \\\"Resource Load Wait\\\", PrefetchIteratorCallback or PageIO::SyncCallbackSemaphore. See SAP Note 1999930 and make sure that I/Os are processed efficiently. For example, bottlenecks in the I/O stack, overloads due to savepoints of huge table optimizations or resource container unloads can result in unnecessarily long I/O read times. various SELECT EABL, EABLG If joins on tables EABL and EABLG are expensive without apparent reasons (like inadequate index design) you can check for the following optimizations: If the tables were recently populated, the join statistics may not reflect the current state and SAP HANA may use an inappropriate execution plan. In this case a restart of SAP HANA would result in a collection of current join statistics. Starting with SAP HANA SPS 09 join statistics are also recreated online when the amount of changes on the underlying tables is significant. If the expensive join happens in context of CRM for Utilities and the IC Webclient / CRM UI you can check SAP Note 2218437 and implement the proposed coding correction or the required IS-UT support package in order to reduce the amount of executed EABL / EABLG joins for displaying historical meter readings from an application perspective. b8d1861e452621f0c6642ed24a50099e f246ac9c3b1ce532d4b51caf6fd70691 SELECT EDIDS This EDIDS query is typically linked to the hourly batch job SAP_CCMS_MONI_BATCH_DP respectively report RBDMONI_CCMS_IDOC that is executed in context of CCMS monitoring. You can adjust or disable the underlying monitoring method ALE_IDC_COLLECT here: Transaction RZ21 -> Methods -> Method definitions -> ALE_IDC_COLLECT -> Control -> Uncheck \\\"Execute method immediately after monitoring segment start\\\" In transaction BDMO you can configure what kind of details should be monitored in context of ALE CCMS monitoring. Alternatively you have to resolve existing problems in the EDI area and make sure that table EDIDS is regularly cleaned (SAP Note 2388483 ). See also SAP Notes 2905493 and 3268073 that provide performance optimizations for ALE CCMS monitoring. 2cafcd6301acc5cf9e5c80de57553052 30cc2a2936e36a3caa732accea1fa711 73393b77ed7d09afce44478e34879786 857c5b8d64f949f1beb9736dec9ef069 SELECT EDIDS This statement originates from function module IDOC_GET_MESSAGE_ATTRIBUTE (function group SAPLBDID) and has a subquery based on COUNTR. It can take advantage of using the HEX engine (SAP Note 2570371 ). Please implement the delivered SAP HANA statement hints (SAP Note 2700051 ) that will among others generate a USE_HEX_PLAN hint for this query. a1652d017b9f415f4411fd8974f3caf4 SELECT EDIQO This query is the selection of the maximum counter for a certain QNAME originating from include LEDI1F09 of the SAPLEDI1 function group: SELECT MAX( \\\"COUNTER\\\" ) FROM \\\"EDIQO\\\" WHERE \\\"MANDT\\\" = ? AND \\\"QNAME\\\" = ? In case of a greater number of records in table EDIQO this operation can be expensive due to the limitations described in section High runtime of MIN and MAX searches . For consistency reasons it is not possible to change the EDI implementation in this context. Instead you have to make sure that the amount of concurrent requests in EDIQO remains at a reasonable level by considering the following aspects: Use qRFC only when required Perform regular commits when performing mass operations related to qRFC Make sure that there are no issues in qRFC processing (e.g. communication problems), transaction WEOUTQUEUE provides an overview of the outbound queue state 64cd1a531cdab0abd980f4a515b83c87 76c77bd3d063cfebb63a03c7e7c4d001 SELECT ENHINCINX These selections from function module RS_GET_ALL_INCLUDES respectively method GET_ALL_INCLUDES of class CL_WB_CROSSREFERENCE only specify PROGRAMNAME in the WHERE clause and no appropriate index exists. The selection with hash 76c77bd3d063cfebb63a03c7e7c4d001 was introduced as part of the correction of SAP Note 3072385 . Create an additional ENHINCINX index on column PROGRAMNAME in order to optimize this request. 68eee9077938d4c8223ef0149ecb0d51 SELECT ESSR The ESSR selection from function module LMEREPI02 / application source SAPLMEREP:8926 is expensive because all records of the current client are selected in case the FOR ALL ENTRIES itab lt_bet_ses is empty. Implement SAP Note 3028018 to make sure that this selection is only executed when the itab is not empty. d956e5bcd4f2756c19a28a92778e1177 CALL SYS.EXECUTE_MDS This is a generic SQL procedure call for executing MDS requests (SAP Note 2670064 ) when the following parameter setting is in place (SAP Notes 2180165 , 2600030 ): indexserver.ini -> [mds] -> use_sql_procedure_for_analytics = true As a lot of different MDS requests are executed based on this procedure, it is normal and expected that it is part of the most expensive database requests in MDS environments. To drill down, you can check the bind values of expensive executions via the expensive statements trace (SAP Note 2180165 ) that map to method, schema, package, object, datasource type and request. See Long InA / MDS accesses for further MDS performance optimization details. SELECT EXTENSION Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 7df11e8ddef118e147bcd96dd848a06b SELECT FAAT_DOC_IT This query is executed in method CL_FAA_MDO_ITEM and suffers from the fact that the maximum (MAX) of a potentially rather larger amount of records needs to be determined. SAP Note 2410056 provides a coding correction. various SELECT FAAV_ANLC FAAV_ANLC is a compatility view on table ANLC. Ceck question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. See SAP Note 2816302 and check if you can directly query the underlying table FAAT_YDDA to avoid unnecessary overhead. See SAP Notes 2816301 and 2796770 and make sure that the compatibility view is set up optimally for performance. various SELECT FAGLFLEXT, FMGLFLEXT, GLT0, JVGLFLEXT, PSGLFLEXT If these tables are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 35c76a107e1aeab12355e3ffb4389ba9 6db0e37f140e3b529ad82ab0cdbc0c2b 7b94dc29b0ff584f3a0e1be2955bfc64 c50d944ceb16cb5aacd20be3c9e811c1 DELETE FEBIP DELETE operations of certain KUKEY values from report RFEBKA30 / application source RFEBKA30:4713 can suffer from transactional locks and deadlocks due to an inadequate application design. SAP Note 3208265 provides a coding correction. 2f8e5433683b98b4fe6b8257d9bd4195 554c18cc7e6018abd21c69429b273f0b a8a84c81280a0c2bc035cff4729e904e SELECT FOR UPDATE FKKDIHDTMP If SELECT FOR UPDATE requests on table FKKDIHDTMP suffer from exclusive lock waits (SAP Note 1999998 ) you should check if a reduction of mass activity intervals can improve the situation (SAP Note 2177798 ). CALL GET_ACCESSED_OBJECTS_IN_STATEMENT This procedure is used to determine which objects are accessed in a database request. Among others it is used when a SQL statement is executed with the SQL editor of ABAP transaction DBACOCKPIT (SAP Note 2222220 ) in order to determine the involved objects for a proper table-level authorization check (SAP Note 1933254 ). The call is only executed when table specific permissions are configured, so it can be suppressed by assigning general permissions (e.g. S_TABU_SQL = * or SAP_ALL). 2cf6dfab2c7a730bc56b40fc3d64734c c38678b42400ea7b7252e3409ad6db3f UPDATE GLFUNCT Long runtimes are typically caused by record lock contention (SAP Note 1999998 ). Implement SAP Notes 2193726 and 2296436 in order to speed up processing and reduce lock times. 5ec05e8ced6066fbe5a94cb1ef5c130a 5fa89dc1471bdeddba67e0088dd52b96 690cf05502836d8ed30d809eadc360ec 7c541b915a48ae457956cda4e9702780 83ac4bf74da990133f1c525d05f43714 a99311b79cb2fb8706e979cda20edb44 db2a5d8b668a837677bb6946de2a8d76 e7aa79c355895c079e00ef03ffdfcc47 fc1e3ccb2049891578e6dcb4deaa71e0 INSERT CALL ALERT_BLOCKED_TRANSACTIONS COLLECTOR_HOST_BLOCKED_TRANSACTIONS HOST_BLOCKED_TRANSACTIONS_BASE In rather idle systems it is normal that these statistics server operations appear at the top of the SQL cache and runtimes up to 300 ms / execution are normal and acceptable. In this case no action is required. With SAP HANA >= 2.00.047 the statistics server actions for blocked transactions are redesigned, providing both more reliable results and better performance. The performance can also suffer if the implicit access to M_ACTIVE_STATEMENTS takes a long time. Check section M_ACTIVE_STATEMENTS for possible reasons and optimizations. 064bc772039ccbfe53c34b0d6bbd0aa9 INSERT HOST_JOB_HISTORY_BASE This INSERT is executed in context of the statistics server (SAP Note 2147247 ) collector COLLECTOR_HOST_JOB_HISTORY. This collector is scheduled every 60 seconds per default although the underlying SAP HANA monitoring view already provides historic information with a much higher retention time. In order to reduce the overhead, you can increase the interval from 60 to 600 seconds: UPDATE _SYS_STATISTICS.STATISTICS_SCHEDULE SET INTERVALLENGTH = 600 WHERE ID = 5048 The SAP default will also be adjusted to 600 seconds (issue number 316265). 3824c73a54b350c95c843aed2aa8c70f DELETE HOST_OBJECT_LOCK_STATISTICS_BASE This DELETE purges old records from the statistics sever history for object lock statistics. It can be particularly expensive if many unnecessary entries with OBJECT_NAME = '(unknown)' exist in HOST_OBJECT_LOCK_STATISTICS_BASE. This problem is fixed starting with SAP HANA 2.0 SPS 00. You can consider using SAP HANACleaner (SAP Note 2399996 ) to configure an automatic cleanup. The following command can be used for a manual cleanup: DELETE FROM _SYS_STATISTICS.HOST_OBJECT_LOCK_STATISTICS_BASE WHERE OBJECT_NAME = '(unknown)' See SAP Note 2147247 for more information related to the SAP HANA statistics server. fb8b1663df25927cbd8041043a715541 INSERT HOST_SERVICE_THREAD_SAMPLES_BASE This command is used by the statistics server (SAP Note 2147247 ) to generates the history of thread samples. Longer runtimes are possible if you increase the thread samples in memory (SAP Note 2114710 , parameters service_thread_sampling_monitor_max_samples, service_thread_sampling_monitor_max_sample_lifetime) or if you reduce the history collector interval length (collector Collector_Host_Service_Thread_Samples, ID 5034). f816711c913d152181a1e2d3d3d1dc43 INSERT HOST_SERVICE_THREAD_CALLSTACKS_BASE This insert populates the call stack history table HOST_SERVICE_THREAD_CALLSTACK_BASE that can e.g. be evaluated via SQL: \\\"HANA_Threads_Callstacks_History\\\" (SAP Note 1969700 ). Historic call stacks are helpful for troubleshooting purposes and so it is usually acceptable that this statement generates some load. See SAP Note 2313619 for more information related to SAP HANA call stacks. There can be overhead if the call stacks are captured with the interval of 300 seconds because in this case often a lot of other statistics server activities are captured. See SAP Note 1999993 (check ID M0752, \\\"Historic thread call stacks interval (s)\\\") and make sure that the interval is set to 299 seconds in order to avoid correlation with other activities. SELECT HRP1002 Accesses to table HRP1002 can suffer from missing indexes on columns TABNR and OTJID. See SAP Note 2549948 for more information. ee8c70fd188e0b92eba9b72e484abc90 UPDATE IBINST_OBJ This update can suffer from exclusive lock waits and deadlocks when many variant configuration statistics are written concurrently via ABAP V2 update. SAP Note 1548171 provides more efficient alternatives, e.g. the collective update using report RCU_IBSTAT_UPDATE_STATISTICS. 66342c6548a87008636bb0cda6db8872 UPDATE IBINVALST_SYM This update can suffer from exclusive lock waits and deadlocks when many variant configuration statistics are written concurrently via ABAP V2 update. SAP Note 1548171 provides more efficient alternatives, e.g. the collective update using report RCU_IBSTAT_UPDATE_STATISTICS. 19d5723835e910684f6df4334659e652 DELETE INDX Deletions from INDX can suffer from the deletion scenario described in SAP Note 1999998 (\\\"Why are there locks and deadlocks that can't be explained by the actual modification mechanisms?\\\"). SAP Note 2640640 provides a fix from business perspective. SAP Note 2666147 provides another application correction to improve lock scenarios from application perspective. SELECT ITEM Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 8574088c0e4e86fee180b6de980ab0b2 SELECT IUUC_ARCH_USERS The selection from table IUUC_ARCH_USERS with a \\\"user_name = SESSION_CONTEXT('APPLICATIONUSER')\\\" condition is part of SLT triggers (SAP Note 2800020 ) when the following option is activated in transcation LTRS: Options for Archiving -> Action Taken if Source Table Record is Archived -> Do Not Delete Corresponding Target Table Record This feature makes sure that data isn't deleted in the target system when archiving (with the archiving user configured in transaction LTRS being stored in table IUUC_ARCH_USERS) happens in the source system. Make sure that you only activate this option when you need to preserve archived data in the SLT target system and there is actually archiving configured for the table in question. Alternatively, check if using the \\\"SAP HANA - any version\\\" approach described in SAP Note 2617971 can be used. This approach directly selects the user names rather than having to select them from the IUUC_ARCH_USERS table. 0f156f5e07b2865a9f1fed5837a036c3 SELECT IUUC_TAB_ALLOWED This SELECT COUNT executed in context of SLT can be improved with an optimized buffering delivered via SAP Note 3154154 . 130dfe8491e99e9960690f9fce983a0a DELETE /IWFND/I_MED_CTC This DELETE originating from method REMOVE_MODELS of class /IWFND/CL_MED_MDL_CACHE_PERSIS (application source /IWFND/CL_MED_MDL_CACHE_PERSISCP:621) can suffer from exclusive lock waits and deadlocks (SAP Note 1999998 ) due to flaws in the application design. Check SAP Notes 2631265 , 2889829 , 3041609 and 3116948 for application corrections reducing the risk of locking and deadlock scenarios. 1c413a671a05e2fb627f278474dcea61 INSERT /IWFND/SU_STATS Increased INSERT times in table /IWFND/SU_STATS can be caused by a significant amount of INSERT operations that can be switched off via SAP Note 2293307 or a programming error that is fixed with SAP Note 2508563 . ce499972ced797474c71d21bae6b4cf9 INSERT JOB_LOG This INSERT into the JOB_LOG table of schema _SYS_XS includes an implicit NOT EXISTS anti join on the same table and so it can become more expensive when there are many records in the JOB_LOG table. See SAP Note 2388483 -> JOB_LOG and make sure that no longer required entries are cleaned in time and the amount of XS classic jobs is kept at a reasonable level. SELECT KNA1 Expensive selections from table KNA1 triggered by SELECT COUNT requests on ABAP side in report RKDFHDB are possible when no KUNNR condition is specified. SAP Note 3246036 provides a correction. 13a2bac734add8e6c33e837b146cef62 c027840449c9bf6bffff7b7ff813feaf SELECT KONV Queries on KONV with a selective condition on column KNUMH can take advantage of an additional KONV index on column KNUMH, see for example SAP Note 2424784 for SD_COND_ARCH_WRITE. 7d45827e7d5b5ce271baa827730985ba SELECT J2EE_CONFIGENTRY This query can be expensive if no index on column CID exists. See SAP Note 3233072 and create an additional J2EE_CONFIGENTRY index on column CID. 969667ed5f022a232f710b5a5af51dff SELECT J2EE_CONFIGENTRY This query is not expensive, but in context of transactional LOBs it can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. 8559ff443a3816983aeac38377e37d8d INSERT LICENSE_MEASUREMENTS_ This statement is used for license measurement. It can be quite expensive in system databases in case a particularly high number of tenants exist. See SAP Note 3334173 for more information. 2d1ea9a365efd64d1504ba5dc255b0db and others SELECT /LIME/NTREE This selection with an application source like /LIME/SAPLQUERY:28438 originates from include /LIME/LQUERYTDI. It is executed with the DBI hint &prefer_join_with_fda 0& so that FDA WRITE is not used (SAP Note 2921070 ) and instead long OR concatenations of \\\"LFT\\\" > ? AND \\\"LFT\\\" <= ? AND \\\"LVL\\\" > ? are generated. This scenario is often not processed in an optimal way by the HEX engine (SAP Note 2570371 ) and a NO_USE_HEX_PLAN statement hint (SAP Note 2400006 ) can improve the performance. 2bae02d679a579d736bf92d660282c2f and others SELECT /LIME/NTREE, /SCWM/HU_IW01 SAP Note 2738822 implements NO_CS_JOIN into ABAP code of /LIME/LQUERYBH1 in order to improve performance. 7a00a013b367f505d7840506e237169e SELECT LMDB_P_INSTANCE This selection originating from CL_LMDB_CIM_PERS_DB can suffer from a missing index on column VALUE. In order to optimize the performance you can create a single column index on column VALUE of table LMDB_P_INSTANCE. 76f246be96028db1e4ac3997b6a07373 SELECT M_ACTIVE_STATEMENTS On SAP HANA 1.0 this query is executed by the mvcc_anti_ager module (MVCCGarbageCollector) that regularly checks for problems impacting the row store garbage collection (SAP Note 2169283 ). It usually doesn't require many resources or show a high runtime, but it may be reported when you have low-level system problems (e.g. communication problems between hosts in SAP HANA scale-out environments). Reason: It is one of the first SQL statements executed against the database, even before any business transaction is started. In large systems with a lot of active or prepared SQL statements it is possible that this query shows a long runtime and an increased resource consumption. In order to prevent overhead it is recommended to adjust the following SAP HANA parameter from a 10 seconds to a 300 seconds check interval (SAP Note 2600030 ): indexserver.ini -> [transaction] -> aggressive_gc_interval = 300 Long runtimes of the query can also because by other reasons described below in the general M_ACTIVE_STATEMENTS section. various SELECT \\ufeffM_ACTIVE_STATEMENTS Long runtimes accessing M_ACTIVE_STATEMENTS are usually caused by many entries in M_ACTIVE_STATEMENTS and / or M_PREPARED_STATEMENTS (issue number 258995). See SAP Note 2088971 -> M_PREPARED_STATEMENTS for possible reasons of a high number of entries. c5c2b97695d4cac5f3993ff79002576b SELECT MARA This selection in method /SCWM/IF_AF_MATERIAL_BASE~READ_PRODUCT_GROUPING of class /SCWM/CL_AF_MATERIAL_BASE_S4 may repeatedly read a rather high number of records. It can be optimized with SAP Note 3041119 . various SELECT MARC, MARD, MARDH If these tables are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. a505eb1288614d619b31f160eb1b0e5b b8b6f286b1ed1ef2e003a26c3e8e3c73 cf2e9b374514550f2b2e522df9f619ec e04936562f4402c22dda54cb98807a5d 16970b5f1021649f4b79dde9017812c2 2afa9311f17e325d6d1418b3dd3eb388 2c7032c1db3d02465b5b025642d609e0 5ec9ba31bee68e09adb2eca981c03d43 5f42d3d4c911e0b34a7c60e3de8a70d2 SELECT M_BACKUP_CATALOG_FILES, M_BACKUP_CATALOG, SOURCE_ALERT_65_BACKUP_CATALOG These SELECTs are regularly issued by the backup console in SAP HANA Studio. In order to minimize the impact on the system you should open the backup console only when required and not permanently. If you repeatedly need to open the backup console, you can increase the refresh interval (default: 3 seconds): Check if the backup catalog is unnecessarily large and delete old catalog entries if possible using: BACKUP CATALOG DELETE ALL BEFORE BACKUP_ID ... e291174cbc7ab2c68740114ac34bfd3e SELECT M_BACKUP_SIZE_ESTIMATIONS This query is executed starting with SAP_BASIS 7.50 SP19 when ABAP transaction ST04 is called or when \\\"Current Status\\\" -> \\\"Overview\\\" is selected in transaction DBACOCKPIT (SAP Note 2222220 ). The runtime depends on the maximum converter page number (M_CONVERTER_STATISTICS.MAX_PAGENUMBER) because all these converter pages have to be scanned and it takes roughly 1 second to scan 50 million converter pages. So in a system with 800 million converter pages runtimes of around 16 seconds can be expected. The related call stack typically looks as follows, indicating a scan of converter pages: PageAccess::Converter::Stream::getCurrent PageAccess::Converter::estimateBackupSize DataAccess::PersistenceManager::estimateBackupSize ptime::BackupSizeEstimationsMonitor::estimateBackupSize The maximum page number (MAX_PAGENUMBER) can be considered as a high water mark of the system while the currently allocated pages (ALLOCATED_PAGE_COUNT) can be much smaller. So if for whatever reason (e.g. many file LOBs, massive garbage collection issues) the number of converter pages was much higher in the past, the MAX_PAGENUMBER will remain on that level and apart from recreating the database it is not possible to reduce it. You can avoid the delays caused by this statement execution when you call transaction DBACOCKPIT instead of transaction ST04, because DBACOCKPIT has another landing page that doesn't require this information. various SELECT MBEW, MBEWH, MBVMBEW, MBVMBEWH, MCHB, MCHBH If tables MBEW, MBEWH, MCHB and MCHBH are already implemented as compatibility views you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. bb45c65f7cc79e50dbdf6bbb3c1bf67e b0f95b51bcece71350ba40ce18832179 SELECT M_CONNECTIONS This SELECT is executed when a SAP ABAP work process connects to SAP HANA. Implement a sufficiently new DBSL patch level according to SAP Note 2207349 in order to replace this query with a more efficient approach based on session variables. 8b0c1d926307b660a1797c05d46162b8 fcdf8f8383886aaf02c3a45e27fbd5f2 SELECT M_CONNECTIONS, M_ACTIVE_STATEMENTS This SELECT originates from function module DB_WP_CURRENT_SQL and is used to determine the statement text and the application source for a database request currently executed in a work process (SAP Note 1902446 ). It is normal that it runs for up to 1 second. A high load is usually caused by a high number of executions that can have different reasons: High number of calls to transaction SM50 / SM66 (manually or due to a monitoring tool) High number of calls to the session monitor in transaction DBACOCKPIT (manually or due to a monitoring tool) Capturing of /SDF/MON data with activated check box \\\"SQL statements\\\" Make sure that monitoring levels and frequency remain on a reasonable level and don't overload the database with monitoring requests. various SELECT M_CONTEXT_MEMORY Long runtimes accessing M_CONTEXT_MEMORY are usually caused by a high number of records in that monitoring view. See SAP Note 2088971 -> M_CONTEXT_MEMORY for possible reasons of a high number of entries. abf60afa26d3529bb3b2ce9599187a3b UPSERT MCSIDIR If this UPSERT operation from program SAPLMCEX fails due to a deadlock you can consider SAP Note 2492966 . f1b12a80a3eb489df86f5e25097725db SELECT M_CS_ALL_COLUMNS This statement is used in context of the SAP Early Watch Alert (EWA) download and selects table column information by specifying the TABLE_OID in the WHERE clause. The TABLE_OID is not supported by SAP HANA for initial filtering, thus, the M_CS_ALL_COLUMNS information is calculated for all tables before the information for the table in question is filtered. SAP Note 3347789 provides a fix by changing the WHERE clause from TABLE_OID to the efficiently supported SCHEMA_NAME and TABLE_NAME filters. various, e.g.: 1964c8ab878cde5f306d9f605e3b94ed 1f989f4acd844d502ccd445f69f5ea02 3596bfa8e0055ff715ef52570ff66128 38a8e11286f7309f2715c07c270a473b 430c496e0fe15c0353c80de1c72caab1 ba81a383d98a296d2e44e04278ccb770 c6edbab2e426f7e83b156ed06ccdf6bb d3759ce6047b78f61d5fc3be392d0336 SELECT M_CS_ALL_COLUMNS M_CS_COLUMNS M_CS_TABLES A particularly high thread activity can be observed in systems where CPU workload management (in particular based on parameter default_statement_concurrency_limit) isn't properly set up, a high number of parallel threads is used and contention on IniFileLock (SAP Note 1999998 ) happens. In this case see SAP Notes 2222250 and 2600030 and make sure that a reasonably small value for default_statement_concurrency_limit is set. With SAP HANA >= 2.00.041 the calculation of values for memory related columns (*MEMORY_SIZE*) in monitoring views M_CS_ALL_COLUMNS, M_CS_COLUMNS and M_CS_TABLES also considers columns that are currently not loaded (fix for \\u00edssue number 198542). This can result in overhead in terms of CPU consumption, JobWorker utilization and runtime when memory related columns are accessed in these monitoring views. Among others the following statistics server actions can suffer: ID 17: Alert_Mon_Column_Tables_Record_Count (statement hash 09ebcf4d5dc0954389551c4b23c05d9a) ID 20: Alert_Column_Tables_Size_Growth (statement hash 1f989f4acd844d502ccd445f69f5ea02, 3596bfa8e0055ff715ef52570ff66128, d3759ce6047b78f61d5fc3be392d0336) ID 27: Alert_Partitioned_Table_Record_Count (statement hash 26daef039f35e5df24eceb94ccdfb462) ID 29: Alert_Delta_Mem_Merge_Dog (statement hashes 38a8e11286f7309f2715c07c270a473b) ID 40: Alert_Mon_Part_Table_Size_Host_Total_Mem (statement hash 430c496e0fe15c0353c80de1c72caab1) ID 45: Alert_Mon_Part_Table_Size_Host_Main_Mem (statement hash ba81a383d98a296d2e44e04278ccb770) ID 5007: Collector_Host_Column_Tables_Part_Size (statement hash c6edbab2e426f7e83b156ed06ccdf6bb) Also accesses in context of SAPHostAgent (user SAPDBCTRL) like hash 1964c8ab878cde5f306d9f605e3b94ed can be affected. Avoid reading memory information from these monitoring views on a regular basis. In order to limit the load spikes / CPU consumption introduced by the statistics server (SAP Note 2147247 ) you can set up a workload class with a maximum thread limit as described in SAP Note 2970921 . Starting with SAP HANA 2.0 SPS 06 this workload class is delivered per default (unless an individual workload class already exists). Make sure that the statistics server related workload class isn't too restrictive in terms of the total thread count (SAP Note 2970921 ). In general you should configure 25 % of the available CPU threads as total thread count, but stay in the range of 4 to 20. You can also consider the following workload parameter setting that limits the maximum parallelism of SAP HANA monitoring view accesses (SAP Note 2222250 ): indexserver.ini -> [row_engine] -> num_parallel_monitor_thread Resource overhead in terms of CPU and memory is reduced with SAP HANA >= 2.00.059.02 and >= 2.00.061 (issue number 266068). c8f682aa3be41b442f4aa42395c5a099 SELECT M_CS_PARTITIONS These selections with TABLE_NAME = 'MAI_UDM_STORE' may be executed way too frequently in FRUN environments. See SAP Notes 3384080 and 3350069 for more details and resolution steps. 3c4d665564c85a6e1fbaf3d149934e5a 4a5b6efc4e44ff925434b049d502f878 6c734feb16ab862202c7221de63131d0 SELECT M_CS_TABLES These queries are linked to the BW sizing report /SDF/HANA_BW_SIZING (SAP Notes 1736976 , 2296290 ). Avoid running it frequently and / or during peak load times. Starting with version 2.5.4 of the sizing report the number of M_CS_TABLES accesses is reduced by factor 3. 53f12e37eda8e9a6f4cade65138ec110 SELECT M_CS_TABLES This query checks for history tables with more than 1 billion records in the history partition: ... WHERE T.RAW_RECORD_COUNT_IN_HISTORY_MAIN + T.RAW_RECORD_COUNT_IN_HISTORY_DELTA > 1000000000 It is related to the \\\"Table histories > 1 billion rows\\\" metric that is executed by Solution Manager / FRun every 30 minutes per default. In the majority of all systems no history tables exist, so the check is not required. You can check via SQL: \\\"HANA_Configuration_Overview\\\" (SAP Note 1969700 ) and check the \\\"History tables\\\" line for the actual number of existing history tables in the system. Depending on the result you can either deactivate the metric or increase the collection interval (e.g. to once per day) for the \\\"Table histories > 1 billion rows\\\" metric in Solution Manager / FRun. See SAP Solution Manager System Monitoring for more information. 4268da3d3e23cf17374d2d849e34927c 7fec4d6bc9b347a461c3edfc739184af fea41d9c262ad1d7413b1dfe33c61ff1 344262f32bb29df38139b498a712987e 2986820e47db8abf2a2a8341e9b81af4 fa0d29c997bcd593669b75226fcff4ff 0ff809b4d4c723a6d1b3d8835d77d89a 1d612f19e240183a67f0434bba4dbdd7 8946ba4387f1fe2dbcda9433fe5c42ef 2fed6e6d601555b4330f6bbe02a985aa d19f00cc8330e8de6ae4d78498229642 SELECT SELECT SELECT SELECT M_CS_TABLES M_RS_TABLES M_SERVICE_MEMORY M_HOST_RESOURCE_UTILIZATION These accesses are standard monitoring queries. If they are executed frequently, they are most likely issued by the SAP HANA monitoring of the SAP Database Migration Option (DMO) - \\\"SAPuptool hdbmonitor\\\". You can reduce or disable the amount of queries with the following SUM parameters (SAPup.par / SAPup_add.par): /proc/dbmonsleep: Frequency of monitoring checks in seconds (default: 30) /clone/hdbmonitor: Activation / deactivation of SAP HANA monitoring (default: ON), can be set to OFF in order to disable the accesses completely 76423bbac93cdca6dec17fed34496465 SELECT M_DATABASE The selection SELECT VERSION, CURRENT_USER FROM SYS.M_DATABASE is executed by JDBC clients (SAP Note 2393013 ) when establishing a connection to the database. Consider the following optimizations: Check if it is possible to keep connections open for a longer time rather than to establish and close connections with high frequency. A high number of parses (SAP Note 2124112 ) can be caused by the requirement to change the password for the connection user (e.g. due to force_first_password_change = true). In this case the JDBC client parses and executes the query before it demands a password change and terminates in case there is no user interaction. In this scenario the invalidation reason is OBJECT VERSION MISMATCH(OBJ-ID:<user_obj_id>). Make sure that JDBC connection users, particularly in context of automatic monitoring tools, are able to connect to the database successfully without a requirement to adjust the password. See also the below general M_DATABASE performance discussion. 3124bc8c7b10028d6797d19db8992d58 76423bbac93cdca6dec17fed34496465 94927f293e398e2e45d82d96a19a96a6 SELECT M_DATABASE Accesses to M_DATABASE are typically quick. Slowness has been observed in context of operating system calls related to time and timezone. Typicall call stacks are: __lll_lock_wait_private __tzset __GI_timelocal str_as_time NameServer::TNSClient::getMinStartTime ptime::DatabaseMonitor::update ptime::DatabaseMonitorHandle::create_objects ptime::Monitor_scan::do_open __lll_lock_wait_private __tz_convert ranged_convert __mktime_internal str_as_time NameServer::TNSClient::getMinStartTime ptime::DatabaseMonitor::update ptime::DatabaseMonitorHandle::create_objects ptime::Monitor_scan::do_open In this case the following details can be checked: Proper timer / timezone configuration on SAP HANA and OS level (SAP Note 2100040 -> __tz_convert) Collection of additional operating system details at the time of the problem: OS kernel stack back trace (SAP Note 2166414 ), gstack (SAP Note 2257203 ) Checking for potentially responsible kernel taints via sapsysinfo.sh (SAP Note 618104 ) 48227050702b6788417bcdd09e6c6760 UPSERT MDG_MDF2035 The table MDG_MDF2035 is used contains information for the master data governance (MDG) timestamp service and transactional lock wait contention is possible in scenarios with an extremely high amount of concurrent changes. In this case SAP Note 2228078 can be considered to deactivate the MDG timestamp service if possible. de32c5c30fab83809bdc8baeec360c63 SELECT MDMA This SELECT issued by method ASSIGN_MRP_AREA of class CL_PPH_MRPRECORD suffers from an empty FOR ALL ENTRIES list, so that only MANDT is used in the WHERE clause and basically the complete table content is retrieved. SAP Note 2260410 provides an application correction. 02a1e492e2f681959509afca5b3a20b3 346fa3f331c1986006ada8918aee1d90 58b942966bd98138845ccca5e5513ea9 6e40b2c493f0fffbe39d79ff66bc1af8 bf12292cbd91dfd4d56053f397224271 ea4958c3e5667e0038894be6674ab4aa SELECT _SYS_SR_SITE_<site_name>.M_EVENTS SELECTs to M_EVENTS on remote system replication sites via the system replication proxy schema _SYS_SR_SITE_<site_name> can suffer from network problems or unavailabilities related to the secondary system. In the worst case selections get stuck until the connection timeout (default: 180 s) is reached. As a consequence the following statistics server (SAP Note 2147247 ) actions can take a longer time: ID 20: Alert_Internal_Events ID 30: Alert_Internal_Disk_Full_Events ID 78: Alert_Replication_Connection_Closed ID 89: Alert_Missing_Volume_File See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. As a temporary workaround it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout various SELECT MKPF If this table is already implemented as compatibility view you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 8e2be385cf6cfba7e80a7c3fc4be81a6 SELECT M_LANDSCAPE_HOST_CONFIGURATION This access is triggered by ABAP report SAPLSRFC respectively function module RFC_SYSTEM_INFO respectively ABAP kernel call RFCSystemInfo. This expensive ABAP kernel check exists in ABAP kernels 7.85 (<= 220) and 7.89 (<= 62) and is fixed with newer kernel versions. See SAP Note 3265730 for more information. various, e.g. 455a6193f1440e44490f0e8d5ee87d0a 8cdf6af3445a91a4f3a9125cb1dbb341 cc8716b21968abdd86f26faea08ceaa3 cc8c76c98e842c0db137e2a03895f2bc ebd30763da6cf827578153bb37f3ed6e SELECT M_LICENSE Accesses to monitoring view M_LICENSE involve communication with the nameserver. The thread waits in state \\\"Network Poll\\\" while the nameserver processes the request and a typical call stack looks like: __GI___poll ... NameServer::TNSInfo::sendRequestTo NameServer::TNSInfo::processRequest NameServer::TNSClient::processRequest NameServer::LicenseClient::getLicenseInfo ptime::LicenseMonitor::create_objects ptime::Monitor_scan::do_open Thus, problems of the nameserver can result in slow or hanging M_LICENSE requests. Double-check the health of the nameserver. SAP Note 3343278 describes one possible root cause of a hanging nameserver with SAP HANA <= 2.00.059.09, <= 2.00.067.02 and <= 2.00.072. Accesses to M_LICENSE can happen in the following contexts: License check (e.g. based on request lic/trigger_update_license, SAP Note 2114710 ) Statistics server (SAP Note 2147247 ) operations (ALERT_LICENSE_EXPIRING, ALERT_PRODUCT_PERCENTAGE_USAGE, COLLECTOR_TEL_LICENSE, COLLECTOR_TEL_LICENSES) Monitoring tools 8bde35673a45c0b2984b24cf199dc175 SELECT M_LOAD_HISTORY_HOST This SELECT is executed by SAP HANA Cockpit (SAP Note 2185556 ) in XS classic when the CPU tile is called. A high number of executions is typically linked to a configured automatic CPU tile refresh. In order to reduce the load you should either reduce the refresh interval or deactivate automatic refresh (Settings -> Automatic Tile Refresh). 4c93c09e31313c80e1e5c427d174d86a SELECT _SYS_SR_SITE_<site_name>.M_LOG_SEGMENTS SELECTs to M_LOG_SEGMENTS on remote system replication sites via the system replication proxy schema _SYS_SR_SITE_<site_name> can suffer from network problems or unavailabilities related to the secondary system. In the worst case selections get stuck until the connection timeout (default: 180 s) is reached. As a consequence the following statistics server (SAP Note 2147247 ) actions can take a longer time: ID 72: Alert_Log_Segment_Count See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. As a temporary workaround it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout various SELECT M_PREPARED_STATEMENTS Queries on M_PREPARED_STATEMENTS can be expensive when many entries exist in M_PREPARED_STATEMENTS. See SAP Note 2088971 -> M_PREPARED_STATEMENTS for possible reasons of a high number of entries. various SELECT M_RS_TABLES Accesses to monitoring view M_RS_TABLES can be expensive in case of large row stores. Check if you can reduce the row store size, e.g. by data management and archiving or by moving large tables to column store. With SAP HANA >= 2.00.080 the implementation of M_RS_TABLES is improved (Jira HDBDEVSUPPORT-160), resulting in reduced runtimes. faf34398b15a9f96958d83a267b99ab4 SELECT _SYS_SR_SITE_<site_name>.M_SAVEPOINTS SELECTs to M_SAVEPOINTS on remote system replication sites via the system replication proxy schema _SYS_SR_SITE_<site_name> can suffer from network problems or unavailabilities related to the secondary system. In the worst case selections get stuck until the connection timeout (default: 180 s) is reached. As a consequence the following statistics server (SAP Note 2147247 ) actions can take a longer time: ID 54: Alert_Mon_SavePoint_Duration See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. As a temporary workaround it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout various SELECT MSEG If this table is already implemented as compatibility view you can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 066d8841facca43e927ef61253ccd2d1 2a4f9ce41e671c3043ca01ee2f53c482 6335cad129f59838e2077de33f61e599 66e2c312f4556658bbb191a63ec4eb32 8f3a3253cd1fc89d43ea42d7532b1015 b00125455f78c4d794fba903eea620aa c0aa71d9f784996b626e589b74544c38 c47f784b695a3f686bd010b765419faf cdd18181642050527b4a1a95d00e3f3e d952da8472c0d4350559843cbfd48d0e ef7d4dc44f62c30868abd8bcdaaacd57 SELECT M_SERVICE_REPLICATION SELECTs to M_SERVICE_REPLICATION read some data from the secondary system replication site. They can suffer from network problems or unavailabilities related to the secondary system. As a consequence starting ABAP transaction DBACOCKPIT (SAP Note 2222220 ) can take a long time (statement hash b00125455f78c4d794fba903eea620aa). In addition, the following statistics server (SAP Note 2147247 ) actions can take a longer time: ID 94: Alert_Replication_Logreplay_Backlog ID 104: Alert_Replication_Logshipping_Backlog ID 5046: Collector_Host_Service_Replication See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. For SAP HANA <= 2.0 SPS 05 see SAP Note 3392865 and make sure that outdated entries in the DNS cache can't impact the query performance. Starting with SAP HANA 2.0 SPS 06 the DNS cache is purged every 5 seconds, so it should no longer have an impact. As a workaround in case of an unavailable secondary system you can disable the secondary system check temporarily (SAP HANA Rev. >= 1.00.102.03 and >= 1.00.111): global.ini -> [system_replication] -> check_secondary_active_status = 'false' Alternatively it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout See SAP Note 2736804 for more details. 09214fa9f6a8b5322d329b156c86313b 28996bd0243d3b4649fcb713f43a45d7 7439e3c53e84b9f849c61d672da8cf79 84e1f3afbcb91b8e02b80546ee57c537 9ce26ae22be3174ceaffcfeee3fdd9b7 f32de4a673809ad96393ac60dfc41347 SELECT UPDATE M_STATISTICS_LASTVALUES STATISTICS_LASTVALUES This SELECT is executed by the Solution Manager in order to read history data from the statistics server. It is particularly critical if the standalone statistics server is used. The UPDATE is executed by the standalone statistics server in order to update statistical information. Implement SAP Note 2147247 (\\\"How can the memory requirements of the statistics server be minimized?\\\"), so that the collection and processing of statistical data is optimized. A switch to the embedded statistics server is typically already sufficient to resolve this issue. 3795b564b0c728737e695b1021ac3582 SELECT M_SYSTEM_OVERVIEW Accesses to monitoring view M_SYSTEM_OVERVIEW can be slow if the retrieval of disk information (thread method: remotediskinfo) takes longer than expected. Check the threads / thread samples for remotediskinfo (SAP Note 2114710 ) and check operating system / hardware for bottlenecks (SAP Note 1999930 ) in case remotediskinfo calls are a major contributor the runtime. 100b6ebd38bbbc84c20e887f4c51339e 32b53f05a6d99d8e55f349cc1d9580e5 35c3ed2d193b5930001b02832ead9245 7d80c6be4848f2f9fe778fbe81427fb4 a0adfca9b52ccf6778f8d5d0e1efb0cb SELECT M_SYSTEM_REPLICATION SELECTs to M_SYSTEM_REPLICATION read some data from the secondary system replication site. They can suffer from network problems or unavailabilities related to the secondary system. As a consequence starting ABAP transaction DBACOCKPIT (SAP Note 2222220 ) can take a long time (statement hash a0adfca9b52ccf6778f8d5d0e1efb0cb). Also some statistics server (SAP Note 2147247 ) queries can suffer. See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. As a workaround in case of an unavailable secondary system you can disable the secondary system check temporarily (SAP HANA Rev. >= 1.00.102.03 and >= 1.00.111): global.ini -> [system_replication] -> check_secondary_active_status = 'false' Alternatively it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout See SAP Note 2736804 for more details. 4db908d3710d6c3639b2a9e48322d10a SELECT M_TABLE_PERSISTENCE_STATISTICS, M_CS_TABLES_ This query is triggered by the SAP HANA license check (lic/license_measurement) on an hourly basis starting with SAP HANA SPS 12. It is usually running with a reasonable performance. In systems with a low overall workload it can nevertheless still be among the top SQL statements. 9a2ce762f4885e347f4567f9ee27c35d SELECT M_TABLE_PERSISTENCE_STATISTICS, TABLES This query is triggered by the SAP HANA license check (lic/license_measurement) on an hourly basis. The underlying table M_TABLE_PERSISTENCE_STATISTICS is quite expensive and so runtimes of several minutes in larger systems are acceptable. Starting with SAP HANA 1.0 SPS 12 the license check was adjusted and so this expensive query is no longer executed. 9d3480ff01bd8fc51759b4e9c8341e0d fe769268583b8b8715593ac10f0c1fd8 SELECT M_TEMPORARY_TABLE_COLUMNS This query retrieves columns for temporary tables following the <sid>.<client>.<guid32> naming convention (SAP Note 2800007 ) in context of preparations for BW query processing. Thus, you have to accept a certain load coming from this query in case many BW queries are executed in the system. Queries on M_TEMPORARY_TABLE_COLUMNS can suffer from a missing CATALOG READ privilege. Proceed according to Long runtime of query on SAP HANA dictionary objects and monitoring views . Situations have been observed where a high number of active, but waiting JobWorkers were spawned, impacting the stability of the database (issue number 283156). If a similar scenario happens, you can capture runtime dumps (SAP Note 2400007 ) and open a SAP case on component HAN-DB-PERF for a more detailed analysis. various, e.g.: 0b7761c6b9ffcda1debae659cfd7a53e SELECT M_TEMPORARY_TABLES Queries on M_TEMPORARY_TABLES can suffer from a missing CATALOG READ privilege. Proceed according to Long runtime of query on SAP HANA dictionary objects and monitoring views . 6ea085309583e2436162a48b8cc62cb4 SELECT M_TEMPORARY_TABLES This selection with a WHERE clause consisting of SCHEMA_NAME and TABLE_NAME originates from report TREX_EXT_INDEX_CELL_TABLE. Among others it can be frequently triggered by the ESH real-time indexing job ESH_EX_FU_DEMON. The related application source is the generic CL_SQL_STATEMENT class. See SAP Note 3225546 for an application optimization. 0ad1a4c1c1a5844d01595f5b3cdc2977 4592fda53cacd47ac7263ae58c4f585d SELECT M_TRACEFILE_CONTENTS This SELECT is regularly issued by the backup console in SAP HANA Studio. In order to minimize the impact on the system you should open the backup console only when required and not permanently and avoid large sizes (> 50 MB) for the backup.log file. 376071168f97a75c7ada05cda1a612ec SELECT M_TRANSACTIONS_ M_TRANSTOKEN_DIRECTORY_ This query is executed in the context of the kernel sentinel (SAP Note 2800055 ) and it can be very expensive when many transactions exist with TRANSACTION_ID = -1 (issue number 258410, fixed with SAP HANA >= 2.00.054). You can improve the performance by checking and eliminating reasons for a high amount of transactions with TRANSACTION_ID = -1. 2820eb46f5c1af6dbfc7e1a890f605a2 SELECT M_UNDO_CLEANUP_FILES Selecting the number of cleanup files from M_UNDO_CLEANUP_FILES via the hdbsql client is related to standard garbage collection monitoring implemented in SAP ECS environments using SAP HANASitter (SAP Note 2399979 ): -cf \\\"M_UNDO_CLEANUP_FILES,WHERE,TYPE='CLEANUP',9999999999\\\" This monitoring is currently reviewed and improvements (e.g. reduction of collection frequency or deactivation) are planned for Q3 / 2023 (Jira MCDGSSDBOPERATIONS-13603). The selection can be particularly expensive if many cleanup files exist. See SAP Note 2169283 and make sure that garbage collection isn't unnecessarily blocked for a long time. 239985e53e6238f61141da5e9e694f32 b0101e8a31307c6f509cbc726e348f21 and others SELECT NAST Accesses to table NAST with selection conditions on columns KAPPL and OBJKY can sometimes suffer from an inadequate order of condition evaluation. Typically, OBJKY is most selective, but if during first parsing a highly selective KAPPL value is used, KAPPL may be used as a starting point of the execution plan, leading to bad performance when later on less selective KAPPL values are processed. In order to stabilize the access, you can define a statement hint (SAP Note 2400006 ) as follows: SAP HANA <= 2.0 SPS 05: CS_FILTER_FIRST(\\\"OBJKY\\\"), NO_CS_PRIMARY_KEY SAP HANA >= 2.0 SPS 06: CS_FILTER_FIRST(\\\"OBJKY\\\"), USE_HEX_PLAN 002a7edbc17efff01430ea0c727b7458 11df5737a4a42ed26e0121151c778785 12eeafbd3aae528306a8158ec8606fe9 165a0f6e267af880a21e667e84dfb311 4476d72f3fd4f3fba01fd14ff1f92be4 5e852025fa9822ebada6b23500c87d8e 62ccf74416521350984e10a991cf71a3 6b1d10732401fe82992d93fa91f4ae86 6e8a0d4379e7da3618e69fa4c72d3312 7e2f1e1dda7eaa7f552f8cce75614838 891cf32d4e5f766efcac3070b576597f 89aef8e08979481bdcfb712dd1796e49 9968071caf7c6402f1b89e85b3d1439d b3687e541a32eab7c40f1dd2c4af6f4d baed6a80eae9d5e505c41cd07d125624 c3d7d15bec5e66ec6ab15e86527bcca5 ed36d9e0501d5392cbde83889ed2ff54 SELECT FOR UPDATE NRIV NRIV_LOKAL Check if you can reduce the critical time frame between SELECT FOR UPDATE and the next COMMIT. Identify the involved number range objects from application perspective and optimize their number range buffering. The NRIV_LOKAL access is related to the local number range buffering approach. As described in SAP Note 504875 this approach can result in exclusive lock waits in case of long running batch jobs requiring new numbers. See SAP Notes 599157 and 840901 and consider using parallel number range buffering in order to reduce the risk of lock situations. Be aware that bad performance for the SELECT FOR UPDATE on NRIV and NRIV_LOKAL can also be a consequence of some underlying general issues, so if it happens occasionally and in combination with some other issues, it can be a symptom for another problem and not an issue itself. Depending on the impacted number range object you can proceed with buffering as follows: RF_BELEG: SAP Note 1398444 ; With SAP S/4HANA >= 1610 buffering is activated per default for RF_BELEG (SAP Note 2376829 ), taking advantage of table NRIVSHADOW for persisting currently used number range values. RV_BELEG: SAP Note 1524325 BW DIMIDs and SIDs (BIM*, BID*): SAP Note 857998 afd1b7b7694d54bca2fc84117792ffbf SELECT FOR UPDATE _SYS_REPO.NRIV Table NRIV in schema _SYS_REPO is responsible for number range handling in context of SAP HANA tasks like activations (DOMAIN = 'activation_id'). Concurrent activations serialize on NRIV locks, so to reduce locking you have to avoid concurrent activations and check for reasons resulting in particularly long running activations. 60c366c73e378be600fe0a0621cb36a8 DELETE NRIVSHADOW Increased runtimes including locks and deadlocks (SAP Note 1999998 ) can be a consequence of the ABAP program error described in SAP Note 2357744 . 0e35685e8bbd908e025f3eca1f063085 17629effd42806185aec18c7d090a7db 7c1c0248cd1601fee27aa2ab8f244d8d SELECT NRIVSHADOW This query is expensive because the INSTANZ column isn't specified in the WHERE clause and so the existing primary key can't be used efficiently. SAP Notes 2257975 , 2790738 , 2924805 and 3055954 provide coding corrections that reduce the number of NRIVSHADOW requests without the INSTANZ condition. If the remaining NRIVSHADOW accesses without INSTANZ are still responsible for significant load and runtime, you can create a secondary index on columns OBJECT, SUBOBJECT, NRRANGENR and TOYEAR to support the query without INSTANZ optimally. A high number of records (millions) in NRIVSHADOW can also be responsible for increased runtimes. See SAP Note 2530392 and consider a cleanup using report NK_REORGANIZE. various SELECT NSDM_V_MARC, NSDM_V_MARD, NSDM_V_MARDH, NSDM_V_MCHB, NSDM_V_MCHBH, NSDM_V_MKPF, NSDM_V_MSEG These objects are compatibility views (e.g. NSDM_V_MARC -> compatibility view on table MARC). You can check question \\\" How can compatibility view accesses be tuned? \\\" below for more details related to compatibility view tuning. 9c560d67815987d3a20359ecc2251b6b SELECT NSDM_V_MCHB This query originating from line 761 of report SAPLCOWB is optimized with the correction available in SAP Note 2737478 . b29bb65fa699a56349dfa9436f523eb6 SELECT NSDM_V_MSSA NSDM_V_MSSQ The existence check on tables MSSA or MSSQ in transaction MB52 can take some time. SAP Note 2830243 provides an optimization from business perspective. 7c99e482e615c0beba986c58e91c0c97 SELECT NSDM_V_V_CF_MCHB This query originating from application source SAPLV01F:1479 respectively line 22 of function module VB_READ_BATCH can be expensive when it is processed with FDA WRITE (SAP Note 2399993 ) because the underlying view NSDM_V_MCHB_DIFF contains some SUM aggregations of CASE expressions and these expressions are calculated first before data is aggregated or filtered. In order to bypass this issue you can implement SAP Note 3115064 that disables FDA WRITE. 953dc8254b96ed3f0af9fbcdbc2513ed SELECT FOR UPDATE NWPLACE_AD1 In program SAPLNPA10_WP the system saves the user variant for Clinical Process Builder even when there were no changes. This can result in unnecessarily long record lock waits (SAP Note 1999998 ). Implementing SAP Note 2737474 ensures that personal settings are saved only when there were changes done. 022a0656e4b70c0e2124497d9cb56075 060a6bf7aa804dae3c55a08136dbe38e 11c7cb3e6ab61c8e9dc01ffe3d673f03 2c578e3d400023f7f367f0860f8b55a0 3a8b3213ddae57ed4f037c19a66d067b 3ba12256f9f2a2b79e75095cd817ad9f 5a0a91f1b589d2cb7d0f55be856be0b2 6ac093ce5b7210ce431f81f7fd798a18 720bb6c662543b066728ee98c6e129ad 7b0b3c36174db29a794b82eebb0d9fc1 884514c4f1ae4cd7aebe63d3bc816167 a227ea3fecf3c34a4d3fdff1488ad005 b5bb116f425f16a50d63c3ea9fd8a4ae c513a46e8923e772c4e5343a0cac82ff cc5a32805a0924dc52d1e8a236711e8c cf4b4a85ebe1be6ffe754d6f7fa08e86 e245088da460600132029e0bcfb9775e ef9d983128f4be754244d5be4aeea34c DELETE ODQDATA Deletions of ODQDATA objects based on TID IN lists from method DELETE_BUCKET of class CL_ODQ_CORE_SERVICE (application source: CL_ODQ_CORE_SERVICE===========CP) may sometimes use a full table scan rather than using the primary key to filter the TID list. Implement the statement hints available via SAP Note 2700051 in order to force a good primary key access for IN lists with lengths <= 10 elements. The deletions may also suffer from the fact that SAP HANA can't evaluate IN lists on DECIMAL columns properly with Revisions <= 2.00.037.04 and <= 2.00.045 (SAP Notes 2914233 ). See SAP Note 2842894 for recommendations how to bypass this issue from application side. SELECT OPERATION Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 32139366d69f10543bc7abfc605b37a2 DELETE /PLMB/FRW_BUFFER This DELETE can suffer from exclusive record lock waits due to an inappropriate application COMMIT strategy. See SAP Note 2236233 for a coding correction. various (due to explicit MANDT literal), e.g. ca5feffa109ed4f58a92e5508232cc13 DELETE /PLMB/SEA_OBJMAP The deletion from /PLMB/SEA_OBJMAP based on the result of a join with tables DRAD and STXH is executed in context of batch job /PLMB/SEA_EXTRACT_OBJKEY on a 3 minutes basis. Due to the complex join with string manipulations like SUBSTRING and TRIM it can be quite expensive. SAP Note 3306740 provides a fix by eliminating this statement. 5468e2292c2d169ac5d318f9f08f287a SELECT POC_D_EVTQ This SELECT originating from report POCR_PROCESS_EVENTQUEUE_UNI suffers in case of a large number of records in table POC_D_EVTQ because at first all records of the current client are sorted and finally the first records are returned. See SAP Note 2071310 and make sure that the process observer event buffer tables are cleaned in time so that they don't grow to large sizes. 4670a96d6abf4b048d40d28fbf1f466a SELECT PPOIX, PPOPX Bad performance of extractor 0HR_PY_PP_1 can be improved by activating the secondary indexes PPOIX~001 and PPOPX~001. See SAP Note 2285753 for more information. 2dcac2da4115847242aed4c2e1d9cb38 SELECT PROCEDURE_PARAMETER_COLUMNS This query retrieves metadata when processing XS engine requests. In particular it is needed when a stored procedure containing one or more in-place table parameters is being called from within an XS classic application, e.g.: create procedure proc_inplace_tbl (in a table(id int), out b table(id int)) language sqlscript reads sql data as begin b = SELECT * FROM :a; end; You can avoid these metadata lookups by specifying a table type or table name as parameter type , e.g.: create type tt_abc as table (id int); create procedure proc_tbl_type (in a tt_abc, out b tt_abc) language sqlscript reads sql data as begin b = SELECT * FROM :a; end; create table t_abc (id int); create procedure proc_tbl (in a t_abc, out b t_abc) language sqlscript reads sql data as begin b = SELECT * FROM :a; end; a77c6081dd733bd4641d4f42205f6c84 3359a412c4e756a428bcafedd78471d9 5f74d263c5c217551d4afc30848c1791 SELECT FOR UPDATE QIWKTAB This SELECT FOR UPDATE can suffer from row store garbage collection activities if the QRFC scheduler issues a very high amount of updates on a few records of this table. This typically happens if there are STOP entries in transaction SMQ2 that never can be processed, but result in a looping scheduler check. Go to transaction SMQ2 and check if there are STOP entries in any client which can be removed. After cleanup the update frequency should significantly reduce and the row store garbage collection is able to keep up without problems. See SAP Note 2169283 for more information related to garbage collection. SAP Note 2125972 provides an application correction to reduce the amount of updates on QIWKTAB. 5331b867628b64a02706379d271bef5b INSERT QRFC_I_QIN_LOCK This insert from class CL_BGRFC_UNIT_HANDLER_INB_Q is used to synchronize concurrent accesses to the same qRFC queue. In case of many parallel work processes accessing the same queue, significant numbers of unique constraint violations are possible, resulting in exception handling overhead that is a normal consequence of many unique constraint violations. See check ID C0400 (\\\"Exception unwinding\\\") of SQL: \\\"HANA_Threads_Callstacks_MiniChecks\\\" (SAP Note 1969700 ) for details (SAP Note 2313619 ). Check from application side if it is possible to reduce highly parallel accesses to the same qRFC queue in order to reduce unique constraint violations and exception handling overhead. Make sure that the fix from SAP Note 2124871 is applied in the system. various SELECT R_COLLSPROMISETOPAYINVOICETP Selections from CDS view R_COLLSPROMISETOPAYINVOICETP (and other tables) originating from UDM_SUPERVISOR can be expensive because predicate pushdown may be blocked by outer join processing. SAP Note 3335213 provides an optimization. 5b9df4f9bb94ffd6ef445e2a7d8d3ad2 \\ufeffSELECT REMOTE_SUBSCRIPTION_DATA_CONTAINERS\\ufeff This selection is triggered by the DPReceiverHouseKeeping thread in context of SDI (SAP Note 2400022 ). Proceed according to SAP Note 3258907 in order to reduce runtime or execution count. 8394dec3bca547c12e1728a859ab3952 87f6d1130f139af11559242490feca24 SELECT REPOLOAD Selections on the ABAP report load table can suffer from delays reading LOB files from disk. In this case you typically see I/O related information in the threads, e.g. \\\"Resource Load Wait\\\", PrefetchIteratorCallback or PageIO::SyncCallbackSemaphore. See SAP Note 1999930 and make sure that I/Os are processed efficiently. For example, bottlenecks in the I/O stack, overloads due to savepoints of huge table optimizations or resource container unloads can result in unnecessarily long I/O read times. 15f8c397054d817883a13955fe18855f ce0d9cedb9b4a0194dd551c7f06c22ad d339f40ed4667a80eef50d8d130e5dec CALL REPOSITORY_REST REPOSITORY_REST is the central procedure for activations based on the repository API. It can be quite expensive in case of comprehensive activations and a lot of dependencies. Among others it is implicitly called when using the perspectives for Modeling, Development and Administration in SAP HANA Studio. Starting with SAP HANA 2.0 SPS 03 this API is deprecated and should be replaced with the SAP HANA deployment infrastructure (HDI). See SAP Notes 2425002 and 2465027 for more details. 33f3b2704fa52cb1b85b047a5def4f01 3fa9f17cac39444f386b6f8a4638ffff 80cd171d058d64da0c342ed85fb1c6a6 bbe68180a2e6fcaec3edc6007985f932 b28dca04ced6f8b3495e3970c0ef36a7 cd2266401b89265e8b51686955960c63 SELECT FOR UPDATE UPDATE REPOSRC These lock waits are typically linked to SAP ABAP compilations. Typical reasons are: Imports of transports in an online system so that ABAP recompilation is required; try to schedule imports during non-critical time frames. ABAP support package upgrades in parallel to business workload; plan them based on SAP Note 1803986 in order to minimize the impact on the system If REPOSRC locks happen on a frequent basis in combination with ABAP dumps like DDIC_TYPE_INCONSISTENCY, DDIC_TYPE_REF_ACCESS_ERROR, DDIC_TYPES_INCONSISTENT, LOAD_PROGRAM_CLASS_MISMATCH, LOAD_PROGRAM_INTF_MISMATCH, LOAD_PROGRAM_MISMATCH, LOAD_PROGRAM_TABLE_MISMATCH, LOAD_TYPE_VERSION_MISMATCH, SYNTAX_ERROR or TYPELOAD_NEW_VERSION it can be caused by inconsistent objects. In this case you have to look out for the impacted objects and repair / regenerate them. These ABAP dumps can also show up in context of the problem described in SAP Notes 2172127 and 2182690 (permanent recompilations in context of ABAP table structure changes like the creation of indexes). In this case it is required to restart the SAP application servers. SAP Notes 2831890 and 3069620 describe REPOSRC lock scenarios caused by bugs in the ABAP kernel. ef04ace2ae2ff448ef0b19ca999a6d37 UPDATE ROOSPRMSC This update is executed in function module RSA1_DELTA_REQUEST_WRITE and can suffer from lock contention in case when extractions of the same data source are simultaneously done for SAPI and ODQ targets. As described in SAP Note 2660461 it is an intended behavior that simultaneous extractions are blocked. You need to adjust your scheduling of extractions to make sure that the extractions for the same data source happen at disjoint times if you want to avoid the lock contention. The related table columns are: OLTPSOURCE: Data source with overlapping extractions SLOGSYS: Source system (i.e. system where the lock contention happens) RLOGSYS: Identifier for receiving system (e.g. <sid>_<client> for SAPI and $ODQ_DUMMY for ODQ target) 806953dd0df65b4f67c0de0c33d28da7 SELECT FOR UPDATE RSAPOADM Check if the correction provided in SAP Note 2371147 can be used to resolve the problem. 47b3bd84eec1cce26c1ba82cde2ee2a0 INSERT RSAU_BUF_DATA A high number of inserts into table RSAU_BUF_DATA can be a consequence of too fine-grained security audit logging. See SAP Note 2191612 for more information related to the security audit log and make sure that it is configured as slim as possible and required from security perspective. Due to the fact that the primary key contains a timestamp on seconds basis it is possible that a significant amounf of unique constraint violations happen that can result in system CPU consumption and exception unwinding as described in SAP Note 2313619 -> C0400 (\\\"Exception unwinding\\\"). efd74759c6dc35f70ccd3529b62588f3 SELECT RSBERRORLOG The runtime is high because of the ORDER BY clause that can't be supported by the primary index with SAP HANA. See SAP Note 2117500 and implement the coding correction or upgrade to a newer SAP BW support package level to eliminate this ORDER BY. 7551257378ec73e2ba93b5e5117b6c8f UPSERT RSBMREQ_DTP Increased UPSERT times on a partitioned RSBMREQ_DTP table with SAP HANA <= 112.06 and <= 122.03 can be caused by the problem described in SAP Note 2373312 . dbff6e35c53899c2c47e38e671f47386 INSERT RSBKDATA The table RSBKDATA is used as runtime buffer for the BW Data Transfer Process (DTP). In case of a very high number of INSERTs you can determine the usual requests (column REQUID30) using this table and check if they can be adjusted, e.g. by switching from full to delta load or by reducing the load frequency. 1d2893314f809f3e491e5628f6683ea5 3ff7bab8b9778b77b821816c3a147f81 6662b470dc055adfdd4c70952de9d239 93db9a151f16022e99d6c1c7694a83b0 c706c6fb087b78f6d07b5ae0848179a3 ccbffaf1607aab010cbd4512626e10fd d740f122ae4c4ae3664a09482c249947 df4382e56e6c6b193dac9ac7ab4d7897 SELECT RSDDSTATEVDATA Consider reducing the execution frequency of the related Solution Manager extraction \\u201cWORKLOAD ANALYSIS (BI DATA)\\u201d. See SAP Note and delete / archive old records from RSDDSTATEVDATA. The minimum STARTTIME in table RSDDSTATINFO is from 30 days ago, so the actual retention time is 30 days. You could reduce it via parameter TCT_KEEP_OLAP_DM_DATA_N_DAYS (SAP Note 891740 ). 07a4169722d9848d8150f53037f3a6fe UPDATE RSDD_TMPNM_ADM Table RSDD_TMPNM_ADM contains counters for temporary BW table names. Only a few records are updated frequently. In row store this can result in garbage collection related performance issues (see SAP Note 2169283 ). Therefore it is recommended (and also default with newer SAP ABAP releases) that RSDD_TMPNM_ADM is located in column store. You can move the table to column store using the following command: ALTER TABLE RSDD_TMPNM_ADM COLUMN 3d408762388035d1c8534ad62e157c8b a70c4031ece0e895dfaaf4c4e9acd690 SELECT FOR UPDATE RSDVENQUEUE Long runtimes of a SELECT FOR UPDATE triggered from an application source like SAPLRSDV:6780 due to transactional lock contention is possible due to inadequate application coding. SAP Note 3136261 provides a coding correction that is also available with support packages SAPK-20011INDW4CORE respectively SAPK-30001INDW4CORE. 915d86cf95219e5c3c51a5aad207e19d SELECT RSHIEDIR If a high number of selections from table RSHIEDIR happen, you can check if SAP Notes 2078190 and 2742363 can help to reduce the execution frequency. 0e30f2cd9cda54594a8c72afbb69d8fd a115fd6e8a4da78f0813cfe35ffc6d42 00218f32b03b44529da647b530bf4e6d 8a8ec2baa7873e835da66583aa0caba2 470c9174c550352f622b5c476a622589 b51046ad3685e1d45034c4ba78293fd8 ec91f8ecc5030996f1e15374f16749a8 f0a4785a6ada81a04e600dc2b06f0e49 02400fb6f17140aba360cc2753c975d3 ceaedf22c0a0de014a6191520d723b81 52063e6acb3492518389d230490492f7 a3df1aa41b220869d74683a8fe9b1f98 c3a9a6620824cd0c931ae3a0e88942c6 8d87d6a303389d8a3f2c72639910da49 78cfbe0776760c57376389aa595672ff 4fe60bbfbfa2979b89e8da75f5b2aac7 ca8758b296bd2321306f4fee7335cce5 2aeb4f7ffd47da8917a03f15a57f411a SELECT SELECT SELECT SELECT SELECT SELECT SELECT SELECT RSICCONT RSSELDONE RSMONICDP RSMONMESS RSSTATMANPART RSSTATMANREQMAP RSSTATMANSTATUS TESTDATRNRPART0 If theses queries return a significant amount of records (> 100), it might be caused by a high number of existing requests. In this case you can check the following: Run SQL: \\\"HANA_BW_DataTargets\\\" (MIN_REQUESTS = 10000) available via SAP Note 1969700 or check table RSMDATASTATE_EXT manually for data targets with REQUESTS_ALL > 10000. See SAP Note 2037093 and reduce the request lists of data targets with more than 10,000 requests using programs like RSSM_AUTODEL_REQU_MASTER_TEXT and RSSM_REDUCE_REQUESTLIST. See SAP Note 2049519 and make sure that you don't run into follow-up problems caused by reduced requests. fb1a593e493a9443415b1b5207054e68 912e1379e9ebf67109eb00df87dd2d76 DELETE UPDATE RSIX_MANDTINDEP The changes to this table depend on the BW configuration RSD_UNION_DEL_SHRD_BUFF_RFC that can be maintained in table RSADMIN. As described in SAP Note 2686186 setting this parameter to ' ' can lead to locking issues during times of increased system load and so transactional SAP HANA locks can be observed (SAP Note 1999998 ). e345f4b3df20ddfbcd37ee4494fd0430 94bc362b8f70a9460f583ac71e08b9d0 9321a96c237894be2e9c80cb6a021f43 7973b84c289fb68dc3d480eb83aad0ee 7c71e94c46f7f62b679b83b2d67746c9 ee341ad92f3a9d4b359b3e7fa7a44cec DELETE SELECT DELETE SELECT SELECT DELETE RS_LOB_GARBAGE_<id>_ These queries are linked to the redesigned row store LOB garbage collection with SAP HANA 1.0 >= SPS 12. The number at the end is the volume ID assigned to the related SAP HANA service. Long runtimes on these tables are typically caused by a bug in SAP HANA 1.00.120 to 1.00.122.04. See SAP Note 2413261 for more information. da78d5abc901ba7166721c78d78697db 1aab02cdb52e14600b7fd7e1b666ad33 2b8bd3d2501cfe61cd64e2ec30a7f83a SELECT DELETE RS_LOB_GARBAGE_<id>_ These queries are related to row store LOB garbage collection. A very high number of executions with 0 processed records can happen on SAP HANA 1.00.122.16, 2.00.012.04, 2.00.024.00 - 2.00.024.01 and 2.00.030 due to the problem described in SAP Note 2633077 . 5a9fa03bb31bec877a0b0d00c761a252 SELECT RS_LOB_GARBAGE_<id>_ Queries of the following type can be expensive in case of blocked garbage collection when more and more row store LOB garbage piles up: SELECT DISTINCT TRANS_ID, TCB_INDEX FROM SYS.RS_LOB_GARBAGE_3_ See SAP Note 2169283 and make sure that garbage collection isn't blocked for a long time. 4522b390b39c745346e4b2c20ca0e730 ac1948f2a0ef6b22ad39e762da017d6c SELECT RSPCLOGCHAIN These selections from class CL_RSPC_LOG===================CP or function module RSPC_GET_DELAY can read a lot of records from RSPCLOGCHAIN. See SAP Note 2388483 and check if you can clean up old entries in the chain run log table RSPCLOGCHAIN. 4fe08f000cb95d6d03a8c279ef74c2d1 5cf54ecb10867fe6c9057d085049d7c4 SELECT RSPCLOGCHAIN, RSPCPROCESSLOG These selections originating from function module RSPC_GET_DELAY (application sources SAPLRSPC_BACKEND:1247, SAPLRSPC_BACKEND:12474) are executed frequently in context of batch jobs ODQ_TQ_JOB. See SAP Note 3080823 in order to minimize the load introduced by these batch jobs and perform housekeeping for RSPCLOGCHAIN (SAP Note 2388483 ). 3732125d72c6d25d5e0eeb8026c1b789 SELECT RSPCLOGTIMECACHE This selection originating from function module RSPC_GET_DEVIATION (application source SAPLRSPC_BACKEND:11364) are executed frequently in context of batch jobs ODQ_TQ_JOB. See SAP Note 3080823 in order to minimize the load introduced by these batch jobs and perform housekeeping for RSPCLOGCHAIN (SAP Note 2388483 ). various, e.g.: 26ce5e2bc2f400c047631aabe411a6f4 SELECT RSPMPROCESS Long runtimes of RSPMPROCESS calls from method IF_RSPM_QUERY_RETRIEVE~QUERY_PROCESS of class CL_RSPM_PERSISTENCY_STANDARD==CP can also be a consequence of insufficient housekeeping. You can use SQL: \\\"HANA_ABAP_MiniChecks\\\" (SAP Note 1969700 ) and have a look at check ID A1001 (\\\"BW data targets with many RSPMREQUEST entries\\\") to see if there is increased demand for housekeeping. You can implement a cleanup strategy based on the instructions provided in SAP Note 3137171 . various SELECT RSPMREQUEST SAP Note 2727934 improves the performance of activation of an RSPM process in method IF_RSPM_QUERY_RETRIEVE~QUERY_REQUEST of class CL_RSPM_PERSISTENCY_STANDARD by implementing the NO_USE_OLAP_PLAN hint. Long runtimes can also be a consequence of insufficient housekeeping. You can use SQL: \\\"HANA_ABAP_MiniChecks\\\" (SAP Note 1969700 ) and have a look at check ID A1001 (\\\"BW data targets with many RSPMREQUEST entries\\\") to see if there is increased demand for housekeeping. You can implement a cleanup strategy based on the instructions provided in SAP Note 3137171 . 8437943036a2a9fd82b290b2c6eafce7 SELECT RSR_CACHE_FFB RSR_CACHE_VARSH See SAP Note 2388483 and check if you can clean up old entries in RSR_CACHE tables. a12aff7f7db3b89f04df28429f4725b4 a7df7509a22f5e64c6f50e2234415f5e UPDATE RSR_CACHE_QUERY RSR_CACHE_VARSH Lock waits and deadlocks related to method __DELETE_VARSH_BY_CREATESTMP of class CL_RSR_CACHE_STORE are tackled by SAP Note 2905290 . 03d58c448c321ae8e78c00563bea0e95 UPDATE RSRNEWSIDS RSRNEWSIDS_740 Updates on tables RSRNEWSIDS or RSRNEWSIDS_740 can suffer from the hierarchy SID check overhead described in SAP Note 2183482 . This check is deactivated per default with current BW releases, but it may be activated on purpose via RSR_HIER_INTVL_CHECK_SID = 'X' in table RSADMIN. In this case you can check if you really need this feature and consider deactivating it by removing the setting from RSADMIN. 6422b644e862ef07e4636d32a7254632 UPDATE RSZCOMPDIR Increased runtimes are often caused by transactional lock contention (SAP Note 1999998 ). SAP Note 3217436 provides an alternative option to update the LASTUSED field of table RSZCOMPDIR, thus reducing the risk of lock contention. 1c340b120554c65c8352b06b8ec16650 286595fe6035f3aefbd0b9d0be0f814e SELECT RSZELTXREF If this selection from SAPLRZD1 with conditions on OBJVERS and INFOCUBE take a long time, you should consider the following: Make sure that the amount of data in the table is as small and efficient as possible (SAP Note 823804 ). Make sure that index RSZELTXREF~IC on column INFOCUBE is created on SAP HANA level. 054ca4ad7d229638928813856d0720b5 079e5109d4bf0ce8d47f8d84704c071c 6cf1986b54b8cf7df708083bc07af1a4 b5b6bd4bb2f842133c9a1f07ea1a48b4 SELECT FOR UPDATE S009 S014 These statements on LIS tables for CAS documents S009 and S014 originating from RMCSS009 and RMCSS014 can suffer from exclusive lock waits and deadlocks (SAP Note 1999998 ). SAP Note 743100 provides suggestions to reduce the risk of contention and deadlocks. UPDATE S061 S062 Depending on the application configuration transactional locks can increase the runtime and also deadlocks are possible on the info structure tables. SAP Note 1619751 describes possible steps to reduce lock contention and deadlocks. 833302af6e1fc2619d9a37be35ec4c16 UPDATE S066 The table S066 is used to track open orders in credit management contexts. In case of concurrent modifications of the same records lock contention is possible, resulting in increased runtimes. SAP Note 3031614 describes typical scenarios that can result in contention scenarios and possible optimizations. In S/4HANA and FSCM environments this table is no longer used. UPDATE S071 The table S071 is used to track special sales conditions. It can suffer from transactional lock contention in case multiple transactions try to modify the same record concurrently. SAP Note 2200612 provides suggestions how to analyze and minimize the transactional lock contention. 673800ad653f8d62d3769a06181c6f8e UPDATE SACF_ALERT Long runtimes of these update operations are usually caused by exclusive lock waits due to an insufficient application coding. SAP Notes 2248439 and 2801827 provide application corrections in order to reduce the lock wait times. 03adbc8704374d289ecce4bd8f1a3150 0555162ef70915eb8eb672670bd6c6b3 099952519dab17d505c8e3cb3d245196 11b1c4b452b55b7fea973c5c3e7547df 11ccb7ff8be1cd39d058c4616ebc1e46 17bd0e4181db35ada2fdb4c682f6eb0b 1db5c91fffcadec8f6551bdca82adf44 20b2fd8aeb4db6a26931cc61f6db4cd3 2b637fd280f55baf313331486ac35723 2efdc94d660cf4b20c235059ab369898 2fe987d90027ee74e82001499bae57bf 31140725b1f4a46369aab247c0b7e097 3371f7da0b57dbf46f4c056921376b2b 3554924da9a9ef871625c35f7127da58 40e6646599fe5c5a04f7441ef1e579d0 439daf39eb6987ab5ef620bd4756ef3b 481f650b89dbedae171995ea9a645d97 4e100847ee8a76ccfd8630d23dad66ba 52273b2666beb1a8ee46f069e7759b3b 540f5a0b90c3dde6badaf758b4f999ca 565f8588bcbf95fdd964510fd4e8ad27 57461b5a5d2adb64e80ebbeebe3a329d 5d110ba33c2e0314ff186019c37be2c7 5fbd592536c53068deb5b4a67272d7a0 683b80896e3550eea06ade4a3a855917 75698fbb304e406dca77e4b87fe3946c 7b54142eca356166c71929d03c462dc6 80aed209c3c3691b535d4d9a82c5a993 81788f9c7ecf4c6253e8099f7ca9eea0 8637ffebfcc65ab0d613d10c42a86f3f 8a1bcc8b73f1a2a21b194779ed603b80 8cd8929305df59e136926a31cf6ba797 8fa09e6dfa200434333e3728dfac986e 9dac2f34cd80ff538bbb4773f8aed514 a34890b38a9b16fb44327f36a618efc9 a464569c7abe55d26c35a07034a99a2f b473d796296125065b1857e239924b11 b52d2cc6afb4f07f8f9c2064e87abd93 b735b0cdfe3c8c595aa76166901f68c2 bafec15d2b8d4cdb478fb1f50d6d2b16 be96367bc1dbd364a00584477fe2261a c004e3e7d510a429896b839c9a0b9e0b c3a6938605f1d3c5a10f58b30be2014d d909d52390ed83bb90f01dd467a0a2f6 e3868db606001d6780f58b56c85b977a ec765107843434d8097b6c796e493a61 f12b30f24a33fde01082983778ef35dd f97424065815221c9b36d27cb671c329 fb0fef08b6e0f1d46cde07d8835545dd fc00dc4612028ce1ce686bd10ecc1761 CALL APS_ACT_GET_BY_ORDER APS_ACT_GET_BY_PEGAREA APS_ACT_SCHEDULE APS_ALERTS_GET_DATA APS_CUS_ORDER_CHANGE APS_CUS_ORDER_GET APS_CUS_SCHD_GET_BY_PEGID APS_DRP_AMOUNT_GET2 APS_DRP_IO_GET APS_FIX_PEGGING_CHANGE APS_GET_ATP_DATA APS_PEGID_GET_IO APS_PEGID_GET_ORDERS APS_OPT_GET_ACTIVITY_NET APS_ORDER_CHANGE APS_ORDER_CREATE APS_ORDER_GET APS_ORDER_MODIFY APS_PEG_CAT_GET_ORDERS APS_PEGID_GET_IO APS_PEGID_SELECT_ORDERS APS_REORG_ATP APS_RESOURCE_CHANGE APS_RESOURCE_GET_BRUTTO APS_RESOURCE_GET_DATA APS_SNP_CAPA_GET APS_SNP_ORDER_GET APS_STOCK_GET_BY_KEY APS_TIMESTREAM_CHANGE LCK_ENQUEUE02 SAPATP_READ_BUCKET_PARAMS_SIM SAPTS_CHANGE_TG SAPTS_GET_ALL_DATA2 SAPTS_GET_DATA SAPTS_SET_DATA SIM_SIMSESSION_CONTROL These database requests are calls of procedures in the integrated liveCache that is used in SCM environments. From a SAP HANA perspective specific tuning is hardly possible, but you can check for general optimizations like reducing the number of calls from application side or optimizing internal lock contention (SAP Note 1999998 ) if applicable. You can use transaction /SAPAPO/OM21 and SQL: \\\"HANA_liveCache_LCApps_Executions\\\" (SAP Note 1969700 ) to identify context and KPIs of liveCache function calls. various, e.g. 18edc88026d0439404488c7c0daf824b 759c32107c725e2629f1e9dc08a1c9fa SELECT \\ufeff/SAPAPO/MARM Accesses to CDS view /SAPAPO/MARM (respectively view SCMPRDMARM) include a CONVERT_UNIT call for unit conversion that introduces a rather fix overhead for building up temporary data structures (often indicated by ceCustomCppPop activities reported in the thread details) and selecting from ABAP tables T006 and T006D of typically a few milli-seconds. Thus, you should avoid executing many small requests (e.g. with only a single MATID) on /SAPAPO/MARM and instead select the data in bigger chunks, so that the fix CONVERT_UNIT overhead is required only fewer times. ba8fc270dc2b851ee2c41e069413bc4 SELECT /SAPAPO/MATGROUP This selection in method /SCWM/IF_AF_MATERIAL_BASE~READ_PRODUCT_GROUPING of class /SCWM/CL_AF_MATERIAL_BASE_S4 may repeatedly read a rather high number of records. It can be optimized with SAP Note 3041119 . cc351578e364bf59a48edd537191829c ee9876a198c6c67a81649487adc9c837 SELECT /SAPAPO/MATLOC See SAP Note 2904036 for optimizing FDA WRITE accesses (SAP Note 2399993 ) for compatibility view /SAPAPO/MATLOC. 0fb83778bdf2ed44eefbe48c92aebca4 e48c904c5fbc4e8dd8dd4ce4b4497dca SELECT /SAPAPO/ORDKEY Per default only the primary key on columns MANDT, ORDID and SIMID is delivered for table /SAPAPO/ORDKEY. Additional indexes need to be created based on individual needs. Transaction /SAPAPO/OM13 -> OrdKeyIndex can provide further insight about the necessity of additional /SAPAPO/ORDKEY indexes. Example: --------------------------------------------------------------------------------- |Optimal Index |Number of Selections |Max. duration [s] |Avg. duration [s]| --------------------------------------------------------------------------------- | LOCID ORDTYPE SIMID| 0 | 0,000 | 0,000 | | LOCID SIMID | 0 | 0,000 | 0,000 | | LOCID TTYPE SIMID | 0 | 0,000 | 0,000 | | ORDNO | 110.385 | 4,345 | 2,732 | | ORDNO SIMID | 1.829.009 | 0,510 | 0,004 | | ORDTYPE SIMID | 48 | 0,794 | 0,141 | | SIMID | 0 | 0,000 | 0,000 | | TROID SIMID | 0 | 0,000 | 0,000 | | TRPID | 88.240 | 0,079 | 0,002 | | TRPID SIMID | 2.713 | 0,139 | 0,007 | | TRPID TTYPE | 43 | 5,108 | 2,171 | | TRPID TTYPE SIMID | 0 | 0,000 | 0,000 | --------------------------------------------------------------------------------- Indexes of table /SAPAPO/ORDKEY -------------------- |Index | -------------------- | MANDT ORDID SIMID| | MANDT SIMID ORDNO| | TRPID | -------------------- Here we can see that a significant amount of selections happen via ORDNO and the average duration is at about 2.7 seconds. An index on MANDT, SIMID and ORDNO already exists but for requests only specifying ORDNO this index can't be used. For an optimal layout, also supporting pure ORDNO selections, you can adjust the MANDT / SIMID / ORDNO index to ORDNO / SIMID. b7944c0c3ee72f83c747db881a2eba05 SELECT /SAPAPO/ORD_LINK This query in report /SAPAPO/LSDORDER_DBF08 can suffer from activated FDA WRITE for FOR ALL ENTRIES (SAP Note 2399993 ). You can disable it globally or via statement hint as described in SAP Note 2486627 . 4a9b3e1d38e9c962d7a3cfc0aac6e2c2 SELECT /SAPAPO/POSMAPN This query with application source /SAPAPO/SAPLOO_TR_READ originating from report /SAPAPO/SDORDER_DEL is used to read the next package of records (default: 10000) based on column POSID. It uses a combination of ORDER BY and TOP that can't be handled efficiently by SAP HANA (see High runtime with ORDER BY and TOP ). In order to reduce the repeated read and sort overhead, you can increase the package size in report /SAPAPO/SDORDER_DEL (\\\"No. of Items in Del. Block\\\") significantly, e.g. by factor 10 to 100. 2328166fbaa978fd21fad94a9054f362 a54c45a29c5405fda0427f9ca7e3ff1f SELECT /SAPAPO/STOCKANC Selections from table /SAPAPO/STOCKANC originating from ABAP source /SAPAPO/SAPLDM_LC_SQL respectively function module /SAPAPO/OM_STOCKANC_SELECT may suffer from the fact that table /SAPAPO/STOCKANC is located in row store and so the combination with other aspects like FDA WRITE (SAP Note 2399993 ) may not work out fine. Moving the table to column store can sometimes improve performance: ALTER TABLE \\\"/SAPAPO/STOCKANC\\\" COLUMN Be aware that this adjustment may impact other database requests, so it should be monitored if there is an overall benefit. Transactions like /SAPAPO/OM11 and /SAPAPO/OM13 may show up with a red traffic light that can be ignored. 4de83928f69e078ebd8b68f65fe3ae30 SELECT /SAPAPO/V_TRQTA This selection originating from ABAP include /SAPAPO/LOO_TR_READF01 is an existence check that reads one record from view /SAPAPO/V_TRQTA with only a MANDT conditions specified in the WHERE clause. Due to SAP HANA design limitations, the limitation to one record may be applied at a late stage after the join has been processed, resulting in an unnecessary long runtime and resource consumption. See High runtime with join and TOP / LIMIT and upgrade to SAP HANA >= 2.0 SPS 07 so that the statement can be processed efficiently by the HEX engine. be788bf5f4482e70db907fbf7c73d857 CALL /SAPSLL/CL_SPL_HANA_SEARCH=>CHECK_NAME_COUNTRY This procedure call can suffer from TRexAPI_StopwordAndTermMappingUtil_tokenizerCacheLock contention (SAP Note 1999998 ) that is optimized with SAP HANA >= 2.00.059.07 and >= 2.00.066 (issue number 296062). A fulltext index (SAP Note 2800008 ) in column TERM_1 of the term mapping table /SAPSLL/TERMMAP can also have a positive impact on performance. ead5ced088741f49bd27a5f860766ecd SELECT /SAPTRX/EH_TASK This selection from the event management tasks table from method /SAPTRX/IF_EH_MODEL~LOAD_TASKS of class /SAPTRX/CL_EH_DB_ACCESS can be expensive if many event management tasks exist and a high number of records needs to be retrieved. See SAP Note 2895344 and minimize the task volume: Disable \\\"Log Task\\\" via transaction RSPO whenever possible. Archive event handlers as early as possible. Additionally check if it is possible to set the flag \\\"No loading of task\\\" in the event handler type customizing (SCP -> SPRO -> Event Management -> Event Handlers and Event Handler Data -> Event Handlers -> Define Event Handler Types). 9f3a4785ad9ff7ee4f1342618a37e8e5 SELECT /SCDL/DB_ADDMEAS This access originating from method READ_GEN_TABS_BY_ID of class /SCDL/CL_DL_DB_SERVICE can suffer from FDA WRITE overhead (SAP Note 2399993 ). Implement SAP Note 3041119 in order to deactivate FDA WRITE. 3631d2eb9948ce2ac6f1dac057d99bb0 SELECT /SCDL/DB_BPLOC This access originating from method READ_GEN_TABS_BY_ID of class /SCDL/CL_DL_DB_SERVICE can suffer from FDA WRITE overhead (SAP Note 2399993 ). Implement SAP Note 3041119 in order to deactivate FDA WRITE. 85cb05c86e4031d22ea077b66bfe02b9 SELECT /SCDL/DB_DATE This access originating from method READ_GEN_TABS_BY_ID of class /SCDL/CL_DL_DB_SERVICE can suffer from FDA WRITE overhead (SAP Note 2399993 ). Implement SAP Note 3041119 in order to deactivate FDA WRITE. c5d8831b456379da4d9f6432881d47e8 SELECT /SCDL/DB_DF This SELECT originating from ABAP class /SCDL/CL_DL_DF_DB_SRV can be optimized with an additional secondary index on column DOCIDTO of table /SCDL/DB_DF. 12a7794553189ead8027ee9a91c52118 SELECT /SCDL/DB_REFDOC This access originating from method READ_GEN_TABS_BY_ID of class /SCDL/CL_DL_DB_SERVICE can suffer from FDA WRITE overhead (SAP Note 2399993 ). Implement SAP Note 3041119 in order to deactivate FDA WRITE. 0a931335817155c25ddeae3183b7ad53 SELECT /SCDL/DB_STATUS This access originating from method READ_GEN_TABS_BY_ID of class /SCDL/CL_DL_DB_SERVICE can suffer from FDA WRITE overhead (SAP Note 2399993 ). Implement SAP Note 3041119 in order to deactivate FDA WRITE. 29058a5de1089af9471b1009f7d09855 SELECT SCHEMAS This SELECT is executed frequently in BW environments in context of report CL_RSD_DTA====================CP. The execution frequency is significantly reduced with correction provided in SAP Note 2726420 . d21cf1dc3fe2111bc1e53d58cacc29b9 effbb7b7b04458296ec46e9858d10451 SELECT SCHEMAS This selection is related to SLT (SAP Note 2014562 ). See Long runtime of query on SAP HANA dictionary objects and monitoring views and make sure that the CATALOG READ privilege is assigned to the database user executing the query. This is anyway a required configuration for SLT as described in SAP Landscape Transformation Replication Server -> \\\"Security Guide\\\" -> \\\"Initial user\\\". 31de26d87576ab66558ee13c54a3e3ed INSERT \\ufeff/SCMB/TBUSSYS This insertion originating from method GET_INSTANCE of class /SCMB/CL_BUSINESS_SYSTEM can suffer from \\\"unique constraint violation\\\" issues in case the usual 1:1 mapping between business key (column BSKEY, primary index 0) and logical system name (column LOGSYS, unique index LOG) can't be maintained, e.g. because another BSKEY should be inserted for an already existing logical system. As a consequence of the \\\"unique constraint issues\\\" also increased transactional lock times are possible (SAP Note 1999998 -> \\\"Why are there locks and deadlocks that can't be explained by the actual modification mechanisms?\\\" -> \\\"Unique constraint violations\\\"). Make sure that a consistent business key is maintained in SLD, table /SCMB/TBUSSYS and /SCMB/TOWNBS. See SAP Notes 3099337 and 3202213 for more information about adjusting the business key and troubleshoot /SCMB/TBUSSYS contention. As a temporary technical workaround you can move table /SCMB/TBUSSYS to row store because for row store tables record locks are released directly after the \\\"unique constraint violation\\\" termination and not at the end of the transaction: ALTER TABLE \\\"/SCMB/TBUSSYS\\\" ROW 3147c8d956a600ad16ca8ee074444078 INSERT /SCMB/TSLDPROD This INSERT can suffer from the same scenario like the INSERT into /SCMB/TBUSSYS. Check for details at /SCMB/TBUSSYS and consider to move table /SCMB/TSLDPROD to row store as a temporary workaround: ALTER TABLE \\\"/SCMB/TSLDPROD\\\" ROW 7f46508f349433ef9716a782bb5c3d7e 9f75f289f8b19fd36da01a861555ec85 a5fe878e070d043417c1a5174a792aab SELECT /SCMTMS/D_TORITE These SELECTs with NOT conditions on *KEY columns originate from class /SCMTMS/CL_DAC_TABLE_OPT. SAP Note 2654211 provides a fix by replacing the NOT conditions with a \\\">\\\" condition. 4580f3059c884ee10de1eeb1fb98ac06 4d7a6a4047decb99f4893347093ca434 6cc91e12c9ed812c4ad3b55a2cd10cfa 78b9774c4797097346cc18f858006c2b 8d7cfa00098e214b537eb4ed54131cb1 c411c649778b2163fb9b5c7a92926e89 c6923a33de2de74d7cd9753432edee12 ea75eeb3e7c6fa58daabd157360cc5bd f33871dd9732011b2597ac24f5ab8f4d SELECT FOR UPDATE /SCWM/LAGP This SELECT FOR UPDATE from application sources like /SCWM/SAPLHU_TO_UPD:57367 or /SCWM/SAPLHU_TO_UPD:59723 respectively include /SCWM/LHU_TO_UPDF57 can suffer from transactional lock contention (SAP Note 1999998 ) due to a high amount of capacity updates that may not be required. See SAP Note 3190865 and check if you can disable this specific update by flagging \\\"No capacity update\\\" in customizing. 2f0edf3918c670f03f0d0a7865be4b11 CALL SDA_EXECUTION_DEV This procedure is executed when a remote system triggers a smart data access (SDA) request on the local system in universal itab mode (SAP Note 2180119 -> \\\"Which data transfer modes exist for SDA?\\\"). The usage of universal itabs between SAP HANA databases is controlled by the DEV_NO_UNIVERSAL_ITAB hint (SAP Note 2142945 ) or by the following Boolean parameter: indexserver.ini -> [smart_data_access] -> enable_universal_itab_hana_sda Deactivating the universal itab can sometimes help to make the remote requests more transparent for analysis reasons rather than summarizing all via SDA_EXECUTION_DEV. See SAP Notes 2514255 and 2943297 for more information. Due to the fact that SDA_EXECUTION_DEV executes different queries from different remote systems, it is not necessarily an issue to see increased total runtimes. If you want to optimize it, you need to identify and optimize the most expensive requests coming from remote systems. CALL SDA_SELECT_AS_ITAB_DEV This procedure is used in context of the binary transfer mode (SAP Note 2180119 -> \\\"Which data transfer modes exist for SDA?\\\") in the target system of a smart data access (SDA) request. It is used when both the local and remote SAP HANA have exactly the same SAP HANA Revision level. If the execution shows increased runtime or resource utilization, you have to analyze and optimize the database request that is provided as first argument of this procedure. 5f13497ba22f20ebd629da493e395f8d c4fd3b8fd975a4cda303590e7b2a707a SELECT FOR UPDATE SEC_CONTEXT_BLKD To prevent individual users from consuming all HTTP security sessions, a \\\"Session Limit\\\" per user is defined via transaction SICF_SESSIONS (default: 100). When the limit is hit, additional requests will be rejected and documented in table SEC_CONTEXT_BLKD. See SAP Note 2760552 for more information. In case of a malicious client with a very high amount of concurrent session requests, the updates of counter and timestamp in table SEC_CONTEXT_BLKD can result in transactional lock wait serialization. In this case an increase of the \\\"Session Limit\\\" in transaction SICF_SESSIONS can be a quick workaround to reduce issues caused by this serialization (e.g. a general lack of available work processes). For a good permanent solution it is important to adjust the behavior of the client flooding the system with security session requests. See SAP Notes 2760552 , 2754328 and 3201227 for more details related to analysis steps, configuration options and optimization approaches. Starting with AS ABAP SAP_BASIS 740 there is a resilience improvement available with SAP Note 3293436 (combined ABAP and kernel patch) where the majority of processing is done via shared memory and so the SEC_CONTEXT_BLKD accesses will significantly reduce. 281f191a6b9fd65216df0d9945735113 dc2bcf2b5aa0f22aec2ea1fc62a49418 SELECT SEC_CONTEXT_COPY This selection is used by the SecuritySessionInactivityWatchdog triggered by the ABAP task handler that checks for outdated HTTP security (https) sessions. In busy systems with many HTTP security sessions there can be millions of selections per hour. You can check for existing HTTP security sessions via transaction SM05 and e.g. identify users with a particularly high or unnecessary amount of HTTP security sessions that can be reduced with appropriate adjustments. See SAP Note 2760552 for more information. Starting with AS ABAP SAP_BASIS 740 there is a resilience improvement available with SAP Note 3293436 (kernel patch) where the high amount of SELECTs is transformed into fewer, more efficient sub-select joins. 0ec0b76746585dbcdc308e168dd9a353 266fb9202f35945f4ee016214e2a67a1 SELECT SECURITY_CONTEXT This table contains security session information that is required for state-less protocols like HTTP. Selections on this table are typically very quick, but due to the fact that it is the very first database request to be executed in transactions, it can suffer from admission control queueing (SAP Note 2222250 ). In this case the execution times on SAP HANA side are fine, but from an ABAP perspective long-running accesses to SECURITY_CONTEXT are visible. You can check via SQL: \\\"HANA_Workload_AdmissionControlEvents\\\" (SAP Note 1969700 ) to what extent the mentioned statement hashes suffer from admission control queueing. If admission control is responsible for queueing overhead, you can either adjust admission control settings (e.g. disabling it) or make sure that the triggering event (e.g. high CPU consumption) happens less frequently by optimizing responsible database requests. SELECT FOR UPDATE SFC_ROUTER Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 113dc3a08e050b79cf423deadc46c16b 145850f44fb63705f273858dcc3257df 3d2724778ca7cd61bcd36fa4924a59c0 46c5fe2884f11257abb74793abf18c9f 486ac42ad0dd8db9e857e6116e835d93 6eccd14a84dec8a1e868706655ca1020 946e87a9511070edf1bbd6177b4b0150 b1e4e649916da9724d0ff114258ac53d d138cc6eaaf2a9050cd51844dfb187f5 e137a8162443fec5dd94150565acf3c6 efaeaf6c8787b2f8e75f496034323482 CALL SHARED_BUILD_AA_VIEWS SHARED_BUILD_SR_VIEWS SHARED_CREATE_UNION_VIEW These procedures in the _SYS_STATITICS schema are used by the statistics server (SAP Note 2147247 ) to build system replication views during initialization. The performance can suffer from network problems or unavailabilities related to the secondary system. In the worst case accesses to remote objects get stuck until the connection timeout (default: 180 s) is reached. See SAP Note 1999880 and make sure that system replication is established properly. Check according to SAP Note 2222200 if the network configuration of the primary and secondary SAP HANA system replication site works properly. As a temporary workaround it is possible to reduce the connection timeout (default: 180000, unit: ms, i.e. 180 seconds): <service>.ini -> [communication] -> default_connect_timeout 753769700d4435edcbc8cd8ecaa2a6fc 806b77333ee56cb908c119e2cc9ade7c CALL SHARED_FOLLOW_UP_ACTIONS This call is executed at the end of every statistics server alert check (independent of the fact if actually an alert was triggered) in order to schedule follow-up actions like creating a runtime dump or activating traces. It is normal to see a certain load coming from this SQL statement. Runtimes of up to 20 ms are normal. The number of executions can theoretically be influenced by adjusting alert check intervals as described in SAP Note 2147247 (\\\"How can the statistics server check intervals be adjusted?\\\"), but normally not required. abf87f501874ae38a04bce633f869019 bedaa3a75085247a6e1f0370c380c3c6 CALL /SHCM/CL_WFD_OP_IM_PERSWRKSTAT=> IF_WFD_IM_PERSONWORKSTATUS~ GET_PERSONWORKSTATUS Statement hash abf87f501874ae38a04bce633f869019 is a non-unfolded part of the procedure used for CDS view I_PersonWorkAgrmtStatus for variable ET_PERSONWORKSTATUS. It can impact various root statement hashes implicitly or explicitly using this CDS view like bedaa3a75085247a6e1f0370c380c3c6. See SAP Note 3277599 and implement the INLINE hint to take advantage of unfolding. dc6ac3aa4b5dcf34b926da4f7adc9088 and others DELETE SMMW_DEVMBOMAPER SMMW_LOG_HDR SMMW_MON_NOTIFY SAP Note 2423891 provides an application fix to reduce the risk of deadlocks on these SMMW tables. 9546a29b93480cab55902d36b3aaefc2 c3f9f2134143eb8da4f860c193024caa SELECT SOOD The SOOD selection in method REORG of class CL_REORG_BCS is executed as part of the SAPoffice reorganization (SAP Note 966854 ). It sorts all matching records and then returns the first records up to the package size defined in report RSBCS_REORG. The sort overhead is fix, independent of the actual package size. So larger package sizes usually improve the throughput per record. If the configured package size is below 10000, consider an increase, e.g. to 50000. 7488ecfdc727226c7bad7e99ebe21b25 e5332b10f3a1a4215728857efc0f8eda SELECT SOURCE_ALERT_65_BACKUP_CATALOG This SELECT is linked to the statistics server action Alert_Backup_Long_Log_Backup. See SAP Note 2147247 (\\\"How can the runtime and CPU requirements of the statistics server actions be analyzed and optimized?\\\" -> \\\"Alert_Backup_Long_Log_Backup\\\") for details about optimizations. 9a79d31c5bcd65e129cddf4e67057c49 2510974b53d11210cfe465e5223a572d SELECT SELECT SRRELROLES BBP_PDBINREL, SRRELROLES This queries originate from report SAPLRREL / include LRRELF01 and from function module BBP_PDH_OR_DB_SELECT_BBP_PDBIN. Activate index SRRELROLES~001 on column ROLEID in order to support the query optimally. Usually an index on ROLEID should be created automatically if the tools available via SAP Note 1794297 are used when migrating to SAP HANA. a7fc8bc85b7b95dddb6d73a12dd00c0c UPDATE SSCOOKIE Lock contention and deadlocks in context of class CL_BSP_SERVER_SIDE_COOKIE can be caused by the application design. An optimization is available via SAP Note 2564753 . Users with activated CRM_CENTRAL_SEARCH = REBUILD_MENU parameter can significantly increase the lock contention and so you should avoid this setting whenever possible. You can adjust it via transaction SU01-> \\\"Parameter\\\". Lock contention and deadlocks in context of job BSP_CLEAN_UP_SERVER_COOKIES can be caused by an ABAP coding issue that is resolved with SAP Note 2944650 . Also the issues described in SAP Notes 3018745 and 3119644 can contribute to the issue. aa01412bfb912d422e4cc07e401ca70f ee83408659f7c61a023c27ab5d784994 DELETE /SSF/BTAB Deletions on the service software framework table /SSF/BTAB can suffer from exclusive lock waits if multiple concurrent monitoring related tasks are running at the same time, e.g.: DVM Cockpit tasks (e.g. collective analysis) TAANA operations Solution Manager tasks (e.g. EFWK resource manager) So you should check in the first place if long running monitoring activities can be improved or scheduled at different times. In general long runtimes only impact monitoring activties, important business transactions shouldn't suffer. 5ef81b3843efbe961b19fdd79c2bd86b a3bbe3e5447bc42eb3f0ece820585294 8343a81d8e7025d91e72be8d530bc095 CALL UPDATE STATISTICS_PREPARE_CALL_TIMER STATISTICS_SCHEDULE The UPDATE on STATISTICS_SCHEDULE is executed from within the procedure STATISTICS_PREPARE_CALL_TIMER. High runtimes in combination with record locks (ConditionalVariable Wait) on table STATISTICS_SCHEDULE can be caused by: Missing COMMIT after STATISTICS_PREPARE_CALL_TIMER: This problem is resolved as of SAP HANA 1.00.101. Missing COMMIT after STATISTICS_PREPARE_CALL_MANUAL: This procedure is linked to Solution Manager requests to the statistics server. Starting with SAP HANA 1.00.122.04 you can eliminate this problem by switching to the new SAP HANA monitoring approach for Solution Manager (SAP Note 2374272 ). These calls are executed at the beginning of every statistics server check. It is normal to see a certain load coming from this SQL statement. Runtimes of up to 30 ms are normal. The number of executions can theoretically be influenced by adjusting alert check intervals as described in SAP Note 2147247 (\\\"How can the statistics server check intervals be adjusted?\\\"), but normally not required. ac08e80aa8512f9669eaf56dedc02202 c6c59150977220ea4fdea412cac7d1ce 0800928edd9d43764935a4e02abbfa15 16c226e204b54efc280091df267152fd 2b7fee2d2b95e792d3890051cbb97ec9 55bb62c4b9ff3f0b32de3a4df248e18c SELECT SELECT STATISTICS_CURRENT_ALERTS STATISTICS_LAST_CHECKS See SAP Notes 2147247 (\\\"How can the runtime and CPU requirements of the statistics server actions be analyzed and optimized?\\\" -> STATISTICS_ALERTS_BASE) and make sure that the size of the underlying table STATISTICS_ALERTS_BASE remains on a reasonable level. d6fd6678833f9a2e25e7b53239c50e9a CALL STATISTICS_SCHEDULABLEWRAPPER (timer) This procedure is a wrapper for all embedded statistics server actions like history collections and alert checks. This statement hash typically indicates that it is called by the statistics server based on the configured check time intervals. Average execution times of up to 400 ms can usually be considered as normal and no analysis or optimization is required. See SAP Note 2147247 (\\\"How can the runtime and CPU requirements of the statistics server actions be analyzed and optimized?\\\") for more information how to analyze and optimize the actions. 0f54bb2482e75ab61657973d9195bbb4 10cec71e46696dbe20a2143a3158dfd4 1716655062c017719efc84121e3ea753 1bf11f8fd6c108e2194bc162032b10a9 383b3cb02f8f692ed9a28c9249a54c66 3bf0fcaebd43f16dc504b9beb0005d29 45e3ba47a0368068d71abb143f26d1d0 673d5512da8404303acbe65a3baaf01a 6801336f2e5de891edc0b806c6048c03 74ab46ff2c01649253edf88abed9a870 869b2445401fa3cb696e631dc39a5f0a 9d334c558cffb2f66af0648598ceebf8 a2210deae1b210ac98a194a57c5ce4bf a562281167397c8e00fe95677fa332a2 aa220713a00ada128ae424d62b5a5868 aa4ab1034dff6b5c016c3db0be67d8cb ba1ee014dd96eb9a21fdae5abbb1f94c d617c529ab701a44b2c40cbafa9a314b d8091e1a6717e82567d58a6c9434a9a1 d9232f67ad8896b521b6a22234fee91f dc571bf5eb7cad9c313c20de904ab709 e02eb8dc390a1db7e1e8779ace4f3ce2 e54cd3f2d069e395bf5bc9414d6369a3 e60fb426874c22c32b92f3b6d1e52372 ec25d42a14422d8e899a7932ab213410 f019d40ff3e156a7fbcd78c5f364b286 f609c887b08bb14c9a218a9756ed64df f978e725069449cf440f5c6b9185e4d5 fa2bdf3047b4de33242305aef6b593e9 fbf6ff256ba159a53893d7bf4bc6d277 CALL STATISTICS_SCHEDULABLEWRAPPER (manual) This procedure is a wrapper for all embedded statistics server actions like history collections and alert checks. This class of statement hashes represents manual calls with the first argument being 'Manual', e.g.: CALL _SYS_STATISTICS.STATISTICS_SCHEDULABLEWRAPPER ('Manual', ?, 40, 0, NULL) These kinds of statements typically originate from SAP Solution Manager or FRUN in order to extract history and alert information from SAP HANA. See SAP Note 2374272 that describes how to enable the new SAP HANA monitoring mechanism in these contexts. Then the Solution Manager and FRUN will rely on information already collected by the statistics server rather than executing statistics server checks on its own. As a consequence this particular STATISTICS_SCHEDULABLEWRAPPER call will no longer be executed. In cases where it is not possible to disable to manual calls, you can check SAP Note 2147247 (\\\"How can the runtime and CPU requirements of the statistics server actions be analyzed and optimized?\\\") for more information how to analyze and optimize the most expensive actions. 05795bf8c58150579e539ad4c762a411 6d112675aaec6e4e853052145f0c0d23 DELETE STATISTICS_USED_VALUES STATISTICS_USED_VALUES is used by the SAP HANA statistics server (SAP Note 2147247 ) to store check results for later evaluation by SAP Solution Manager (SAP Note 2529478 , internal.check.store_results). When the statistics server executes an alert check, it deletes previous information for the same ALERT_ID with this statement so that it can later on insert new alert results. Due to the high modification frequency the performance of this request is particularly sensitive for garbage collection issues, so in the first place you should check if garbage collection is blocked for an unusual long time (SAP Note 2169283 ). Starting with SAP HANA 2.00.037.03 and 2.00.043 an index on column ALERT_ID is delivered per default which will significantly reduce the amount of scanned records and improve performance. SELECT STATUS_DESCRIPTION STATUS_MEANING Check SAP Note 2535647 that provides optimizations for this database request from Manufacturing Execution application side. 1626b0156893dab5438186b6160139b7 SELECT ST_GEOMETRY_COLUMNS This query originates from the SAP HANA license measurement (thread method lic/license_measurement). It can be particularly expensive if the underlying table CS_VIEW_ATTRIBUTES_ is large because many millions of custom view attributes are defined. 6edaa7fd2e3585b77132c650493b58c3 SELECT STOREDTASK This access is not expensive, but in context of transactional LOBs it can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Note 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. 00497cb80638494864725be90bbf2eb6 12b4ef1b19369b76831360dc02d074d2 2723663c7576a23c860ba55ab91c513a 36723585726e7f185d6a855af6cff433 3dd71780f165ebfa059ba8b96890aa77 4dff04027f025c711d7009bd3ce90be2 87f18bf0f023ccfc0993cba21c1e26ce 98153cbbd47009f99b0a24e0b1121d9f 9d5690b9382a98abbf80ab837540c75d a97c6b2c11324d68599a578211625c19 ab45c2515763669bd81f1bf0b3e842d6 b3f8f9ee4e70c32358aa9fdfda447ef8 b5535cbcb7f4ae77f92875b556536c7c bbcf8c3d170a3473855932dd2de67f19 c1f08adead4051c58289f552b4a6d71a dae85597f14a53bed3af1368e06153f2 deefdc73509bee2c6e78103e6ba5a44c e8e01d06eac7f4a9e077c82b1841c00e f1b7014a4be2d66f7ce29de385f2313c feb6b7a1c1f054ab4e42a50c2a24feb3 SELECT STXH Check (e.g. via SQL: \\\"HANA_SQL_StatementHash_BindValues\\\" available in SAP Note 1969700 ) if the TDNAME values sometimes end with '*', e.g. '0030646364*'. If yes, you suffer from a problem introduced with SAP Note 2208025 where accidentally the '*' constant rather than a wild card was added at the end of the value. This bug is fixed with the correction provided in SAP Note 2302627 . In other cases the performance can suffer from a wrong query optimizer decision. Please implement SAP Note 2700051 in order to provide good statement hints for the most important STXH queries. 6805026a381879e9e5469be3f09cc654 1f19127a9f6efcb0aa7b7da8b6e0f31d SELECT SELECT STXH STXL Analogue to SAP Note 2107400 for Sybase ASE the following kind of SELECTs can cause trouble with SAP HANA while performing a TDMS import: SELECT DISTINCT 'HUGO' FROM SVERS WHERE EXISTS ( SELECT * FROM \\\"<table_name>\\\" ) Reason is that SAP HANA sometimes evalutes the sub-query completely rather than finishing after the first retrieved record (that already satisfies the EXISTS semi-join condition). A simplified record existence check will be provided in R3trans that is processed much more efficiently: SELECT TOP 1 'HUGO' FROM \\\"<table_name>\\\" On SAP HANA side the processing of these kinds of EXISTS subqueries is optimized starting with 1.00.122.13, 2.00.012.02, 2.00.021 and 2.00.030. 0fe0c8111a52cf95def4b55893464ca1 717514f88a91cadb97ef879f6608e29c 9063ecdcca002cb5368be439b5ee8f39 c3389e9753cb46da85cd640da8ed3caa SELECT STXL STXL selections related to application sources like SAPLSTXD:11322 may use an unfortunate primary key access. Implement the statement hints provided via SAP Note 2700051 in order to disable primary key accesses. b11500ff88187cd8f06a308fc8e568f0 SELECT SUAUTHVALTRC This selection is linked to the ABAP user trace for authorization checks (transaction STUSERTRACE, ABAP profile parameter auth/auth_user_trace). Make sure that this trace is only activated when really required. See SAP Notes 2388483 and 2421733 and make sure that no longer required data is purged in time. 769d7b44a80f1756ed4da215f9744a72 efbf8ae66edaab4e26ba700d7f7ffac6 a609f96b4ec877d38c9881fb9b8810b0 INSERT UPDATE SUSAGE The table SUSAGE tracks the usage of different SAP components. In case of significant tracking activities there can be lock contention (SAP Note 1999998 ) and deadlock. SAP Note 2182269 describes how the SUSAGE accesses can be optimized. 0d9248912d550b8fb9f50699904a2309 UPDATE SWNCMONI This update from class CL_SWNC_COLLECTOR_DB is related to data collection for the ABAP workload transaction ST03. Among others it adjusts the LOB column CLUSTD. In context of packed LOBs (SAP Note 2220627 ) this can be quite expensive and it may acquire the VarSizeEntryFreeSpaceHandler lock. You can bypass this issue by switching the data storage to transparent tables (SAP Note 2274315 ) or by implementing workarounds on SAP HANA side as described in SAP Note 1999998 -> VarSizeEntryFreeSpaceHandler. e7ad51351b6725e665885fd80adedac1 SELECT SWWWIHEAD The selection from table SWWWIHEAD with a specific WI_ID restriction in function module SWW_WI_HEADER_READ (application source SAPLSWW_SRV:187) can e.g. show up in context of the regularly scheduled batch jobs SWWERRE when many work items in ERROR state exist. You can check for work items in ERROR state via ABAP transaction SWPR or using SQL: \\\"HANA_ABAP_Workflow_WorkItems\\\" (WI_STAT = 'ERROR') available via SAP Note 1969700 . Minimize the number of work items in error state, e.g. by resolving the root cause and restarting the workflow in transaction SWPR or using the cleanup report RSWWWIDE (SAP Note 49545 ). SAP Note 3194886 provides a correction that reduces the amount of SWWWIHEAD selections when many work items are in ERROR state. b4a0dfe6daab873b41ce5e0a5f5b8716 SELECT SWW_WI2OBJ This selection with application sources CL_SWF_UTL_UPDATE_00013=======CP:84 originating from line 16 of method IF_SWF_UTL_UPDATE~EXECUTE of class CL_SWF_UTL_UPDATE_00013 can suffer from a small default package size in combination with high sorting efforts. The default package size is MV_PACKAGE_SIZE = 3000. Configuring larger package sizes (e.g. 30000) can reduce the number of executions and the sort overhead, but would be a modification of the SAP standard. Usually the expensive processing is only required once per system, thus, increased runtimes and resource consumption is typically acceptable. 0a60a633e2e1f038aacfadd17a772003 SELECT SWW_WIREGISTER This selection can be expensive due to a high number of retrieved records for APPLICATION = 'WIM'. See SAP Note 2388483 and make sure that the batch job RSWWWIM is scheduled on a regular basis and runs successfully. bba5b88bebf8051e8ac3dcc70d7ef589 SELECT SYNONYMS, TABLES, VIEWS This SELECT is executed when an ODBC or SQLDBC application requests table related meta data from the client (e.g. in context of SAP Data Services / BODS). SELECT \\ufeffT006 T006D Selections on tables T006 and T006D like select <column_list> from \\\"<sid>\\\".\\\"<schema>\\\".\\\"T006D\\\" where <conditions> [ with parameters ( 'HINT' = ( 'LATE_MAT', 'OFF' ) ) ] originate from ERP unit conversion being implemented with the CONVERT_UNIT function call of SAP HANA. This call is used in different application scenarios, e.g. as part of accessed to CDS view /SAPAPO/MARM . Contrary to currency conversion no caching is possible for unit conversion on SAP HANA side. In case there is a significant overhead, it is recommended to user fewer larger conversion requests rather than a high number of accesses with only a few records each. With SAP HANA <= 2.0 SPS 05 a LATE_MAT hint is generated that can negatively impact the runtimes in some scenarios because the usage of the HEX engine (SAP Note 2570371 ) is disabled (issue number 261730). With SAP HANA >= 2.0 SPS 06 the hint is no longer generated. 513fc810543cf3fc331cb3015257610b SELECT TABLE_COLUMNS This query, executed by CL_SQL_STATEMENT==============CP on ABAP side with filters on schema and table, is related to SLT (SAP Note 2014562 ) structure comparison. A SLT target system checks the structure of the table in the SLT source system when a SLT job is executed in order to detect the necessity to adjust the replication process. The following optimizations exist on SAP side: Check if the number of related jobs (/1LT/IUC_LOAD_MT_*, report DMC_MT_STARTER_BATCH) in the SLT system can be reduced as more jobs can result in more structure comparisons over time. Switch real-time replication so scheduled replication, e.g. once a minute. This will result in some delays until a source change is replicated to the target system, but it will reduce the number of required structure comparisons. c7aa02f6ae8919061f5708ad63e56d33 SELECT TABLE_COLUMNS INDEX_COLUMNS If an access to TABLE_COLUMNS and INDEX_COLUMNS from ABAP module DB_GET_TABLE_FIELDS takes long, SAP Note 2402693 can be implemented to improve the performance. 027a69b3097e6c6c93176c6fec540d53 c335135bcf1af4323675498fea42d5f9 SELECT TABLE_COLUMNS_ODBC Complex queries on TABLE_COLUMNS_ODBC and several other SAP HANA dictionary objects are used when an ODBC client (SAP Note 2393013 ) calls the GetColumns function. It takes catalog, schema, table and column as arguments and returns SAP HANA column information. In case of increased runtimes and CPU consumption consider the following optimization approaches: See Long runtime of query on SAP HANA dictionary objects and monitoring views and make sure that the CATALOG READ privilege is assigned to the database user executing the query. Get in touch with the related ODBC client partner being responsible for the calls and check if there is a way to reduce the GetColumns calls, e.g. by caching information on client side. 25a6171ba41bdf171e818c986177f37e SELECT TABLE_GROUPS This selection from SAP HANA view TABLE_GROUPS is executed in certain scenarios by SAP HANA in order to determine table distribution (SAP Note 2081591 ) details for a specific table. Frequent executions can happen in systems with many tables using dynamic range partitioning (SAP Note 2044468 ) because for every table a TABLE_GROUPS selection may be executed in each dynamic range check time interval. It can be controlled with the following SAP HANA parameter (default: 900 seconds): indexserver.ini -> [partitioning] -> dynamic_range_check_time_interval_sec Besides on a high number of executions also the execution time of each TABLE_GROUPS selection can be critical. In particular in context of a high number of temporary tables in BW environments (GROUP_TYPE = sap.bw.temp) long runtimes were observed. See SAP Note 2800007 and make sure that the number of NO LOGGING tables created in context of BW remain at a reasonable level, e.g. by scheduling SAP_DROP_TMPTABLES on a regular basis. See also SAP Note 2388483 that among others discusses options to keep the size of table TABLE_GROUPS_ at a reasonable level. With SAP HANA >= 2.00.056 the dynamic range partition check is cut into smaller pieces so that it doesn't block the garbage collection for a long time (issue number 262672). 97171744726a50e248699f758de23444 ba8161b6133633e2984828ce38aa669a 7f55e72c22694541b531e83d8bab8557 d24d8b8d00dd8b8df3e13dd9fb17b5f9 2b459e0d42037fe4d6880b238018a6f7 70efe3b4e470438543e6ecec418e4f02 905dbaa93a672b087c6f226bc283431d CALL CALL SELECT SELECT ALERT_DEC_EXTRACTOR_STATUS COLLECTOR_GLOBAL_DEC_EXTRACTOR_STATUS GLOBAL_DEC_EXTRACTOR_STATUS_BASE TABLES This CALL and SELECT is executed by the statistics server under the _SYS_STATISTICS user. It is linked to DXC (see M_EXTRACTORS) and looks for tables with names like '/BIC/A%AO'. Tables following this naming convention are technical tables to control the DXC DSO activation process. If the CATALOG READ privilege is not granted to _SYS_STATISTICS, the query can take quite long and return limited results. Proceed according to Long runtime of query on SAP HANA dictionary objects and monitoring views . 0132d9eeaf22c1d38d4c5a8f4d6a36af a058f013fb0ec2f4c1141e5b396588e8 SELECT TABLES This selection is related to SLT (SAP Note 2014562 ). See Long runtime of query on SAP HANA dictionary objects and monitoring views and make sure that the CATALOG READ privilege is assigned to the database user executing the query. This is anyway a required configuration for SLT as described in SAP Landscape Transformation Replication Server -> \\\"Security Guide\\\" -> \\\"Initial user\\\". 2d57b02cadda185883f5b21f278fc9ec 4bcb3c3b073c1c0b3ec8c6189452c3ca 8009724b392f39ef358c8fcd399b9f72 SELECT TBTCO A TBTCO selection with STATUS = 'F' or STATUS = 'A' from SAPMSSY2 (start_stuck_eoj_jobs) is related to jobs that are defined with the SAP_END_OF_JOB event, so they wait until another batch job is finished. In case a high amount of these jobs exists, there can be a high number of (typically quick) TBTCO selections. You can use SQL: \\\"HANA_ABAP_BatchJobs\\\" (EVENTID = 'SAP_END_OF_JOB' ) available via SAP Note 1969700 to check for these jobs. Check ID A0860 (\\\"Old batch jobs waiting for END OF JOB event\\\") of SQL: \\\"HANA_ABAP_MiniChecks\\\" (SAP Note 1969700 ) reports an issue in case a high number of jobs exist waiting for event SAP_END_OF_JOB. The related jobs often have names following the convention USR_ATCR_IMP<timestamp>. SAP Note 2076491 describes the background of the creation of these jobs. SAP Note 2640389 describes an ABAP job scheduling problem that can result in more and more jobs in SAP_END_OF_JOB wait state. SAP Note 2998530 provides an optimization to reduce the number of requests from a SAP ABAP batch perspective. Check why many jobs exist since a long time waiting for event SAP_END_OF_JOB and delete jobs via report RSBTCDEL2 in case they are no longer required relics from the past. 05a6382b4675387c44e3008c8a6fb32a 72175743393a7015e247647acc1bb7b5 b80f8bd3c59c1bab3c692c55e4d5b3f9 f9fe5a1f2bbfe7be197f89f4813dee95 SELECT TBTCO TBTCO selection from CL_RSPC_CHAIN=================CP usually specify selective STATUS values like 'S' (released) or 'Z' (put active) and so index TBTCO~3 is most efficient. Index TBTCO~5 on EVENTID and EVENTPARM also appears to be selective, but there can be scenarios with a high amount of records for specific EVENTID / EVENTPARM combinations. Implement SAP Note 2700051 that delivers an appropriate hint for index TBTCO~3 for this request. 4f765870d762c351b57c4217e6093f9f SELECT TBTCO This selection from TBTCO with a LIKE condition on column JOBCOUNT originates from the consistency check report RSTS0024 (SAP Note 666290 ) that compares table TBTCO in TST01 in order to identify orphan job logs. Consider the following recommendations: Make sure that the number of records in table TBTCO remains on a reasonable level (typically not more than 1 million) by keeping the number of scheduled jobs as small as possible and by implementing a proper house-keeping strategy (SAP Note 2388483 ). Avoid running report RSTS0024 too frequently (more than once a month). If required, an additional index on column JOBCOUNT can speed up the selections. ad5ab291e053d2c9c6c18af69c0b6493 SELECT TBTCO This TBTCO selection from SAPLBTCH with the following conditions in the WHERE clause can be unnecessarily expensive in cases when the JOBCOUNT condition is more selective than the JOBNAME condition: \\\"JOBNAME\\\" LIKE ? ESCAPE ? AND \\\"JOBCOUNT\\\" = ? In this case an additional TBTCO index on column JOBCOUNT can help to improve the performance. 4871402a099e866795707c31bc5aaea1 SELECT TBTCO TBTCO selections with EVENTID and an selective STATUS condition (e.g. from SAPLBTCH) suffer from the absence of an index on both columns. You can create an additional TBTCO index on columns EVENTID and STATUS in order to optimize the access. fb8b46b90b80619b1b97af549e3a5772 SELECT TBTCO, TBTCP Selections from table TBTCO with a NOT EXISTS condition on table TBTCP executed in function module BP_JOB_SELECT_SM37C (application source SAPLBTCH) are related to an implicit consistency check between tables TBTCO and TBTCP. It is executed when transaction SM37C is called explicitly or implicitly and a step condition is provided. This scenario can regularly happen in context of the job monitoring functionality of the FRUN Simple Diagnostics Agent. In this case the ENDTIME / ENDDATE conditions are typically most selective and creating an additional TBTCO index on columns ENDTIME and ENDDATE can improve the performance. In a real life scenario the runtime improved from 3500 ms to 10 ms after the index was created. SAP Note 3108256 provides a coding correction so that this consistency check is only executed when transaction SM37C is called explicitly, but no longer when it is called implicitly, e.g. via monitoring tools. bfa3071482fd3d80f268bb48cd312095 UPDATE TBTC_TASK Updates on TBTC_TASK from class CL_SDL_TASK and method SUBMIT_NEXT_JOB (source CL_SDL_TASK===================CP) may touch an unnecessary high amount of records and so lock waits and deadlocks become more likely. SAP Note 2843720 provides a correction. SELECT TCURF Implicit accesses to table TCURF are related to currency conversion. Consider setting up the currency conversion cache (SAP Note 2502256 ) to make sure that conversion rates are cached and so repeated accesses to table TCURF are no longer required. The statement hash differs between systems because database name and user are explicitly used in the statement text. Example: (typical implicit TCURF access) select \\\"FCURR\\\",\\\"TCURR\\\",\\\"KURST\\\",\\\"GDATU\\\",\\\"FFACT\\\",\\\"TFACT\\\",\\\"ABWCT\\\" from \\\"C11\\\".\\\"SAPC11\\\".\\\"TCURF\\\" where \\\"MANDT\\\" = ? order by \\\"FCURR\\\" asc,\\\"TCURR\\\" asc,\\\"KURST\\\" asc,\\\"GDATU\\\" asc,\\\"ABWCT\\\" asc with parameters ( 'HINT' = ( 'LATE_MAT', 'OFF' ) ) 371d616387247f2101fde31ed537dec0 aaff34b83d10188d08109cb52a0069ae 3f96e3762412e8d4cf4c670f2993aa8a 51bffaafaddd0ec825ecb9f5bf1b5615 5d0b4a21b0c17c08a536bec04f2825cd a3f19325031503e9e4088a8e57515cd3 1cbbe08ef2bf075daecb9e383ae74deb a5539c73611c1d0ba9e4a5df719329b8 a382ffaeafac914c422752ab9c2f9eab 81d83c08828b792684d4fd4d2580038e 1ce978ccb4cfed10e1ef12dc554c2273 54c8e15427f41b9074b4f324fdb07ee9 da7ef0fee69db516f1e048217bca39e7 c197df197e4530b3eb0dcc1367e5ba4b DELETE DELETE DELETE DELETE DELETE DELETE DELETE INSERT INSERT INSERT TESTDATRNRPART0 TESTDATRNRPART1 TESTDATRNRPART2 TESTDATRNRPART3 TESTDATRNRPART4 TESTDATRNRPART5 TESTDATRNRPART6 TESTDATRNRPART0 TESTDATRNRPART1 TESTDATRNRPART2 SAP Note 1964024 provides a correction that reduces the DML operations on the TESTDATRNRPART<id> tables for all types of databases. The optimization is available as of BW support packages SAPKW73012, SAPKW73112 and SAPKW74007. Also make sure that no data targets exist with a high number of requests. See SAP Note 2037093 for more information. various, e.g.: 75a374cc7d49afa729f4379608270db4 SELECT TFACS Queries like the following are implicit SAP HANA factory calendar lookups: select \\\"IDENT\\\",\\\"JAHR\\\",\\\"MON01\\\", ... FROM \\\"<schema>\\\".\\\"TFACS\\\" WHERE \\\"IDENT\\\"=? ORDER BY \\\"JAHR\\\" ASC They are executed in context of SQL functions like WORKDAYSBETWEEN or ADDWORKDAYS. Usually the time per execution is quick (significantly below 1 ms) but there can be a high number of lookups depending on the modelling design. If a very high number of executions consumes significant time, you need to check the design in context of the factory calendar functions. Starting with SAP HANA 2.0 SPS 04 you can identify the responsible main database requests via the root statement hash, e.g. using SQL: \\\"HANA_Threads_ThreadSamples_FilterAndAggregation\\\" (STATEMENT_HASH = '<statement_hash>', AGGREGATE_BY = 'ROOT_STATEMENT_HASH') available via SAP Note 1969700 . When a more complex procedure or view is involved, you can check for all dependent objects using SQL: \\\"HANA_Objects_ObjectDependencies_Hierarchy\\\" (SAP Note 1969700 ) and inspect related procedures, functions or views for workday related functions. 6079a94e9b06f8fb6cde8b4f96fcc3f3 SELECT TFDIR TFDIR is usually single-record buffered on SAP ABAP side and so these single record accesses should not be directed to SAP HANA frequently. If you see a high amount of accesses, please check on SAP ABAP side if table buffering is set up properly and with reasonable sizes: In the technical settings (transaction SE11) table TFDIR should be shown with \\\"Buffering Activated\\\" and \\\"Single records buff.\\\" In transaction ST02 the table buffer hit ratio should be > 99 % and the amount of swaps in the table buffer should be low. It may be required to increase the following parameters if they aren't sufficiently sized, yet: Buffer size (MB): rsdb/tbi_buffer_area_MB Directory entries: rsdb/tbi_dir_entries See SAP Note 2103827 for more information related to the SAP table buffer parameters with SAP ABAP kernel 7.40 and higher. 1e4dcf71cd91a988edcdcabfac5ff3c8 SELECT FOR UPDATE TMY06 This SELECT FOR UPDATE and a related UPDATE of table TMY06 in context of include MRYF_COMMIT_WORK respectively application source RMNIWE20_01 is usually not required and can be removed with the coding correction available via SAP Note 3084147 . 56f3adf67e6975e26436563177651f21 SET TRANSACTION This \\\"SET TRANSACTION READ WRITE\\\" command is executed while establishing a connection to SAP HANA. Long runtimes are typically not caused by the statement itself, but by general infrastructure or SAP HANA issues. See SAP Note 2000000 and check for general performance issues, e.g.: High CPU consumption (SAP Note 2100040 ) Network issues (SAP Note 2222200 ) I/O issues (SAP Note 1999930 ) System replication issues (SAP Note 1999880 ) Lock issues (SAP Note 1999998 ) Savepoint issues (SAP Note 2100009 ) various, e.g.: 35e994d35949a8e6aad9836a756f5152 SELECT TRANSACTION_HISTORY See SAP Note 2388483 -> TRANS_TOKEN_HISTORY (which is the table accessed via view TRANSACTION_HISTORY) and make sure that the transaction history is kept at a reasonable size. 23b55b68c5131a580959ec19dc986441 2d5f7b7a147b7c23dfc28d19591d3b85 6a822f11564fc5b1779be273e2782077 9344fd6a680c25d0e28bd66c55b14e32 acdedb945a5b0083ecc662acac88af75 c94fc486be0b78a9b6e7d1f84882631d e1cdd703df87fc61ce8163fa107162a9 ee6a8a43d1165fe8cc2dd5b0c6b43799 f306ecaa6d72a7f56d0620259f8181a1 CALL TREXviaDBSL TREXviaDBSL is used to execute queries that are not possible with plain SQL (e.g. BW queries with execution modes > 0). In the SAP HANA SQL cache both the content and the bind values are hidden, so a direct analysis and optimization is not possible. See SAP Note 2800048 for more information related to TREXviaDBSL calls. 75ea317896b0f3b001e0bbdd462edc67 SELECT TRFC_O_UNIT This selection in class CL_BGRFC_UNIT_HANDLER_OUT_T can be executed very frequently in context of the bgRFC (background RFC) watchdog (passport action <BGRFC WATCHDOG>) in case of bgRFC inconsistencies (e.g. entries in table BGRFC_O_RUNNABLE without related entries in other tables). Check for inconsistencies via report RS_BGRFC_DB_CONSISTENCY and repair them to eliminate permanent watchdog activities. 0a67253485f849f94eac4d7acd74f71d 1716815ae5051c3afbc19c2c5902cc5b 1cbc1b492e2fcebad473cfc5eec8cba0 28e4bfc6c342971f9ab54c7abdf6250e 2f28892b2843a8b159ae89a1571e3e1a 4543c836acce6ef174c42a61e443134e 4696934057f8212f8dc5ef82c0c0380b 59b3a1d01421e944a0c7949f483f0e8c 6cc8ae0345f0cd530f6f3754fcd55688 74644c4d6d4426a4aeb5f4f3bb6b49a3 972175e7136b1569ca4979b2cf8e754c 973e198ee37f31f2cbc2b7b3252eacc3 a33e7292527107bc7ff6cfa2531b0a21 a5744848b279e912379e1d2501f4ad0b b2bb2d97fb55b064dcc303d88954ad2e c3c626adae60fcf3f8f662ac75477d41 c6ce4430af8001676f1860ac129b4ad5 e4460b851e3d9dea92825cf65aec3bf7 e63a0e779cdc70b9053276a455b99c5b SELECT TRFCQIN Different TRFCQIN selections in function module QIWK_RUN (SAPLQIWK) with QNAME and QCOUNT restrictions can be expensive due to a wrong optimizer index decision. Pilot SAP Note 2398412 provides an ABAP coding correction as a workaround. SAP Note 2700051 provides statement hints so that the queries are forced to use index TRFCQIN~1 even if the ABAP coding correction isn't in place. Independent of optimizer decisions accesses can also be expensive in case many records exist in table TRFCQIN. See SAP Note 375566 that provides analysis and cleanup guidance. 0bb645d182f850aa5beee23f9704f48b SELECT TRFCQIN This selection can be expensive in case many failed inbound queue RFCs exist (e.g. states RETRY, SYSFAIL or CPICERR). Check for failed RFCs in transaction SMQ2 and consider SAP Note 375566 for more details about analysis and resolution. 49ba27b9fc819c9a4589eeda7e787c7a SELECT TRFCQOUT This statement can suffer from the inability of SAP HANA to evaluate a MIN conditions based on an index. It is particularly slow if many records exist for the queried MANDT / QNAME / DEST combination. The following options exist to optimize the scenario: Avoid flooding TRFCQOUT with too many records for the same queue / destination. Move table TRFCQOUT to column store where the MIN condition is typically evaluated with better performance compared to row store. 04889f506b60827138e56436d3474786 0c0de3325053aed2e2a1260163974c8b 0e7c0ef5261f68e200db6db6070e2979 0f32e3f24f81f94a6812c093511a851f 1ee164315208046bb05ca070a2bfa3d9 219c467bfaff3e2ce2f9d66dc2e1e290 32c31293c1bc36f7d5bc880684000d40 3539d0edfd130e2a95708c7ef7a4f326 38f7c968a229dfb7ee2e8688888619b0 3a0a3b53f8aa937d796c7f45a9f94628 3d6233854ca57d2bea43e2a8ea593b70 46f7ed701af8e271a94a592944aeeda5 4cc734c0928b50441befd3782e52ca71 65541ea9ff3ca589ab3785a2d247270f 76e9d6bafebb0e8ee65c294eb589b295 7edde48cace745d72cfaa18f8c38cd23 80e3134b8b8ca5b6f0f652c5d538f94f 917a5ce67e2e9ae6049a542756a75374 92280dc954620c61741b9648e020311d 96e55a4f0622c3c6646ef03470701962 9ee1c7a5b0d1e6c4093f54b64c25189a aaf99cc61499b23eea6881439945544a ab60f50ee7fb8b2f7303561db045e5df b19b5145b99c58559e1678a39b4211c4 b5f9056a7ee1022042e20306c0b48e59 b7f626060290b24f84e777d5b56a2527 c387fc1ba5614b32f26f46c8762b4d07 c67e0ff151ad6c6de696a4b15f9d0c64 cdc85b0be8f104d0b6390f84fdc9f918 ddcf8f3f2322f37456fbdfa7d7d431fb e4ee5310d59f1a949294663ec9901639 e62051ca906a3a694d8b2345d0dcb942 f053abebad183def228db1b506d49307 f792b66c21e64442bac2276b94155a4a fa4b1ed668d15956ce86d56f3632a4d9 fc938671ce5893de9dbb99b81ca18698 fd11181f9b4580948ce5b658163a1c12 fe7ebefcad7f33418ea1d43558ea6bfe SELECT TRFCQOUT In order to minimize the risk of performance overhead caused by inadequate optimizer decisions you should implement the statement hints delivered via SAP Note 2700051 (SAP HANA >= 1.00.122.03). 616d53e49560d76d919176535b4bc185 e314592f5bce71690d6ae05d088a1d41 SELECT FOR UPDATE TSP01 Long runtimes of this database request are typically a consequence of transactional lock contention (SAP Note 1999998 ). If you see many spool work processes being active with printing into the file system (showing details like \\\"print 92336781/1\\\"), there may be contention / overload on lower layers. You can consider the following analysis steps and optimizations: If spooling to the file system is active (SAP Note 10551 , rspo/store_location = G) and spool work processes are often busy with printing, you should check in collaboration with your hardware partner if there is any bottleneck on operating system or file system level. As a workaround you can set rspo/store_location = db so that spool information is written to table TST03. Check if you can separate concurrent spool activities to different spool requests so that contention on spool request level is no longer possible. Avoid running spool intensive operations like the reorg jobs RSPO0041 and RSPO1041 (SAP Note 2388483 ) during times of high concurrent workload. Check if it is possible to reduce the amount of data spooled by the application. various, e.g.: 44efafa3b4d42376ddd05ae489eb02a2 6aef2dd2693056867b6aeef7d08d94d4 DELETE INSERT SELECT FOR UPDATE UPDATE TSP02 TSPEVJOB Modifications on spool tables like TSP02 or TSPEVJOB can terminate due to transactional deadlocks (SAP Note 1999998 ) in case printers can't be reached, e.g.: S temp disable PRTR for 300 s (until 1564113637) (0 retries): connect problem S *** ERROR => Safeguard: Printer PRTR disabled for 5 minutes [rspowunx.c 489] Connection to SAPLPD or LPD Broken; Computer <computer>. See SAP Note 173856 for more information. In order to eliminate the deadlocks you have to take care for the terminations and make sure that connection issues are reduced. You can find errors e.g. in the SAP ABAP syslog (transaction SM21) or in the work process traces (transaction ST11). It isn't easily possible to tackle the deadlock on its own. a4ba84c9f41719decfd7bf1472c79227 f0a0081a03e2d6275d3849853ddb9125 SELECT TST01 These selections are done in context of the TBTCO / TST01 consistency check, ABAP report RSTS0024 (SAP Note 666290 ). Consider the following recommendations: Make sure that the number of records in table TBTCO remains on a reasonable level (typically not more than 1 million) by keeping the number of scheduled jobs as small as possible and by implementing a proper house-keeping strategy (SAP Note 2388483 ). Avoid running report RSTS0024 too frequently (more than once a month). If required, an additional index on column DNAME can speed up the selections. 84cb85a3eba68fe6ae4fbcf29ad26a14 9b2eec360ac068a98bd0e7f2bd12f8aa bf5cee7981f70de69deeba55c59c3d1f (and others, dependent on <schema> and <client>) SELECT TTZCU The following selection from timezone table TTZCU happens in context of evaluating the ABAP_SYSTEM_TIMEZONE function that can be part of other database requests and view definitions: SELECT TOP 1 TZONESYS FROM <schema>.TTZCU WHERE CLIENT = '<client>' Also CDS functions like DATS_TIMS_TO_TSTMP, TSTMP_TO_DATS, TSTMP_TO_DST and TSTMP_TO_TIMS may implicitly access the TTZCU table. Starting with SAP HANA 2.0 SPS 04 you can use the root statement hash to track this implicit statement back to its origin, e.g. via SQL: \\\"HANA_Threads_ThreadSamples_FilterAndAggregation\\\" (STATEMENT_HASH = '<statement_hash', AGGREGATE_BY = 'ROOT') available via SAP Note 1969700 . Then you can check if the root statement can be optimized in terms of timezone related procedure calls. In scale-out systems it can be of advantage to activate table replication for table TTZCU (SAP Note 2340450 ) in order to make sure that the table information is locally available on all SAP HANA nodes. 9f0a8508e0543774562406e080a258d2 INSERT TXMILOGRAW This table holds data of the ABAP XMI interface. Increased trace levels can result in an unncessarily high data volume. Check the trace configuration as follows and make sure that tracing is only activated as short and targeted as possible: Server-side tracing: Function module SXBP_SET_TRACE or transaction RZ15 -> \\\"Goto\\\" -> \\\"Set Trace for XBP\\\" Tracing triggered by external scheduler: Check log entries in transaction RZ15. Entries like \\\"Setting audit level to 3\\\" indicate that an external job scheduler has increased the trace level. In this case, review the configuration of the external scheduler to minimize tracing. 9e514d0f1ca916769b36d52aec4ae1e9 SELECT T001L This access from /ISDFPS/CL_FDP_STOCK_LIST (transactions /ISDFPS/DISP_EQU_SIT or /ISDFPS/DISP_MAT_SIT) reads the whole T001L entries of the current client because a check for an empty FOR ALL ENTRIES list is missing on ABAP side. SAP Notes 2398901 and 2415082 provide corrections. DELETE INSERT UPDATE UCONCACONFIGRT UCONRFCCFGRT These modification requests can take longer due to exclusive lock waits (SAP Note 1999998 ) and eventually they are terminated due to lock wait timeout or deadlock. See SAP Notes 2904355 , 2907832 , 2943690 and 2979153 for application changes to minimize lock contention. 1c4fc4ab7e637405c3383d4140db89ef UPDATE UKMBP_CMS_SGM The update of table UKMBP_CMS_SGM can suffer from transactional lock contention (SAP Note 1999998 ) and deadlocks in case of a parallelized credit check. SAP Note 3145867 provides an ABAP coding correction. 7343a8b8c43d641e21e9e28339c23a89 SELECT USOB_AUTHVALTRC This query is linked to the ABAP authorization trace. See SAP Note 1854561 and make sure that the parameter auth/authorization_trace isn't permanently set to \\\"Y\\\", resulting in a global authorization trace. Either activate it only as short as possible or use reasonable filtering. In case you need to keep the authorization trace globally active, you can activate ABAP table buffering for table USOB_AUTHVALTRC in transaction SE13 so that the selections are satisfied in the ABAP table buffer and accesses to the database are no longer required. 09b3d2f0452e0e63a59bfcc5d7e3a99f 3e3b8e11cad214e9a7a5cec6193ad270 8b46525ae5c6cb03474ce50b0665d2ca edbc92bf95fdb9ed1e0746be18ae4e55 SELECT SELECT FOR UPDATE UPDATE \\ufeffUSR02 Accesses to USR02 are frequently executed in SAP ABAP environments in order to update user specific information. Long runtimes are typically not caused by the statement itself, but by general infrastructure or SAP HANA issues. See SAP Note 2000000 and check for general performance issues, e.g.: High CPU consumption (SAP Note 2100040 ) Network issues (SAP Note 2222200 ) I/O issues (SAP Note 1999930 ) System replication issues (SAP Note 1999880 ) Lock issues (SAP Note 1999998 ) Savepoint issues (SAP Note 2100009 ) 070f0383a839f783d1bc3a2ae66c21ae SELECT VARI This selection originating from include LSVARF02 (application source SAPLSVAR:21472) is executed frequently in context of batch jobs ODQ_TQ_JOB. See SAP Note 3080823 in order to minimize the load introduced by these batch jobs and perform housekeeping for RSPCLOGCHAIN (SAP Note 2388483 ). 5fe2acb9cfd7b372f100e8133e502cf2 939eaa61a3d9dfd4a8d610e607a8c8f6 cbf7228b6601e0485769afbfc5077b5a UPDATE VARINUM This update is usually expensive due to exclusive lock waits (SAP Note 1999998 ) when variants are created for simultaneously running batch jobs using the same programs. SAP Notes 1791958 and 2928893 provide corrections to minimize the lock contention using a dedicated connection for the VARINUM update to decouple the VARINUM commit from the batch job commit. The related report is RSDBSPJS. In addition you should check if you unintentionally scheduled concurrent batch jobs using the same program. aa9c7d24619a3fa5c9159c352f4dcb05 SELECT VBHDR This selection from application source SAPLC14Z / report C14Z_COMMIT_CHECK checks for related VBHDR entries that still need to be processed for a specific user (VBUSR) and transaction code (VBTCODE). This check is repeated in a loop until no more fitting records are found, so it is usually expensive due to the number of executions and not due to the individual execution time. Consider the following optimization approaches: Calling C14Z_COMMIT_CHECK with 'i_flg_wait_on_updtsk = lc_true' is a SAP internal setting and must not be used in production mode by customers. When you keep this parameter deactivated, the problem with the repeated VBHDR accesses will no longer happen. In cases when SAP standard coding suffers, you need to check why the VBHDR entries aren't processed in time. Eliminate related bottlenecks like RFC misconfigurations and make clean up previous failed updates (transaction SM13) to make sure that no orphan VBHDR records result in an endless selection loop. 6cc478091b517a1329060e2397101774 ba1c044211c8520ba553a12654cf2747 6066d80ea76a9efa5c6b80d7b3bbe198 SELECT LIKPUK VBAKUK VBRKUK In case many quick selections are executed on *UK tables in context of the DVM readiness check you can disable this unnecessary functionality as described in SAP Note 2820779 . 9dd0bb3115913df9208c3388f7f5f461 SELECT V_ML_ACDOC_EX_UL This query originating from function module IF_FML_XBEW_AGGREGATION~GET_AGGREGATION_MOTHER_MULTI can result in volatile optimizer decisions, resulting in varying runtimes and resource consumption. The following options exist to stabilize the execution plans: Add statement hint (SAP Note 2400006 ) NO_AGGR_THRU_JOIN for this query. Implement SAP Notes 3156880 and 3216523 878c325b36eaae5309594f17e0f7bc3f SELECT V_OP Selections from view V_OP in function module BP_JOB_SELECT_SM37C (application source SAPLBTCH) are related to explicit or implicit calls to transaction SM37C. This scenario can regularly happen in context of the job monitoring functionality of the FRUN Simple Diagnostics Agent. In this case the ENDTIME / ENDDATE conditions are typically most selective and creating an additional TBTCO index on columns ENDTIME and ENDDATE can improve the performance. In a real life scenario the runtime improved from 8470 ms to 17 ms after the index was created. 3ef368dc1b2ce64e1e8ae065f846e84f SELECT WLF_P_FACTORINGARITEMFLOW This selection executed in method _FILL_AR_ITEM_BUFFER of class CL_WLF_FACTORING_SERVICES is removed with the correction provided via SAP Note 3245234 . b74b5839acb3869a79b889c325e94070 SELECT XMII_CSTMATTRIBMAP This access is not expensive, but in context of transactional LOBs they can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. 97edbffda3d971680763d1653a73fe7c SELECT XMII_DB_MEMORY This access is not expensive, but in context of transactional LOBs they can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. b10c4869dce505ffda821289f9876d33 SELECT XMII_FILES XMII_PATHS This access is not expensive, but in context of transactional LOBs they can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. 4275f64f1da015109fbb7cb187a17534 SELECT XMII_JOBPROP This access is not expensive, but in context of transactional LOBs they can be responsible for a high number of SQL contexts and an increased size of the Pool/Statistics or Pool/RowEngine/QueryExecution/SearchAlloc. See SAP Notes 2220627 -> \\\"What are transactional LOBs?\\\" and 2711824 for more details. \\ufeff14. Is it required to create optimizer statistics in order to support optimal execution plans? It is in most cases not necessary to create optimizer statistics in the SAP HANA context. SAP HANA determines optimal execution plans by certain heuristics (e.g. based on unique indexes and constraints), by ad-hoc sampling of data or by internally collecting and re-using statistical information. Nevertheless there can be exceptions, e.g. for remote tables or when the query optimizer isn't able to determine column correlations properly. For details see SAP Note 2800028 . \\ufeff15. Are all database operations recorded in the SQL cache (M_SQL_PLAN_CACHE)? Standard operations like SELECT, INSERT, UPDATE or DELETE are recorded in M_SQL_PLAN_CACHE, but there are some exceptions listed below. In general the thread samples can be used to determine details. If more specific monitoring views are available, they are provided in column \\\"Alternative source\\\". Scenario SAP Note Alternative source Details IGNORE_PLAN_CACHE hint 2142945 M_SERVICE_THREAD_SAMPLES M_EXPENSIVE_STATEMENTS When the hint IGNORE_PLAN_CACHE is used, the SQL cache is bypassed and no information is stored there. DDL 2366291 M_EXECUTED_STATEMENTS DDL statements like CREATE, ALTER or DROP aren't stored in the SQL cache. The same applies to TRUNCATE operations. Instead some of operations can be found in the executed statements trace (SAP HANA >= 1.0 SPS 11). UPDATE ... WITH PARAMETERS commands like UPDATE \\\"T000\\\" WITH PARAMETERS('OPTIMIZE_COMPRESSION'='YES'); UPDATE \\\"T000\\\" WITH PARAMETERS('LOAD_TYPE' = 'UNLOAD'); are actually DDL commands but due to the UPDATE key word they were treated as DML commands up to SAP HANA 2.00.056 and stored in the SQL cache. They are no longer part of the SQL cache starting with SAP HANA 2.00.057. Backup 1642148 M_BACKUP_CATALOG M_BACKUP_CATALOG_FILES Backup operations (e.g. BACKUP DATA) are neither considered as DDL nor recorded in the SQL cache. Smart Data Access (SDA) 2180119 M_REMOTE_STATEMENTS Accesses to virtual tables in SDA environments aren't tracked in the SQL cache. Temporary objects in procedures 2003736 M_SERVICE_THREAD_SAMPLES M_EXPENSIVE_STATEMENTS Accesses involving temporary objects in procedures aren't cached starting with SAP HANA 1.00.74. See SAP Note 2003736 (\\\"Changed the implementation of SQL queries...\\\") for more details. Calculation view unfolding with specific features 2441054 M_SERVICE_THREAD_SAMPLES M_EXPENSIVE_STATEMENTS Starting with SAP HANA 1.0 SPS 12 unfolded calculation views aren't cached in specific scenarios. As a workaround it is possible to disable unfolding. Multiprovider pruning 2691117 M_SERVICE_THREAD_SAMPLES M_EXPENSIVE_STATEMENTS Queries taking advantage of multiprovider pruning can't be cached in the SQL cache because for technical reasons changes to the pruning table wouldn't result in an invalidation of the cache entry and the existing plan would produce wrong results. SAP Note 2691117 describes how to deactivate multiprovider pruning as a workaround (imposing a risk of performance regressions). You can set the database trace level for ceinstantiate to info in order to verify if multiprovider pruning is the reason for a missing SQL cache entry: indexserver.ini -> [trace] -> ceinstantiate = info The relevant trace file message is: Force transient plan because of mp pruning is active Ad-hoc queries via TREX_EXT_AGGREGATE 2749360 M_SERVICE_THREAD_SAMPLES M_EXPENSIVE_STATEMENTS With newer SAP basis SP levels ad-hoc queries via TREX_EXT_AGGREGATE are no longer recorded in the SQL cache because they anyway are never reused. \\ufeff16. Can sorting be supported by an appropriate index? No, due to the technical structure it is not possible that a sort operation is supported by an index. \\ufeff17. Is it possible to capture bind values of prepared SQL statements? Often SQL statements are prepared with bind variables in order to avoid frequent reparses. Rather than literals you will then see question marks in the WHERE clause, e.g.: SELECT * FROM \\\"A004\\\" WHERE \\\"MANDT\\\" = ? AND \\\"KNUMH\\\" = ? AND \\\"KAPPL\\\" = ? In order to understand selectivities and correlation and to be able to reproduce a problem with a SQL statement it is important to know which actual bind values are used. This information can be determined based on specific traces (e.g. the ST05 SQL trace in ABAP environments). On SAP HANA side the expensive statement trace captures bind values which can e.g. be evaluated via SQL: \\\"HANA_SQL_ExpensiveStatements_BindValues\\\" (SAP Note 1969700 ). Additionally SAP HANA is able to capture the bind values of critical SQL statements in the SQL plan cache per default as of SPS 08. This capturing is controlled by the following parameters: Parameter Details indexserver.ini -> [sql] -> plan_cache_parameter_enabled true: Activate capturing of bind values (for non-LOB columns) for long running SQL statements (default) false: Deactivate capturing of bind values indexserver.ini -> [sql] -> plan_cache_parameter_sum_threshold Minimum threshold for the total execution time of a SQL statement before first set of bind values is captured (in ms, default: 100000) indexserver.ini -> [sql] -> plan_cache_parameter_threshold After having captured the first set of bind values for a certain SQL statement, it will capture further sets of bind values if the single execution time exceeds the parameter value (in ms, default: 100) and is higher than the single execution time of the previously captured bind values. indexserver.ini -> [sql] -> plan_cache_parameter_for_lob_enabled true: Activate capturing of bind values for LOB columns for long running SQL statements, can result in signifcant data volume false: Deactivate capturing of bind values for LOB columns (default) The captured values are stored in view M_SQL_PLAN_CACHE_PARAMETERS and can be evaluated via SQL: \\\"HANA_SQL_StatementHash_BindValues\\\" (SAP Note 1969700 ). \\ufeff18. How can the performance of data modifications be tuned? Data modifications are operations modifying existing data in the database, e.g.: INSERTs (not the SELECT part of INSERT ... SELECT) UPDATEs, DELETEs, UPSERTs (not the evaluation of the WHERE clause) If you want to improve the performance of data modifications, you can consider the following areas: Area Details Lock waits See SAP Note 1999998 and optimize internal and transactional lock wait situations if required. Typical situation when an INSERT has to wait for a lock are: Blocking savepoint phase (ConsistentChangeLock) Concurrent modifications of same primary key (transactional lock) Delta storage contention (Sleeping, SleepSemaphore) DDL operation on same table active (transactional lock) I/O bottleneck (DeltaDataObjectAppendRollover) Columns During an INSERT every column has to be maintained individually, so the INSERT time significantly depends on the number of table columns. For UPDATEs the overhead depends on the number of columns in the SET clause, so try to avoid including columns in the SET clause when the original and new value is identical. When columns are unnecessarily specified, not only the modification itself needs to be done. SAP HANA also has to take care for dependent structures like multi-column indexes, primary keys or foreign key constraints. Indexes Every existing index slows down modifications. Check if you can reduce the number of indexes during mass modifications and data loads. SAP BW provides possibilities to automatically drop and recreate indexes during data loads. Primary index normally mustn't be dropped. Batch load If a high number of records is loaded, you shouldn't perform modifications for every individual record. Instead you should take advantage of batch loading options (i.e. inserting / updating multiple records with a single operation) whenever possible, for example using the \\\"FROM TABLE\\\" clause in ABAP. As a general rule of thumb batch sizes smaller than 100000 can introduce a measureable performance impact. The size of individual batches in SAP ABAP environments is controlled by the command buffer size that can be configured with SAP ABAP profile parameter dbs/hdb/cmd_buffersize. Per default it is set to 1048576 byte (1 MB). If the size of a record is 5000 byte, this would result in INSERT batch sizes of 200 records. An increase of dbs/hdb/cmd_buffersize (e.g. to 10485760 bytes / 10 MB) can increase the batch size and improve the performance. Be aware that the command buffer is allocated by each SAP ABAP work process, so the memory requirements on ABAP side will increase (dbs/hdb/cmd_buffersize * #work processes). Parallelism If a high number of records is loaded, you should consider parallelism on client side, so that multiple connections to SAP HANA are used to load the data. Partitioning Modifications can only use a single CPU per column and partition. Furthermore there can be delta storage contention (BTree GuardContainer waits, SAP Note 1999998 ) so in case of a very high amount of inserted records it can be helpful to configure more partitions (SAP Note 2044468 ). Be aware that an increased number of partitions can have negative side effects, so you should consider this option with care. Commits Make sure that a commit is executed on a regular basis when mass modifications are done (e.g after each bulk of a bulk load). Redo logging Processing of redo logs can be a significant overhead and result in different wait situations like LoggerBufferSwitch or LogBufferFreeWait (SAP Note 1999998 ). The following options exist to minimize the overhead: Make sure that disk I/O to the redo logs (SAP Note 1999930 ) and system replication (SAP Note 1999880 ) is working fine and without bottlenecks from a performance perspective. Consider increasing the log buffer size and count (SAP Note 2215131 ) if modifications have to wait for log buffer related locks. If it is possible to reconstruct a table completely in case of issues, you can consider deactivating delta logging via ALTER TABLE ... DISABLE DELTA LOG. Attention: When the delta log is disabled, the table content can no longer be recovered in case of crash or restore and so you need to be able to recreated the table content from redundant data. Delta merge Usually auto merges should do an acceptable job also during mass modifications. In exceptional cases deactivating auto merges and using an individual manual merge strategy can be useful. See SAP Note 2057046 for more details. An extremely large delta storage can reduce the load performance and increase the memory footprint, so executing delta merges also during mass modifications can be of advantage. Avoid repeated merges of small delta storages or with a high amount of uncommitted data in order to avoid unnecessary overhead. Table vs. record lock In cases where only a single, non-parallelized modification is possible and concurrent changes to the underlying table aren't required, it can be useful to use a global table lock instead of a high number of individual record locks. The table lock can be set via: LOCK TABLE \\\"<table_name>\\\" IN EXCLUSIVE MODE Afterwards SAP HANA no longer needs to maintain individual record locks. This approach is also valid for INSERT ... SELECT operations which may be parallelized internally. Savepoints Savepoints are required to write modified data down to disk (SAP Note 2100009 ). Normally it is the main intention to shorten the blocking savepoint phase as much as possible and accept longer savepoint durations at the same time. During mass imports the opposite can be better: Shorter savepoints with the risk of increased blocking phases. Shorter savepoints can reduce the amount of data written to disk and they can reduce the amount of logs that need to be kept, reducing the risk of file system overflows. During mass changes the following parameter adjustments can be considered to reduce the overall savepoint duration: lower values for global.ini -> [persistence] -> savepoint_max_pre_critical_flush_duration (e.g. 300 instead of 900) higher values for global.ini -> [persistence] -> savepoint_pre_critical_flush_retry_threshold (e.g. 10000 instead of 3000) INSERT ... SELECT When an INSERT is based on a SELECT, the SELECT part can be the dominating performance factor and it can be analyzed individually like a normal database query. In the special case when the SELECT is used as a scalar subquery, performance issues are possible with SAP HANA 2.0 SPS 04 (SAP Note 2911162 ). Trigger If triggers (SAP Note 2800020 ) fire for each modification, this can significantly extend the runtime. In the worst case a highly parallelized batch insert operation has to execute the individual trigger operations sequentially. Thus, you should make sure that only really required triggers are in place when doing high volume data loads. Triggers also prevent concurrent modification of different table partitions. Bugs The following SAP HANA bugs can have a negative impact on INSERT performance: Impacted Revisions SAP Note Details 1.00.120 - 1.00.122.11 2.00.000 - 2.00.012.00 1999998 If a lot of spatial data is inserted row-by-row without commit, the performance can be quite bad due to a SAP HANA bug and a lot of time is spent in call stack module AttributeEngine::spatialae::DeltaComponents::reserveDocid. As a secondary effect contention on \\\"GeometryDeltaAttribute Lock\\\" is possible. 2.0 SPS 00 - 2.0 SPS 02 2220627 INSERTs can suffer from packed LOB processing and spend a lot of time in call stack module DataContainer::VarSizeEntryUserDataHandler::getPageWithFreeSpaceFromFreeList. As a workaround you can disable packed LOBs by setting the following parameter: indexserver.ini -> [persistence] -> midsizelob_threshold = 0 In general this setting should be kept as short as possible, e.g. only during the time of the critical data load during migration. <= 2.0 SPS 02 2646143 Up to SAP HANA 2.0 SPS 02 mass inserts into temporary tables defined with NO LOGGING can slow down due to main storage optimizations that are executed after every insert batch. The related call stack module is typically AttributeEngine::AttributeValueContainer::commitOptimize. In ABAP environments the amount of insert batches and the related optimization overhead can be reduced by increasing the SAP profile parameter dbs/hdb/cmd_buffersize. See SAP Note 2600030 for suggestions to generally use higher parameter values for dbs/hdb/cmd_buffersize. Typical INSERT throughputs are: Constellation Typical throughput Problem situations like long critical savepoint phases or other locks < 500 records / second Normal, sequential single-row INSERTs 1,000 - 10,000 records / second Highly parallelized bulk loads 1,000,000 records / second \\ufeff19. Why are there significant differences between SQL statements on ABAP and SAP HANA side? The ABAP Open SQL statements are transferred to the database via the database interface (DBI). In many cases, the statement is modified in the DBI before being sent to the database. Typical adjustments are: Area Indicator SAP Note Details Bind variables ? 2124112 Per default literals are replaced with bind variables (\\\"?\\\"), so DOCNO = 1234 will show up as DOCNO = ? on database side. In case of newer ABAP version levels and constants, a replacement no longer happens and the constant value is directly passed to the database. SELECT * * A SELECT * (i.e. the selection of all columns) is propagated to SAP HANA if all of the following conditions are fulfilled: Primary connection ABAP kernel < 7.81 ABAP nametab is aware about field order (DBTABPOS) If at least one of the conditions is not fulfilled, a comma separated column list is generated. \\ufeffSELECT COUNT COUNT Counting the number of records via SELECT COUNT is usually done on database level and the result is returned to the ABAP. There is one important, exception, though. A SELECT COUNT in combination with FOR ALL ENTRIES reads all matching records via SELECT DISTINCT and performs the counting on ABAP side. This is required because it is possible that FOR ALL ENTRIES is split into several database requests with potentially overlapping result records. Example: ABAP coding: SELECT COUNT(*) FROM kna1 INTO max_count FOR ALL ENTRIES IN DD_KUNNR WHERE KUNNR EQ DD_KUNNR-LOW AND land1 IN dd_land1 AND konzs IN dd_konzs AND vbund IN dd_vbund AND (kna1_where). Database request: SELECT DISTINCT \\\"MANDT\\\" , \\\"KUNNR\\\" FROM \\\"KNA1\\\" WHERE \\\"MANDT\\\" = ? In this case the database request looks completely different from the ABAP coding because: ABAP itab DD_KUNNR is empty and so only the client condition is used in the WHERE clause SELECT COUNT is executed as SELECT DISTINCT <primary_key_columns> The SELECT DISTINCT can be much more expensive than the SELECT COUNT. Here it reads 90 million records in 40 seconds while a SELECT COUNT finishes in around 0.1 seconds. Thus, SELECT COUNT with FOR ALL ENTRIES and potentially large amounts of matching records should generally be avoided. National character set N'<literal>' Constant string literals are prefixed with 'N', indicating that the national character set is used, e.g. FLG_DELIVERED = N'X'. Empty ABAP variable for column in WHERE clause If a column with an empty variable is part of the WHERE clause on ABAP side, the DBI omits this condition and doesn't send it to SAP HANA. FOR ALL ENTRIES (single itab reference, no FDA WRITE) <column> IN ( ? , ... , ? ) 1987132 ABAP FOR ALL ENTRIES statements that don't use FDA WRITE and that only have a single reference to the itab in the WHERE clause are transformed into IN list selections with a maximum length of rsdb/max_in_blocking_factor (default: 100). FOR ALL ENTRIES (multiple itab references, no FDA WRITE) <conditions> OR ... OR <conditions> 1987132 ABAP FOR ALL ENTRIES statements that don't use FDA WRITE and that have multiple references to the itab in the WHERE clause are transformed into OR concatenation selections with a maximum length of rsdb/max_blocking_factor (default: 50). FOR ALL ENTRIES (empty itab) SELECT ... FROM <table> WHERE MANDT = ? In case of an empty FOR ALL ENTRIES itab all records of the current client are selected. Neither the FOR ALL ENTRIES columns nor any other restriction is propagated to the database. FOR ALL ENTRIES DISTINCT 1987132 Starting with ABAP kernel 7.42 a DISTINCT is generated for FOR ALL ENTRIES statements in any of the following situations: FDA is used and no LOB columns are selected The whole FOR ALL ENTRIES list can be satisfied with a single database selection and no LOB columns are selected FDA WRITE /* FDA WRITE */ ? AS \\\"t_00\\\" 2399993 FDA WRITE writes an ABAP itab to the database and joins it there, using the alias t_00. The comment \\\"FDA WRITE\\\" indicates this specific context. The most common context of FDA WRITE are FOR ALL ENTRIES statements, but it can also happen in context of mass modifications. This feature can be controlled with ABAP parameter rsdb/prefer_join_with_fda. FDA READ /* FDA READ */ 2399993 FDA READ reads a database result in ABAP itab structure from the database. Many database requests take advantage of this optimization that is indicated via \\\"FDA READ\\\" comment. This feature can be controlled via ABAP parameter rsdb/supports_fda_prot. ABAP data aging \\ufeffWITH RANGE_RESTRICTION('<date>') WITH RANGE_RESTRICTION('CURRENT')\\ufeff 2416490 This clause is added at the end of database requests when data aging is activated on ABAP side. This feature can be controlled with the following ABAP parameter: \\ufeffabap/data_aging = true | false\\ufeff ABAP table buffering /* Buffer Loading */ SELECT ... FROM <table> WHERE MANDT = ? ORDER BY <primary_index> If tables are completely or generically buffered on SAP side, the buffers are reloaded, if necessary, with special statements that may be completely different to the statement from the ABAP source code. The comment \\\"Buffer Loading\\\" indicates this context. ABAP table buffering /* Table Buffer could not be used */ This comment is included in a SQL statement in case ABAP table buffering is in place and could normally be used by the statement, but one of the following scenarios exists, preventing the use of the table buffer: 5 executions after a table buffer invalidation \\\"displaced\\\" or \\\"error\\\" buffer state, e.g. due to an insufficient ABAP table buffer size or a character sorting issue Buffer synchronizations (SAP Note 3186413 ) You can use ABAP transactions ST02 and ST10 to check for ABAP table buffer details. ABAP IN conditions \\\"=\\\", \\\"IN\\\", \\\"LIKE\\\", \\\"BETWEEN\\\", \\\">\\\", \\\"<\\\", \\\">=\\\", \\\"<=\\\" IN conditions on ABAP side can be converted into different types of conditions on the database level, depending on the selection criteria configured on ABAP side: \\\"=\\\", \\\"IN\\\", \\\"LIKE\\\", \\\"BETWEEN\\\", \\\">\\\", \\\"<\\\", \\\">=\\\", \\\"<=\\\" Columns both in selection list and WHERE clause Columns that appear in both the selection list and the WHERE condition are removed from the selection list if it is clear from the WHERE condition what the column's value must be. Expressions ending with blank and placeholder (<column> LIKE ? OR <column> LIKE ?) If an expression ends with a space followed by a placeholder, the system generates an OR concatenation as follows: SQL statement: ... WHERE <column> LIKE '<string> %' Statement after DBI transformation: ... WHERE (<column> LIKE '<string> %' OR <column> LIKE '<string>') Reason: In ABAP trailing blanks are always removed. So if the blank is the last character, it must be removed. If subsequent characters follow, it needs to remain. CDS view accesses /* Entity name: ... */ ABAP core data services (CDS) view accesses are indicated with a comment containing the related entity name. Compatibility view accesses /* Redirected table: <table> */ ABAP compatibility view accesses are indicated with a comment containing the related original table name. Native SQL being part of open SQL /* Contains Native SQL */ 2800008 Using the %_NATIVE syntax it is possible to include native, database specific coding in the ABAP open SQL statement. On SAP HANA level the comment \\\"Contains Native SQL\\\" is an indicator for this scenario. A common reason for this approach is the use of CONTAINS / FUZZY clauses in context of fulltext indexes. ABAP conversion exits ABAP conversion exits can be responsible for additional conditions that are not explicitly present in the ABAP source code. ABAP kernel requests The ABAP kernel may directly execute database requests that are not visible explicitly in the ABAP coding. \\ufeff20. How does the EXPLAIN functionality work? The following EXPLAIN functionalities exist: Command Rev. Details EXPLAIN PLAN FOR <sql_statement> Generation of explain plan for explicitly specified SQL statement <sql_statement> EXPLAIN PLAN FOR SQL PLAN CACHE ENTRY <plan_id> >= 90 Generation of explain plan for SQL statement with PLAN_ID <plan_id> in M_SQL_PLAN_CACHE This option is helpful to understand why a previously recorded SQL statement shows a different performance than current executions. Results of these commands are written to the EXPLAIN_PLAN_TABLE. Among others, you can use SQL: \\\"HANA_SQL_ExplainPlan\\\" (SAP Note 1969700 ) to evaluate the results. In order to identify the proper entries in EXPLAIN_PLAN_TABLE, you can use \\\"EXPLAIN PLAN SET STATEMENT_NAME = '<statement_name>' ...\\\" when generating the explain plan. Then you can check EXPLAIN_PLAN_TABLE for entries related to <statement_name>. \\ufeff21. How can I identify the client coding responsible for a database request? In order to understand the business background of a database request it is important to understand from where it originates. The following options can be used for drill-down. Be aware that limitations apply as described in Is client related information like application name or application source always filled properly? : Environment Option SAP Note Details all Thread samples 2114710 The thread samples views M_SERVICE_THREAD_SAMPLES and HOST_SERVICE_THREAD_SAMPLES contain application information in columns APPLICATION_SOURCE and APPLICATION_NAME. all Traces 2119087 The following traces provide application related information in the output: Expensive statements trace: APPLICATION_SOURCE and APPLICATION_NAME in M_EXPENSIVE_STATEMENTS Performance trace: APPLICATION_USER_NAME and APPLICATION_NAME in M_PERFTRACE all SQL statement views The SQL statement views M_PREPARED_STATEMENTS and M_ACTIVE_STATEMENTS contain APPLICATION_SOURCE information. ABAP SQL plan cache in DBACOCKPIT 2222220 The \\\"Navigation to Editor\\\" button allows to jump from a SQL statement to the related ABAP coding location. For performance reasons this may only work if the SQL statement is currently executed. With an ABAP basis SP level of at least 7.02 SP20, 7.30 SP18, 7.31 SP21, 7.40 SP18, 7.50 SP9, 7.51 SP4, 7.52 SP1 or 7.53 SP0 and SAP HANA 2.0 the coding location is also available if the statement isn't executed at the moment. ABAP Function module DB_SQL_SOURCE_DISPLAY Report RSDB6GETSOURCE If it is not possible to jump from the DBACOCKPIT SQL plan cache section to the ABAP location, you can manually call the ABAP function module DB_SQL_SOURCE_DISPLAY and execute it with the proper PROG_NAME and CONT_OFFS values (in SOURCE_REF) you can derive from the APPLICATION_SOURCE column available in views like M_SERVICE_THREAD_SAMPLES, HOST_SERVICE_THREAD_SAMPLES, M_PREPARED_STATEMENTS or M_ACTIVE_STATEMENTS. Example: Location Name Value Monitoring view APPLICATION_SOURCE SAPLSBAL_DB_INTERNAL:410 DB_SQL_SOURCE_DISPLAY PROG_NAME SAPLSBAL_DB_INTERNAL DB_SQL_SOURCE_DISPLAY CONT_OFFS 410 Similar functionality is also available via ABAP report RSDB6GETSOURCE. \\ufeff22. Are there known issues with particularly large SQL statement texts? SQL statements with a large SQL text can result in different problems. These statements are not necessarily complex statements from a processing logic, but may be large for other reasons, e.g. due to a very long IN list. The following problems can originate from large SQL texts: Problem SAP Note Details Long preparation times 2092196 Very long IN lists can result in long preparation times with processing in module ptime::qo_Comp::is_same_pred. Blocked garbage collection 2169283 SQL statements typically can't be terminated while they are prepared, so long preparation times can also result in blocked garbage collection. In the worst case only a SAP HANA restart can resolve the problem. Increased Pool/malloc/libhdbrskernel.so allocator size 1999997 2114710 The allocator Pool/malloc/libhdbrskernel.so can grow when repeatedly thread samples of very large SQL statements are taken, including thread details. Terminations 2124112 If very long SQL statements are parsed, the following termination can happen: [3]: fatal error: cannot generate more expressions It can be useful to reduce or limit the maximum allowed SQL statement size. The following options exist: Area Approach / Parameter Details SAP ABAP dbs/hdb/stmtlng You can use the SAP ABAP profile parameter dbs/hdb/stmtlng to limit the maximum SQL statement size. The default is 104857600 byte, so 100 MB. This large value basically disables the size threshold and SQL statements of any size are sent to SAP HANA. As an example, the following setting would implement a statement length limitation of 2 MB: dbs/hdb/stmtlng = 2097152 Be aware that there are specific situations (e.g. in the context of TREX_EXT_CREATE_CALC_SCENARIO in BW) where statements of 10 MB and more may be generated. A SQL statement exceeding the defined limit will be terminated with an error message like: *** ERROR => max. statement length (2097152) exceeded [dbhdbsql.cpp 1051] SAP ABAP FOR ALL ENTRIES If SQL statements with long IN lists are generated, you can cut it into individual pieces using the ABAP FOR ALL ENTRIES feature. The split is done based on the configured blocking factors. See SAP Note 1987132 for more details. SAP BW TREX modules SAP Notes 2294033 and 2341605 provide optimizations in handling large database requests in the context of TREX modules. \\ufeff23. How can details for prepared SQL statements be determined? In order to reduce parsing overhead (SAP Note 2124112 ), bind variables are used in many environments like SAP ABAP: Variant Example SQL command Literals SELECT * FROM DBSTATC WHERE OBJOW = 'SAPR3' AND DBOBJ = 'AFPO' Bind variables SELECT * FROM DBSTATC WHERE OBJOW = ? AND DBOBJ = ? It can make a significant difference in terms of execution plan, performance and resource consumption if a SQL statement is executed with explicit literals or with bind variables. Therefore it is recommended that you analyze an expensive SQL statement that uses bind variables in the same way, i.e. also with bind variables. This can be achieved by using a prepared SQL statement. SAP Note 2410208 describes how to generated an explain plan for a prepared SQL statement. Similarly you can create a PlanViz (SAP Note 2073964 ) of a prepared statement by choosing \\\"PlanViz\\\" -> \\\"Prepare\\\" in the SQL editor of SAP HANA Studio. \\ufeff24. Are there special considerations for analyzing SQLScript procedures? General advice for the following standard SQLScript procedures is provided in \\\"Are there standard recommendations for specific SQL statements available?\\\" above: BW_CONVERT_CLASSIC_TO_IMO_CUB BW_F_FACT_TABLE_COMPRESSION CHECK_TABLE_CONSISTENCY DSO_ACTIVATE_PERSISTED DSO_ROLLBACK_PERSISTED STATISTICS_SCHEDULABLEWRAPPER Other statistics server related procedure (SAP Note 2147247 ) You can use SQL: \\\"HANA_SQL_ActiveProcedures\\\" (SAP Note 1969700 ) in order to monitor database requests related to currently running SQLScript procedures. The following parameters can be used to activate an increased retention time in the underlying monitoring view M_ACTIVE_PROCEDURES so that requests can still be analyzed after the actual procedure call is finished: Parameter Default Unit Details indexserver.ini -> [sqlscript] -> execution_monitoring_level 2 SQLScript monitoring level for SAP HANA 1.0: 0 -> disabled 1 -> full information, but STATEMENT_PARAMETER not populated 2 -> full information 3 -> full information + additional tracing of intermediate status of internal statements in database trace Attention: With SAP HANA >= 2.0 the parameter monitoring_level is used for this purpose. indexserver.ini -> [sqlscript] -> execution_monitoring_limit 10000 Maximum number of child records for an individual procedure call indexserver.ini -> [sqlscript] -> monitoring_level 1 SQLScript monitoring level for SAP HANA >= 2.0: 0 -> reduced information (e.g. without STATEMENT_PARAMETER, STATEMENT_START_TIME and STATEMENT_END_TIME) 1 -> full information, but STATEMENT_PARAMETER not populated 2 -> full information Attention: With SAP HANA 1.0 the parameter execution_monitoring_level is used for this purpose. indexserver.ini -> [sqlscript] -> number_of_calls_to_retain_after_execution calls Number of procedure calls to be retained after execution is finished If this parameter is set and retention_period_for_sqlscript_context isn't set, a value of 3600 s will be used for retention_period_for_sqlscript_context will be used. If both parameters aren't set, nothing will be retained after execution. indexserver.ini -> [sqlscript] -> retention_period_for_sqlscript_context s Retention time after end of procedure call If this parameter is set and number_of_calls_to_retain_after_execution isn't set, a value of 100 calls will be used for number_of_calls_to_retain_after_execution. If both parameters aren't set, nothing will be retained after execution. Values like 1000 retained calls and 3600 seconds retention time can be a reasonable starting point. \\ufeff25. How can compatibility view accesses be tuned? Compatibility views (or shorter: compat views) are required to map tables from the old world (e.g. COSS) to tables of the new S/4HANA world (e.g. ACDOCA) so that old coding can still work without changes. This mapping is typically rather complex, involving a lot of views and dependencies, so there is a significant risk that there are performance regressions compared to the former plain table access. You can identify compatibility view accesses using SQL: \\\"HANA_SQL_SQLCache_SpecialStatements\\\" (STATEMENT_CLASS = 'COMPAT_VIEW') available via SAP Note 1969700 . Below you can find typical tuning approaches in context of compatibility views: Tuning approach Details Standard SQL and performance optimization In order to make sure that no standard issue impacts the compatibility view performance (e.g. infrastructure, parameter settings, missing index) you should at first make sure that the recommendations of SAP Note 2000000 are in place and you should perform an initial SQL analysis based on the instructions of this SAP Note here. Particularly pay attention for SAP HANA and SAP ABAP parameter recommendations provided in SAP Note 2600030 . Avoid individual queries Individual queries against compatibility views (e.g. coming from ABAP transaction SE16) without reasonable restrictions can be responsible for long runtimes and high resource consumption, so you should make sure that the possibility of individual queries is as restricted as possible and users should be trained to avoid unselective conditions. SAP HANA hints Check if the execution time improves when using a SAP HANA hint (SAP Note 2142945 ). In particular the following hints can make a difference: USE_OLAP_PLAN / NO_USE_OLAP_PLAN CS_JOIN / NO_CS_JOIN CS_UNION_ALL / NO_CS_UNION_ALL LIMIT_THRU_JOIN / PRELIMIT_BEFORE_JOIN when limiting the result significantly, see SAP Notes 2793263 , 2900345 in context of ABAP transaction SE16 / SE16N If a hint works out fine you can consider pinning it to the related database requests (SAP Notes 2222321 , 2400006 ), at least as a temporary workaround until a better solution is available. Application changes Several best practices exist how to optimize the use of compatibility views from an application perspective: Objects (traditional tables, compatibility views) SAP Notes ANEA, ANEK, ANEP, ANLC 2572477 , 2796770 , 2816301 , 2816302 BSAD, BSID, BSAK, BSIK, BSAS, BSIS 2219527 COEP, COSP, COSS, COVP, V_COEP, V_COVP 2185026 , 2222535 FAGLFLEXT, FMGLFLEXT, GLT0, JVGLFLEXT, PSGLFLEXT 2221298 MARC, MARD, MARDH, MBEW, MBEWH, MBVMBEW, MCHB, MCHBH, MKPF, MSEG, NSDM_V_MARC, NSDM_V_MARD, NSDM_V_MARDH, NSDM_V_MCHB, NSDM_V_MCHBH, NSDM_V_MKPF, NSDM_V_MSEG 2206980 , 2217299 , 2337368 , 2713495 More frequent precompacting See SAP Notes 2246602 and 2342347 and consider a more frequent precompacting of data in tables ACDOCA_M_EXTRACT and / or MATDOC_EXTRACT. Switch from compatibility view to direct S/4HANA table access Compatibility views are an intermediate solution to map the old application coding to the new table structures. In the long term the application coding should directly access the S/4HANA tables and so the compatibility view accesses are no longer required. \\ufeff26. How can SLT specific performance issues be tuned? SLT (SAP Note 2014562 ) is used to replicate tables into other systems. The replication mechanism is rather standardized and so there are some typical SLT performance issues that are described in the following overview. Further details about SLT performance tuning can be found in the SLT Performance Optimization Guide . Scenario Details Sequence performance Changes in the main tables are tracked in logging tables using SAP HANA triggers and sequences (SAP Note 2600095 ). In order to optimize the sequence handling, you can consider the following: Activate sequence caching (e.g. \\\"CACHE 100\\\") Use DMIS 2011 SP15 or higher where sequence caching (with a cache size of 100) is active per default. When doing a full replication you should make sure that for the target table no SLT replications to other systems are active. Otherwise the efficient full synchronization from the source to the target system can be massively slowed down by the logging of the changes of the target table using triggers and sequences. In scale-out: Put source tables for SLT replication on the master node if possible, in order to avoid network communications with the sequence manager on the master node In scale-out: Avoid distributing partitions of source tables for SLT replication to different SAP HANA nodes in order to avoid frequent remote requests and shipment of the sequence cache between the nodes. Trigger performance Changes in the main tables are tracked in logging tables using SAP HANA triggers (SAP Note 2800020 ) and sequences (SAP Note 2600095 ). In order to optimize the trigger activities, you check SAP Note 2800020 -> \\\"What do I have to consider in terms of database trigger performance?\\\". Initial load Initial loads based on \\\"Reading Type 3 - Primary Key Order\\\" (as described in the SLT Performance Optimization Guide ) are not efficient on SAP HANA in case of large source tables due to the fact that the primary key doesn't provide a sorted result set and so a large number of rows has to be read and sorted before returning the first few thousand of rows in the sorted order. Related queries have an ORDER BY clause based on the primary key, \\\"=\\\" conditions on some leading index columns and a \\\">\\\" condition on the subsequent primary key column (not necessarily the last column in the index). Example: SELECT * FROM \\\"ZTABLE\\\" WHERE \\\"MANDT\\\" = ? AND \\\"ZTABLE_SYSTEMID\\\" = ? AND \\\"MATNR\\\" > ? ORDER BY \\\"MANDT\\\" , \\\"ZTABLE_SYSTEMID\\\" , \\\"MATNR\\\" , \\\"STATM\\\" , \\\"ZHLER\\\" In this case you can consider the following optimizations: Use an initial load based on other mechanisms (e.g. \\\"Reading Type 1 - Range Calculation\\\" or \\\"Reading Type 4/5 - Sender Queue\\\"). Increase the package size significantly (LTRS -> \\\"Table Settings\\\" -> \\\"Size of Read Portions\\\", default: 10000). This information is technically stored in column RD_PORTION_SIZE of table IUUC_REPL_TABSTG. Logging table selection When content in logging tables is replicated to the target systems, selections like the below example are executed: SELECT TOP 5000 * FROM \\\"/1CADMC/00296603\\\" WHERE \\\"DOCNUM\\\" >= ? AND \\\"DOCNUM\\\" < ? AND \\\"IUUC_PROCESSED\\\" IN ( ? , ? ) ORDER BY \\\"IUUC_PROCESSED\\\" , \\\"DOCNUM\\\", \\\"MANDT\\\" , \\\"COUNTER\\\" , \\\"SEGNUM\\\" In order to optimize these requests, you can consider the following aspects: Keep the amount of data in logging tables as small as possible. If the amount of data that has to be sorted is quite large and the package size is rather small (\\\"TOP 5000\\\" in the case above), you can maintain a larger value (e.g. 99999) in column NUM_RECS_LOGTAB of table IUUC_PERF_OPTION for the involved source table and regenerate the runtime object. Then you can process significantly more records and still sort only once. Be aware that the length of the field is limited to 5 characters, so values beyond 99999 aren't possible. When increasing the package size you also have to make sure that the data volume processed in one package (i.e. package size * length of source table record) doesn't exceed 1 GB. In transaction LTRS you can use the \\\"Performance Options\\\" -> \\\"Replication Options\\\" to configure a parallelization of the replication. For this purpose, in section \\\"Ranges for Logging Table\\\" you can choose \\\"Use Key Fields to calculate Ranges\\\". The above example uses an older, no longer recommended option to parallelize the replication by defining a parallelization column (DOCNUM in the above example) in PARALLEL_FIELDNAME of table IUUC_PERF_OPTION. This parallelization column is included as second column of an index after column IUUC_PROCESSED. As SAP HANA can't evaluate ranges in multi column indexes, you can manually create an additional single column index on the parallelization column (in this case on DOCNUM). \\ufeff27. How can SAP ABAP client copies be tuned? The following table lists common performance tuning approaches for SAP ABAP client copies using transaction SCCL or SCC9: Scenario SAP Note Details General optimization options 2163425 This SAP Note provides an overview about general performance optimizations. Optimization of default behavior 2550545 This SAP Note provides several coding corrections for the default client copy behavior. Expert settings for SAP HANA contexts 2555451 This SAP Note suggests specific settings to speed up client copies in context of SAP HANA. Parallelism configuration 541311 This SAP Note describes how to define a reasonable parallelism for client copies. Deletion of target client If the target client already exists, you should use transaction SCC5 to delete it before starting a new client copy. Otherwise a quite time-consuming record-by-record deletion happens at the beginning of the client copy. \\ufeff28. Are there best practices for efficient application development on SAP HANA? Consider the following general rules when developing applications on SAP HANA: Only select the columns that you actually require in the current context. Selecting many columns imposes some overhead in a column oriented database like SAP HANA. Avoid executing many small requests and instead execute fewer requests with a larger result set. SAP HANA is particularly efficient in processing larger requests and at the same time overhead due to network roundtrips is avoided. Specify ORDER BY whenever you require a sorted result set. This should be obvious, but due to the fact that other databases sometimes appear to return the data in a sorted manner (e.g. based on primary key) even without ORDER BY, this explicit sort is sometimes forgotten. See the ABAP Development Performance Guidelines for more information how to create ABAP code that is optimized for SAP HANA. SAP Note 1912445 provides best practices and tools for custom code when migrating to SAP HANA. \\ufeff29. What is the root statement hash? For normal database requests the root statement hash is identical to the statement hash - i.e. it is a 32 character hash representation for one specific SQL statement that is generated based on the SQL text. In cases where a main database request triggers other implicit database requests (e.g. a procedure call that contains database requests) the root statement hash refers to the outer database request (e.g. the procedure call). This makes it easier to understand the context of a statement execution. The root statement hash is available starting with SAP HANA 2.0 SPS 04 via column ROOT_STATEMENT_HASH of monitoring views M_SERVICE_THREADS, M_SERVICE_THREAD_SAMPLES and M_PERFTRACE and statistics server history HOST_SERVICE_THREAD_SAMPLES. For example, the statistics server schedulable wrapper call (root statement hash d6fd6678833f9a2e25e7b53239c50e9a) executes various more or less expensive internal database requests with different statement hashes that are recorded in the thread samples: --------------------------------------------------------------------------- |ROOT_STATEMENT_HASH |STATEMENT_HASH |SAMPLES| --------------------------------------------------------------------------- |d6fd6678833f9a2e25e7b53239c50e9a|4359b2aac11d212ea7ad9153cd25b7f8| 976 | |d6fd6678833f9a2e25e7b53239c50e9a|d6fd6678833f9a2e25e7b53239c50e9a| 604 | |d6fd6678833f9a2e25e7b53239c50e9a|3554da8720da1f2de90f6ff0caa503a2| 574 | |d6fd6678833f9a2e25e7b53239c50e9a|d41efb11db78d68b7b1e4cdc275216a0| 313 | |d6fd6678833f9a2e25e7b53239c50e9a|420f78838e9d2b95d948d8401556fba0| 294 | |d6fd6678833f9a2e25e7b53239c50e9a|9d2c75830edbc1ecacfeb7577637325f| 243 | |d6fd6678833f9a2e25e7b53239c50e9a|eaa7a4db84281fc26370f913da7df597| 166 | |d6fd6678833f9a2e25e7b53239c50e9a|28ca313866cc6b3302bfbbcda7dc1084| 158 | |d6fd6678833f9a2e25e7b53239c50e9a|c15e482937b9628a627e27b537444445| 154 | |d6fd6678833f9a2e25e7b53239c50e9a|9aaf8105bdad5308ab706eea28078d28| 124 | ... --------------------------------------------------------------------------- \\ufeff30. Are all SQL cache entries written to the history in HOST_SQL_PLAN_CACHE? The SQL cache (M_SQL_PLAN_CACHE) can contain hundreds of thousands of SQL statements and it would be very inefficient to write all of them to the statistics server history HOST_SQL_PLAN_CACHE. Therefore some important key figures are considered and only the 100 SQL cache entries with the highest delta values (since the last snapshot) are considered, e.g.: Top 100 execution time Top 100 executions Top 100 lock wait time Top 100 result records Top 100 execution memory size (SAP HANA >= 2.00.040) \\ufeff31. Where do I find details about the SAP HANA SQL optimizer? You can find details about the SAP HANA SQL optimizer in the SAP HANA SQL Optimizer section of the SAP HANA Performance Guide for Developers . \\ufeff32. Where can I find details about unfolding? The term unfolding is used in different contexts - for calculation views and for TUDFs. On the one hand side it indicates that a calculation engine request is translated into SQL and handed over to the SQL engine for processing. See SAP Note 2291812 for more details. You can use the hints CALC_VIEW_UNFOLDING / NO_CALC_VIEW_UNFOLDING (SAP Note 2142945 ) and related advanced view properties to control calculation view unfolding. Calculation view unfolding can have significant impact on resource consumption and runtime because completely different code paths are used for query processing. In general unfolding is of advantage, but there can be exceptions. On the other hand side unfolding can be related to table user-defined functions (TUDF). Unfolding in context of TUDF means to combine different SQL statements inside the SQLScript body into a single SQL statement and convert it into query optimizer (QO) operators. This can often improve the quality of the generated execution plan. See the blog How to investigate if table user-defined function is unfolded or not? for details. Certain scenarios can block TUDF unfolding: Reason Details NOT UNFOLDED BECAUSE FUNCTION BODY CANNOT BE SIMPLIFIED TO A SINGLE STATEMENT Multiple statements in TUDF body cannot be simplified into a single statement. NOT UNFOLDED DUE TO ANY TABLE TUDF uses ANY TABLE type. NOT UNFOLDED DUE TO BINARY TYPE PARAMETER TUDF has a binary type as its parameter. NOT UNFOLDED DUE TO DEV_NO_SQLSCRIPT_SCENARIO HINT The caller of TUDF disables unfolding with the DEV_NO_PREPARE_SQLSCRIPT_SCENARIO hint. NOT UNFOLDED DUE TO DEBUGGING SESSION TUDF is executed in debugging session. NOT UNFOLDED DUE TO ENCRYPTED PROCEDURE OR FUNCTION TUDF is an encrypted function. NOT UNFOLDED DUE TO IMPERATIVE LOGICS TUDF has an imperative logic, including SQLScript IF, FOR,WHILE, or LOOP statements. NOT UNFOLDED DUE TO INTERNAL SQLSCRIPT OPERATOR TUDF unfolding is blocked by an internal SQLScript operator. NOT UNFOLDED DUE TO INPUT PARAMETER TYPE MISMATCH The type of the input argument does not match the defined type of the TUDF input parameter. NOT UNFOLDED DUE TO JSON OR SYSTEM FUNCTION TUDF uses JSON or system function. NOT UNFOLDED DUE TO NATIVE SQLSCRIPT OPERATOR TUDF has a SQLScript native operator, which does not have an appropriate SQL counterpart. NOT UNFOLDED DUE TO NO CALCULATION VIEW UNFOLDING The caller of TUDF disables Calculation View unfolding. NOT UNFOLDED DUE TO PRIMARY KEY CHECK TUDF has a primary key check. NOT UNFOLDED DUE TO RANGE RESTRICTION Table with RANGE RESTRICTION is used within the TUDF. NOT UNFOLDED DUE TO RECURSION The TUDF has a recursive call. NOT UNFOLDED DUE TO SEQUENCE OBJECT A SEQUENCE variable is used within the TUDF. NOT UNFOLDED DUE TO SEQUENTIAL EXECUTION TUDF is executed with SEQUENTIAL EXECUTION clause. NOT UNFOLDED DUE TO SPATIAL TYPE PARAMETER TUDF has a spatial type as its parameter. NOT UNFOLDED DUE TO TIME TRAVEL OPTION TUDF uses a history table OR the time travel option is used. NOT UNFOLDED DUE TO WITH CLAUSE TUDF uses a WITH clause. NOT UNFOLDED DUE TO WITH HINT TUDF uses a WITH HINT clause that cannot be unfolded. NOT UNFOLDED DUE TO WITH PARAMETERS TUDF uses a WITH PARAMETERS clause. NOT UNFOLDED DUE TO XML CLAUSE TUDF has an XML clause. See EXPLAIN PLAN for Table User-Defined Functions for details. \\ufeff33. Is client related information like application name or application source always filled properly? Monitoring views like M_SQL_PLAN_CACHE or M_SERVICE_THREAD_SAMPLES contain columns with application related information like APPLICATION_NAME, APPLICATION_USER or APPLICATION_SOURCE. These values are only filled properly when the application explicitly sets it on session level (see SET Statement ). In SAP ABAP contexts this information is per default provided during prepare operations, i.e. during parsing. This means that details like the coding location (APPLICATION_SOURCE) is not necessarily correct on session level in case database requests are executed and the last prepare in the session happened at a different coding location. While these details are correct in M_SQL_PLAN_CACHE (because it is populated during prepare), it can be wrong in other views like M_SERVICE_THREAD_SAMPLES. If you want to make sure that the application source information is always up to date, you can set the following SAP ABAP profile parameter in case SAP ABAP kernel >= 7.77 (324), >= 7.81 (110) or >= 7.83 (11) is in place (SAP Note 3017584 ): dbs/hdb/send_application_source = 2 Be aware that this setting can introduce performance overhead, so it should be tested and monitored. \\ufeff34. What are typical reasons for internal statement executions? Internal SAP HANA statement executions are indicated in M_SQL_PLAN_CACHE via IS_INTERNAL = 'TRUE'. Unlike database requests directly triggered by end users these internal statements are triggered by internal SAP HANA mechanisms. Check ID M1165 (\\\"Internal executions (%)\\\") of the SAP HANA Mini Checks (SAP Note 1999993 ) reports a potentially critical issue if the share of internal executions is significant. Typical scenarios that can result in an increased amount of internal executions are: Scenario SAP Note Detail Trigger processing 2800020 The implicit processing of triggers in context of a statement execution happens as internal statement. In case many triggers exist and fire, the amount of internal statements can be significant. Blocked TUDF unfolding 2000002 If procedures / TUDFs can't be unfolded, parts of it are executed individually as internal statement. Make sure that unfolding is blocked as rarely as possible in order to reduce the amount of internal statements. The amount of internal statements can be even larger if no bind variables are used, because then for every different bind value a different statement is stored. You can use SQL: \\\"HANA_SQL_SQLCache_SpecialStatements\\\" (STATEMENT_CLASS = 'PROC_NOT_UNFOLDED') available via SAP Note 1969700 in order to display internal statements originating from unfolding limitations. Activated TREXviaDBSL SQL cache trace 2800048 The SQL cache trace for TREXviaDBSL can be activated with the following setting: indexserver.ini -> [sql] -> plan_cache_trexviadbsl_enabled = true Subsequently internal cache entries are stored with a statement string of the following convention: TrexViaDbsl42584D4C3F0356455203302E3... Make sure that the trace is only active as targeted as possible if you want to avoid the generation of these internal statement. You can use SQL: \\\"HANA_SQL_SQLCache_SpecialStatements\\\" (STATEMENT_CLASS = 'TREXVIADBSL_TRACE') available via SAP Note 1969700 in order to display SQL cache entries related to the TREXviaDBSL SQL cache trace. Statistics server 2147247 Database requests issued by the SAP HANA statistics server (user _SYS_STATISTICS) are marked as internal. In systems with limited production load these requests can be responsible for a significant portion of the SQL cache without being an issue. Thus, check ID M1165 (\\\"Internal executions (%)\\\") of the SAP HANA Mini Checks (SAP Note 1999993 ) generally ignores statistics server requests. \\ufeff\\ufeff35. How can core data services (CDS) view accesses be optimized? Core data services views (CDS views) can be quite complex and so it is important to adhere to best practices in order to avoid long runtimes and instabilities. In terms of ABAP CDS accesses (involving ABAP CDS views, ABAP CDS hierarchies and ABAP CDS view entities) you can use SQL: \\\"HANA_SQL_SQLCache_SpecialStatements\\\" (STATEMENT_CLASS = 'CDS_VIEW') available via SAP Note 1969700 to identify the most expensive CDS view accesses based on the SAP HANA SQL cache. On top of the general suggestions provided in this SAP Note further important considerations for achieving good performance are: ABAP Core Data Services - SAP S/4HANA Best Practice Guide ABAP Core Data Services - SAP Business Suite Best Practice Guide ABAP Development Performance Guidelines Specify join cardinality TO ONE / [0..1] / [1] and TO MANY / [0..*] / [*] / [] whenever possible. Avoid aggregations on calculated fields, e.g. by deferring on-the-fly calculations to the consumption layer if possible, using annotations for aggregation instead of aggregation functions, so that the GROUP BY clause can be generated dynamically (as long as most queries do not ask for the calculated field). In case hard-coded aggregation is needed (for example for pre-aggregation purposes) aggregate as early as possible (on as few joins as possible) on as few fields as possible and strive for aggregation on persisted database fields. Access base table instead of redirected views whenever possible. Avoid re-use of calculated fields or calculated expressions within the data model (anything else than pure projection into the field list). Avoid using compatibility views as data sources for performance intensive accesses. Minimize the number of joins by using the shortest possible association path. Avoid NOT NULL-preserving calculations (like ELSE with a constant, COALESCE with a constant) in views to be used as association targets or on right side of a left outer join. Try to re-formulate the join conditions using on the fly calculations without calculations on persisted database table fields if possible. In case of simple CASE expressions consider refactoring into UNION ALL. Consider persisting the calculated values or in case of ABAP code context, moving the join to ABAP (by SELECT FOR ALL ENTRIES) in case the calculation in the condition cannot be avoided. Alternatively use global temporary tables to simplify huge number of line items caused by FOR ALL ENTRIES. Avoid non-equal join conditions in ON clause for large tables in SAP HANA column store \\u2013 consider moving interval checks into WHERE condition Avoid OR operator in ON condition for large tables in SAP HANA column store \\u2013 consider refactoring into UNION ALL. Consider using minimalistic ABAP CDS views for search helps via annotation @Consumption.valueHelp or @Consumption.valueHelpDefinition, especially avoiding search helps on calculated fields. For simple CASE expressions to be used in search help, consider refactoring into UNION ALL. Use ABAP layer for data pre-calculation, especially with SAP Analytic Cloud (SAC) to avoid unnecessary parameters calculations on CDS level. In case of long runtime or high CPU consumption caused by CDS views using fast data access (FDA), consider deactivating it (SAP Note 2399993 ).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "\n",
    "    # Initialize with your uploaded file\n",
    "extractor = SAPKBAExtractor('../2000002.json')\n",
    "\n",
    "# Run Extraction\n",
    "dataset = extractor.extract_pairs()\n",
    "\n",
    "# Save Result\n",
    "extractor.save_to_jsonl(dataset, 'sap_instruction_dataset.jsonl')\n",
    "\n",
    "# Preview\n",
    "print(\"\\n--- Sample Pair ---\")\n",
    "print(json.dumps(dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e4560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

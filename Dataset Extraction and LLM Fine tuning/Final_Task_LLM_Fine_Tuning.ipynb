{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a5d48e",
   "metadata": {},
   "source": [
    "# ðŸš€ Final Task: LLM Fine-Tuning & Evaluation\n",
    "\n",
    "### ðŸ“ Overview\n",
    "In this final stage, we transformed a \"generalist\" AI model (**TinyLlama-1.1B**) into a \"specialist\" for SAP technical support.\n",
    "* **Goal:** Train an LLM to answer technical SAP questions using the Q&A pairs extracted in Task 3.\n",
    "* **Challenge:** Fine-tuning a Billion-parameter model on a standard laptop GPU (RTX 3050, 6GB VRAM).\n",
    "* **Solution:** We employed **QLoRA** (Quantized Low-Rank Adaptation) to reduce memory usage by ~70% while retaining performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ Tech Stack & Libraries\n",
    "| Library | Purpose |\n",
    "| :--- | :--- |\n",
    "| **`Transformers`** | Loads the pre-trained model architecture and tokenizer. |\n",
    "| **`BitsAndBytes`** | **Crucial for Low VRAM.** Compresses the model from 16-bit to **4-bit**, shrinking size from ~5GB to ~2GB. |\n",
    "| **`PEFT`** | Implements **LoRA**. Freezes the main model and trains only tiny \"adapter\" layers (approx 1% of parameters). |\n",
    "| **`TRL`** | Provides **`SFTTrainer`**, a specialized tool for Supervised Fine-Tuning of chat models. |\n",
    "| **`Rouge_Score`** | Evaluates the quality of generated answers against the reference SAP text. |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Technical Decisions (How we made it fit)\n",
    "\n",
    "#### 1. 4-Bit Quantization\n",
    "* **Logic:** Instead of loading the full model, we loaded a compressed version using `load_in_4bit=True`.\n",
    "* **Benefit:** This allowed the 1.1B parameter model to fit comfortably into 6GB VRAM alongside the training data.\n",
    "\n",
    "#### 2. LoRA (Low-Rank Adaptation)\n",
    "* **Logic:** Standard training updates *all* weights (impossible on a laptop). LoRA injects small trainable matrices into the attention layers (`q_proj`, `v_proj`).\n",
    "* **Benefit:** We only trained ~8 million parameters instead of 1.1 billion, making training 100x faster.\n",
    "\n",
    "#### 3. Paged Optimizers\n",
    "* **Logic:** We used `paged_adamw_8bit`.\n",
    "* **Benefit:** If GPU memory spikes, this optimizer temporarily offloads data to the CPU RAM, preventing \"Out of Memory\" crashes.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‰ Outcome: Before vs. After\n",
    "* **Before Training:** The base model gives generic, often irrelevant answers about SQL or databases.\n",
    "* **After Training:** The model provides structured, SAP-specific troubleshooting steps.\n",
    "\n",
    "> **Example Output:**\n",
    "> * **User:** \"SQL statements run for a long time... How do I resolve this?\"\n",
    "> * **Model:** \"Check SQL statements... Analyze performance... Optimize joins...\" (Matches the tone and structure of the official SAP KBA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cee9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. CONFIGURATION\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "new_model_name = \"sap-tinyllama-finetuned\"\n",
    "\n",
    "# 2. LOAD DATASET\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"sap_instruction_dataset.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. LOAD MODEL (4-BIT QUANTIZATION)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f5429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 30.59 examples/s]\n",
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.99 examples/s]\n",
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\accelerate\\accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on RTX 3050...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.5397, 'train_samples_per_second': 1.376, 'train_steps_per_second': 0.459, 'train_loss': 1.5790642102559407, 'epoch': 2.0}\n",
      "Training Complete!\n",
      "\n",
      "--- Running Evaluation ---\n",
      "\n",
      "[Generated]: <|user|>\n",
      "I am facing an issue in SAP HANA. The symptoms are: SQL statements run for a long time or consume a high amount of resources in terms of memory and CPU.... How do I resolve this?\n",
      "<|assistant|>\n",
      "To resolve this issue, you can follow the steps below:\n",
      "\n",
      "1. Check the SQL statements that are taking a long time or consuming high resources.\n",
      "2. Identify the SQL statements that are causing the issue.\n",
      "3. Analyze the performance of the database and identify bottlenecks.\n",
      "4. Optimize the SQL statements by reducing the number of joins, using indexes, and optimizing the query plan.\n",
      "5. Implement caching or other techniques to improve the performance of the database.\n",
      "6. Monitor the database performance regularly to identify any trends or patterns that may indicate potential issues.\n",
      "7. Regularly review and update the database schema to ensure that it is optimized for performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. LORA CONFIGURATION\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 5. TRAINING ARGUMENTS\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./sap_finetune_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "# 6. INITIALIZE TRAINER\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"instruction\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# 7. TRAIN\n",
    "\n",
    "print(\"Starting training on RTX 3050...\")\n",
    "trainer.train()\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Save\n",
    "trainer.model.save_pretrained(new_model_name)\n",
    "\n",
    "# 8. EVALUATION\n",
    "\n",
    "print(\"\\n--- Running Evaluation ---\")\n",
    "test_sample = dataset[0]\n",
    "prompt = f\"<|user|>\\n{test_sample['instruction']}\\n<|assistant|>\\n\"\n",
    "reference = test_sample['output']\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=150)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n[Generated]: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sap_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

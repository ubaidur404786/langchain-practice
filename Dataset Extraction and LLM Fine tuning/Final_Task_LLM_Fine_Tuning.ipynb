{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92f5429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 30.59 examples/s]\n",
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 11.99 examples/s]\n",
      "c:\\Users\\HP\\anaconda3\\envs\\sap_agent\\lib\\site-packages\\accelerate\\accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on RTX 3050...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.5397, 'train_samples_per_second': 1.376, 'train_steps_per_second': 0.459, 'train_loss': 1.5790642102559407, 'epoch': 2.0}\n",
      "Training Complete!\n",
      "\n",
      "--- Running Evaluation ---\n",
      "\n",
      "[Generated]: <|user|>\n",
      "I am facing an issue in SAP HANA. The symptoms are: SQL statements run for a long time or consume a high amount of resources in terms of memory and CPU.... How do I resolve this?\n",
      "<|assistant|>\n",
      "To resolve this issue, you can follow the steps below:\n",
      "\n",
      "1. Check the SQL statements that are taking a long time or consuming high resources.\n",
      "2. Identify the SQL statements that are causing the issue.\n",
      "3. Analyze the performance of the database and identify bottlenecks.\n",
      "4. Optimize the SQL statements by reducing the number of joins, using indexes, and optimizing the query plan.\n",
      "5. Implement caching or other techniques to improve the performance of the database.\n",
      "6. Monitor the database performance regularly to identify any trends or patterns that may indicate potential issues.\n",
      "7. Regularly review and update the database schema to ensure that it is optimized for performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "# ====================================================\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "new_model_name = \"sap-tinyllama-finetuned\"\n",
    "\n",
    "# 2. LOAD DATASET\n",
    "# ====================================================\n",
    "dataset = load_dataset(\"json\", data_files=\"sap_instruction_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "# 3. LOAD MODEL (4-BIT QUANTIZATION)\n",
    "# ====================================================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 4. LORA CONFIGURATION\n",
    "# ====================================================\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,  \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 5. TRAINING ARGUMENTS\n",
    "# ====================================================\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./sap_finetune_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "# 6. INITIALIZE TRAINER\n",
    "# ====================================================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"instruction\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# 7. TRAIN\n",
    "# ====================================================\n",
    "print(\"Starting training on RTX 3050...\")\n",
    "trainer.train()\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Save\n",
    "trainer.model.save_pretrained(new_model_name)\n",
    "\n",
    "# 8. EVALUATION\n",
    "# ====================================================\n",
    "print(\"\\n--- Running Evaluation ---\")\n",
    "test_sample = dataset[0]\n",
    "prompt = f\"<|user|>\\n{test_sample['instruction']}\\n<|assistant|>\\n\"\n",
    "reference = test_sample['output']\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=150)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n[Generated]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fd325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sap_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
